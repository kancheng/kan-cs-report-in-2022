# DMSASD - 数字媒体软件与系统开发 - Digital Media Software And System Development

> 2101212850 干皓丞

PKU 2022 個人實驗報告作業

## Details

Arsenal 中文素材

## lists 3.

0. The Creation and Detection of Deepfakes: A Survey

https://arxiv.org/abs/2004.11138


```
Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. 

In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these 'deepfakes' have advanced significantly.

In this paper, we explore the creation and detection of deepfakes and provide an in-depth view of how these architectures work.

The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas which require further research and attention.
```

目前生成式深度學習算法已經發展到難以區分真假的程度，而自 2018 年起，人們發現將這項技術用於不道德和惡意應用是多麼容易，例如傳播錯誤信息、冒充政治領導人以及誹謗無辜個人。 從那時起，這些“深度偽造”取得了顯著進展。該研究探討了 deepfakes 的創建和檢測，並深入了解這些架構的工作原理。本次調查的目的是讓讀者更深入地了解（1）如何創建和檢測深度偽造，（2）該領域的當前趨勢和進步，（3）當前防禦解決方案的缺點，以及（ 4）需要進一步研究和關注的領域。

Bibliography

```
@article{DBLP:journals/corr/abs-2004-11138,
  author    = {Yisroel Mirsky and
               Wenke Lee},
  title     = {The Creation and Detection of Deepfakes: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2004.11138},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.11138},
  eprinttype = {arXiv},
  eprint    = {2004.11138},
  timestamp = {Tue, 28 Apr 2020 16:10:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-11138.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```
1. Dami Lee. Deepfake salvador dal takes selfies with museum visitors - the verge. https://bit.ly/3cEim4m, 5 2019.

2. Holly Kathleen Hall. Deepfake videos: When seeing isn’t believing. Cath. UJL & Tech, 27:51, 2018.

3. Robert Chesney and Danielle Keats Citron. Deep fakes: a looming challenge for privacy, democracy, and national security. 2018.

4. Arije Antinori. Terrorism and deepfake: from hybrid warfare to posttruth warfare in a hybrid world. In ECIAIR 2019 European Conference on the Impact of Artificial Intelligence and Robotics, page 23. Academic Conferences and publishing limited, 2019.

5. Daniil Kononenko et al. Photorealistic monocular gaze redirection using machine learning. IEEE transactions on pattern analysis and machine intelligence, 40(11):2696–2710, 2017.

6. Luan Tran, Xi Yin, and Xiaoming Liu. Representation learning by rotating your faces. IEEE transactions on pattern analysis and machine intelligence, 41(12):3007–3021, 2018.

7. Oscar Schwartz. You thought fake news was bad? – the guardian. https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth, 5 2018. (Accessed on 03/02/2020).

8. Karen Hao. The biggest threat of deepfakes isnt the deepfakes themselves - mit tech review. https://www.technologyreview.com/s/614526/the-biggest-threat-of-deepfakes-isnt-the-deepfakes-themselves/, 2019.

9. Sigal Samuel. A guy made a deepfake app to turn photos of women into nudes. it didnt go well. https://www.vox.com/2019/6/27/18761639/ai-deepfake-deepnude-app-nude-women-porn, 5 2019.

10. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. arXiv preprint arXiv:1912.04958, 2019.

11. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014.

12. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Imageto-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125–1134, 2017.

13. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.

14. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223–2232, 2017.

15. Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE International Conference on Computer Vision, pages 7184–7193, 2019.

16. Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer. Ganimation: One-shot anatomically consistent facial animation. International Journal of Computer Vision, pages 1–16, 2019.

17. Aliaksandr Siarohin, Stephane Lathuili ´ ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Advances in Neural Information Processing Systems 32, pages 7135–7145. Curran Associates, Inc., 2019.

18. Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. End-toend speech-driven realistic facial animation with temporal gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 37–40, 2019.

19. Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy. Reenactgan: Learning to reenact faces via boundary
transfer. In Proceedings of the European Conference on Computer Vision (ECCV), pages 603–619, 2018.

20. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526–1535, 2018.

21. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

22. Hyeongwoo Kim, Pablo Carrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick Perez, Christian Richardt, Michael Zollhofer, and Christian Theobalt. Deep video portraits. ACM Transactions on Graphics (TOG), 37(4):163, 2018.

23. Hai X Pham, Yuting Wang, and Vladimir Pavlovic. Generative adversarial talking head: Bringing portraits to life with a weakly supervised neural network. arXiv preprint arXiv:1803.07716, 2018.

24. Enrique Sanchez and Michel Valstar. Triple consistency loss for
pairing distributions in gan-based face synthesis. arXiv preprint
arXiv:1811.03492, 2018.

25. Yujun Shen, Ping Luo, Junjie Yan, Xiaogang Wang, and Xiaoou Tang. Faceid-gan: Learning a symmetry three-player gan for identitypreserving face synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 821–830, 2018.

26. Yujun Shen, Bolei Zhou, Ping Luo, and Xiaoou Tang. Facefeat-gan: a two-stage approach for identity-preserving face synthesis. arXiv preprint arXiv:1812.01288, 2018.

27. Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, Hao Li, Richard Roberts, et al. pagan: real-time avatars using dynamic textures. ACM Trans. Graph., 37(6):258–1, 2018.

28. Olivia Wiles, A Sophia Koepke, and Andrew Zisserman. X2face: A network for controlling face generation using images, audio, and pose codes. In Proceedings of the European Conference on Computer Vision (ECCV), pages 670–686, 2018.

29. Jiangning Zhang, Xianfang Zeng, Yusu Pan, Yong Liu, Yu Ding, and Changjie Fan. Faceswapnet: Landmark guided many-to-many face reenactment. arXiv preprint arXiv:1905.11805, 2019.

30. Chaoyou Fu, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, and Ran He. High fidelity face manipulation with extreme pose and expression. arXiv preprint arXiv:1903.12003, 2019.

31. Soumya Tripathy, Juho Kannala, and Esa Rahtu. Icface: Interpretable and controllable face reenactment using gans. arXiv preprint arXiv:1904.01909, 2019.

32. Shengju Qian et al. Make a face: Towards arbitrary high fidelity face manipulation. In Proceedings of the IEEE International Conference on Computer Vision, 2019.

33. Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and Kun Zhou. Warp-guided gans for single-photo facial animation. ACM Transactions on Graphics (TOG), 37(6):231, 2019.

34. Naima Otberdout, Mohamed Daoudi, Anis Kacem, Lahoucine Ballihi, and Stefano Berretti. Dynamic facial expression generation on hilbert hypersphere with conditional wasserstein generative adversarial nets. arXiv preprint arXiv:1907.10087, 2019.

35. Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. Imaginator: Conditional spatio-temporal gan for video generation. 2020.

36. Aliaksandr Siarohin, Stephane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2377–2386, 2019.

37. Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learning of realistic neural talking head models. arXiv preprint arXiv:1905.08233, 2019.

38. Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, and Dongyoung Kim. Marionette: Few-shot face reenactment preserving identity of unseen targets. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.

39. Zhaoxiang Liu, Huan Hu, Zipeng Wang, Kai Wang, Jinqiang Bai, and Shiguo Lian. Video synthesis of human upper body with realistic face. arXiv preprint arXiv:1908.06607, 2019.

40. Supasorn Suwajanakorn, Steven M Seitz, and Ira KemelmacherShlizerman. Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics (TOG), 36(4):95, 2017.

41. Ohad Fried, Ayush Tewari, Michael Zollhofer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh Agrawala. Text-based editing of talking-head video. arXiv preprint arXiv:1906.01524, 2019.

42. Seyed Ali Jalalifar, Hosein Hasani, and Hamid Aghajan. Speech-driven facial reenactment using conditional generative adversarial networks. arXiv preprint arXiv:1803.07461, 2018.

43. Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Niessner. Neural voice puppetry: Audio-driven facial reenactment. arXiv preprint arXiv:1912.05566, 2019.

44. Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7832–7841, 2019.

45. Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. Talking face generation by adversarially disentangled audio-visual representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9299–9306, 2019.

46. Amir Jamaludin, Joon Son Chung, and Andrew Zisserman. You said that?: Synthesising talking faces from audio. International Journal of Computer Vision, pages 1–13, 2019.

47. Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Realistic speech-driven facial animation with gans. arXiv preprint arXiv:1906.06337, 2019.

48. Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Proceedings of the IEEE International Conference on Computer Vision, pages 5933–5942, 2019.

49. Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard, Marc Habermann, Wenping Wang, and Christian Theobalt. Neural rendering and reenactment of human actor videos. ACM Transactions on Graphics (TOG), 38(5):139, 2019.

50. Kfir Aberman, Mingyi Shi, Jing Liao, D Liscbinski, Baoquan Chen, and Daniel Cohen-Or. Deep video-based performance cloning. In Computer Graphics Forum. Wiley Online Library, 2019.

51. Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019.

52. Volker Blanz, Curzio Basso, Tomaso Poggio, and Thomas Vetter. Reanimating faces in images and video. In Computer graphics forum, volume 22, pages 641–650. Wiley Online Library, 2003.

53. Yao-Jen Chang and Tony Ezzat. Transferable videorealistic speech animation. In Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 143–151. ACM, 2005.

54. Justus Thies, Michael Zollhofer, Matthias Niessner, Levi Valgaerts,
Marc Stamminger, and Christian Theobalt. Real-time expression transfer for facial reenactment. ACM Trans. Graph., 34(6):183–1, 2015.

55. Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Niessner. Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2387–2395, 2016.

56. Justus Thies, Michael Zollhofer, Christian Theobalt, Marc Stamminger, and Matthias Niessner. Headon: Real-time reenactment of human portrait videos. ACM Transactions on Graphics (TOG), 37(4):164, 2018.

57. Runze Xu, Zhiming Zhou, Weinan Zhang, and Yong Yu. Face transfer with generative adversarial network. arXiv preprint:1710.06090, 2017.

58. Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. Recycle-gan: Unsupervised video retargeting. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.

59. Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Cvae-gan: fine-grained image generation through asymmetric training. In Proceedings of the IEEE International Conference on Computer Vision, pages 2745–2754, 2017.

60. Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, and Hao Li. Realistic dynamic facial textures from a single image using gans. In Proceedings of the IEEE International Conference on Computer Vision, pages 5429–5438, 2017.

61. Yuqian Zhou and Bertram Emil Shi. Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII), pages 370–376. IEEE, 2017.

62. Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. Bringing portraits to life. ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia 2017), 36(6):196, 2017.

63. Yunxuan Zhang, Siwei Zhang, Yue He, Cheng Li, Chen Change Loy, and Ziwei Liu. One-shot face reenactment. arXiv preprint
arXiv:1908.03251, 2019.

64. Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-shot video-to-video synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

65. Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 187–194, 1999.

66. Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexandre de Brebisson, and Yoshua Bengio. Obamanet: Photo-realistic lip-sync from text. arXiv preprint arXiv:1801.01442, 2017.

67. Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, and Yoshua Bengio. Char2wav: End-to-end speech synthesis. Openreview.net.

68. Justus Thies, Michael Zollhofer, and Matthias Niessner. Deferred neural rendering: Image synthesis using neural textures. arXiv preprint arXiv:1904.12356, 2019.

69. Taiki Shimba, Ryuhei Sakurai, Hirotake Yamazoe, and Joo-Ho Lee. Talking heads synthesis from audio with deep neural networks. In 2015 IEEE/SICE International Symposium on System Integration (SII), pages 100–105. IEEE, 2015.

70. Yang Song, Jingwen Zhu, Xiaolong Wang, and Hairong Qi. Talking face generation by conditional recurrent adversarial network. arXiv preprint arXiv:1804.04786, 2018.

71. Lingyun Yu, Jun Yu, and Qiang Ling. Mining audio, text and visual information for talking face generation. In 2019 IEEE International Conference on Data Mining (ICDM), pages 787–795. IEEE, 2019.

72. Triantafyllos Kefalas, Konstantinos Vougioukas, Yannis Panagakis, Stavros Petridis, Jean Kossaifi, and Maja Pantic. Speech-driven facial animation using polynomial fusion of features. arXiv preprint arXiv:1912.05833, 2019.

73. Yibo Hu, Xiang Wu, Bing Yu, Ran He, and Zhenan Sun. Pose-guided photorealistic face rotation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8398–8406, 2018.

74. Jie Cao, Yibo Hu, Bing Yu, Ran He, and Zhenan Sun. 3d aided duet gans for multi-view face image synthesis. IEEE Transactions on Information Forensics and Security, 14(8):2028–2042, 2019.

75. Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, and Victor Lempitsky. Deepwarp: Photorealistic image resynthesis for gaze manipulation. In European conference on computer vision. Springer, 2016.

76. Yu Yu, Gang Liu, and Jean-Marc Odobez. Improving few-shot userspecific gaze adaptation via gaze redirection synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11937–11946, 2019.

77. Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L Berg. Dance dance generation: Motion transfer for internet videos. arXiv preprint arXiv:1904.00129, 2019.

78. Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere, and Nicu Sebe. Deformable gans for pose-based human image generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3408–3416, 2018.

79. Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei Wang, and Xiang Bai. Progressive pose attention transfer for person image generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2347–2356, 2019.

80. Natalia Neverova, Riza Alp Guler, and Iasonas Kokkinos. Dense pose transfer. In Proceedings of the European conference on computer vision (ECCV), pages 123–138, 2018.

81. Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.

82. Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Durand, and John Guttag. Synthesizing images of humans in unseen poses. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8340–8348, 2018.

83. Liqian Ma et al. Disentangled person image generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 99–108, 2018.

84. Rodrigo De Bem, Arnab Ghosh, Adnane Boukhayma, Thalaiyasingam Ajanthan, N Siddharth, and Philip Torr. A conditional deep generative model of people in natural images. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.

85. Jessica Lee, Deva Ramanan, and Rohit Girdhar. Metapix: Few-shot video retargeting. arXiv preprint arXiv:1910.04742, 2019.

86. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic metalearning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.

87. iperov. Deepfacelab: Deepfacelab is a tool that utilizes machine learning to replace faces in videos. https://github.com/iperov/DeepFaceLab, 2019. (Accessed on 12/31/2019).

88. deepfakes/faceswap: Deepfakes software for all. https://github.com/deepfakes/faceswap, 2017. (Accessed on 01/27/2020).

89. shaoanlu. faceswap-gan: A denoising autoencoder + adversarial losses and attention mechanisms for face swapping. https://github.com/shaoanlu/faceswap-GAN, 2018. (Accessed on 12/17/2019).

90. Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis. Fast face-swap using convolutional neural networks. In Proceedings of the IEEE International Conference on Computer Vision, 2017.

91. Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Towards open-set identity preserving face synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.

92. Ryota Natsume, Tatsuya Yatagawa, and Shigeo Morishima. Rsgan: face swapping and editing using face and hair representation in latent spaces. arXiv preprint arXiv:1804.03447, 2018.

93. Ryota Natsume, Tatsuya Yatagawa, and Shigeo Morishima. Fsnet: An identity-aware generative model for image-based face swapping. In Asian Conference on Computer Vision, pages 117–132. Springer, 2018.

94. Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high fidelity and occlusion aware face swapping. arXiv preprint arXiv:1912.13457, 2019.

95. Shaoanlu. fewshot-face-translation-gan: Generative adversarial networks integrating modules from funit and spade for face-swapping. https://github.com/shaoanlu/fewshot-face-translation-GAN, 2019.

96. Joel Ruben Antony Moniz, Christopher Beckham, Simon Rajotte, Sina Honari, and Chris Pal. Unsupervised depth estimation, 3d face rotation and replacement. In Advances in Neural Information Processing Systems, pages 9736–9746, 2018.

97. Fanyi Xiao, Haotian Liu, and Yong Jae Lee. Identity from here, pose from there: Self-supervised disentanglement and generation of objects using unlabeled videos. In Proceedings of the IEEE International Conference on Computer Vision, pages 7013–7022, 2019.

98. Volker Blanz, Kristina Scherbaum, Thomas Vetter, and Hans-Peter Seidel. Exchanging faces in images. In Computer Graphics Forum, volume 23, pages 669–676. Wiley Online Library, 2004.

99. Dmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter Belhumeur, and Shree K Nayar. Face swapping: automatically replacing faces in photographs. In ACM Transactions on Graphics (TOG), volume 27, page 39. ACM, 2008.

100. Kevin Dale, Kalyan Sunkavalli, Micah K Johnson, Daniel Vlasic, Wojciech Matusik, and Hanspeter Pfister. Video face replacement. In ACM Transactions on Graphics (TOG). ACM, 2011.

101. Ira Kemelmacher-Shlizerman. Transfiguring portraits. ACM Transactions on Graphics (TOG), 35(4):94, 2016.

102. Yuval Nirkin, Iacopo Masi, Anh Tran Tuan, Tal Hassner, and Gerard Medioni. On face segmentation, face swapping, and face perception. In 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), pages 98–105. IEEE, 2018.

103. Qianru Sun, Ayush Tewari, Weipeng Xu, Mario Fritz, Christian Theobalt, and Bernt Schiele. A hybrid model for identity obfuscation by face replacement. In Proceedings of the European Conference on Computer Vision (ECCV), pages 553–569, 2018.

104. Lilei Zheng, Ying Zhang, and Vrizlynn LL Thing. A survey on image tampering and its detection in real-world photos. Journal of Visual Communication and Image Representation, 58:380–399, 2019.

105. Valentina Conotter, Ecaterina Bodnari, Giulia Boato, and Hany Farid. Physiologically-based detection of computer generated faces in video. In 2014 IEEE International Conference on Image Processing (ICIP), pages 248–252. IEEE, 2014.

106. Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi: Exposing ai created fake videos by detecting eye blinking. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1–7. IEEE, 2018.

107. Umur Aybars Ciftci and Ilke Demir. Fakecatcher: Detection of synthetic portrait videos using biological signals. arXiv preprint arXiv:1901.02212, 2019.

108. Pavel Korshunov and Sebastien Marcel. Speaker inconsistency detection in tampered video. In 2018 26th European Signal Processing Conference (EUSIPCO), pages 2375–2379. IEEE, 2018.

109. Pavel Korshunov et al. Tampered speaker inconsistency detection with phonetically aware audio-visual features. In International Conference on Machine Learning, number CONF, 2019.

110. Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8261–8265. IEEE, 2019.

111. Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li. Protecting world leaders against deep fakes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 38–45, 2019.

112. Akshay Agarwal, Richa Singh, Mayank Vatsa, and Afzel Noore. Swapped! digital face presentation attack detection via weighted local magnitude pattern. In 2017 IEEE International Joint Conference on Biometrics (IJCB), pages 659–665. IEEE, 2017.

113. Ying Zhang, Lilei Zheng, and Vrizlynn LL Thing. Automated face swapping and its detection. In 2017 IEEE 2nd International Conference on Signal and Image Processing (ICSIP), pages 15–19. IEEE, 2017.

114. Zahid Akhtar and Dipankar Dasgupta. A comparative evaluation of local feature descriptors for deepfakes detection.

115. Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces identification via convolutional neural network. In Proceedings of the 6th ACM Workshop on Information Hiding and Multimedia Security. ACM, 2018.

116. Ricard Durall, Margret Keuper, Franz-Josef Pfreundt, and Janis Keuper. Unmasking deepfakes with simple features. arXiv preprint arXiv:1911.00686, 2019.

117. Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face x-ray for more general face forgery detection. arXiv preprint arXiv:1912.13458, 2019.

118. Huy H Nguyen, Fuming Fang, Junichi Yamagishi, and Isao Echizen. Multi-task learning for detecting and segmenting manipulated facial images and videos. arXiv preprint arXiv:1906.06876, 2019.

119. Mengnan Du, Shiva Pentyala, Yuening Li, and Xia Hu. Towards generalizable forgery detection with locality-aware autoencoder. arXiv preprint arXiv:1909.05999, 2019.

120. Joel Stehouwer, Hao Dang, Feng Liu, Xiaoming Liu, and Anil Jain. On the detection of digital face manipulation. arXiv preprint arXiv:1910.01717, 2019.

121. Jia Li, Tong Shen, Wei Zhang, Hui Ren, Dan Zeng, and Tao Mei. Zooming into face forensics: A pixel-level analysis. arXiv preprint arXiv:1912.05790, 2019.

122. Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping artifacts. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.

123. Yuezun Li and Siwei Lyu. Dsp-fwa: Dual spatial pyramid for exposing face warp artifacts in deepfake videos. https://github.com/danmohaha/DSP-FWA, 2019. (Accessed on 12/18/2019).

124. Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: A new dataset for deepfake forensics. arXiv preprint:1909.12962, 2019.

125. Jeremy Straub. Using subject face brightness assessment to detect deep fakes(conference presentation). In Real-Time Image Processing and Deep Learning 2019, volume 10996, page 109960H. International Society for Optics and Photonics, 2019.

126. Pavel Korshunov and Sebastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018.

127. Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake images to gans: Learning and analyzing gan fingerprints. In Proceedings of the IEEE International Conference on Computer Vision, 2019.

128. Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and Giovanni Poggi. Do gans leave artificial fingerprints? In 2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pages 506–511. IEEE, 2019.

129. Marissa Koopman, Andrea Macarulla Rodriguez, and Zeno Geradts. Detection of deepfake video manipulation. In Conference: IMVIP, 2018.

130. David Guera and Edward J Delp. Deepfake video detection using recurrent neural networks. In IEEE Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–6. IEEE, 2018.

131. Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, and Prem Natarajan. Recurrent-convolution approach to deepfake detection-state-of-art results on faceforensics++. arXiv preprint arXiv:1905.00582, 2019.

132. Irene Amerini, Leonardo Galteri, Roberto Caldelli, and Alberto Del Bimbo. Deepfake video detection through optical flow based cnn. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0–0, 2019.

133. Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video forgery detection network. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1–7. IEEE, 2018.

134. Nhu-Tai Do, In-Seop Na, and Soo-Hyung Kim. Forensics face detection from gans using convolutional neural network, 2018.

135. Shahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin, and Simon S Woo. Detecting both machine and human created fake face images in the wild. In Proceedings of the 2nd International Workshop on Multimedia Privacy and Security, pages 81–87. ACM, 2018.

136. Xinyi Ding, Zohreh Raziei, Eric C Larson, Eli V Olinick, Paul Krueger, and Michael Hahsler. Swapped face detection using deep learning and subjective assessment. arXiv preprint arXiv:1909.04217, 2019.

137. Tharindu Fernando, Clinton Fookes, Simon Denman, and Sridha Sridharan. Exploiting human social cognition for the detection of fake and fraudulent faces via memory networks. arXiv preprint arXiv:1911.07844, 2019.

138. Francesco Marra, Diego Gragnaniello, Davide Cozzolino, and Luisa Verdoliva. Detection of gan-generated fake images over social networks. In 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pages 384–389. IEEE, 2018.

139. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. Faceforensics++: Learning to detect manipulated facial images. arXiv preprint:1901.08971, 2019.

140. Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. Capsuleforensics: Using capsule networks to detect forged images and videos. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2307–2311. IEEE, 2019.

141. Run Wang, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Jian Wang, and Yang Liu. Fakespotter: A simple baseline for spotting ai-synthesized fake faces. arXiv preprint arXiv:1909.06122, 2019.

142. Xinsheng Xuan, Bo Peng, Wei Wang, and Jing Dong. On the generalization of gan image forensics. In Chinese Conference on Biometric Recognition, pages 134–141. Springer, 2019.

143. Davide Cozzolino, Justus Thies, Andreas Rossler, Christian Riess, Matthias Niessner, and Luisa Verdoliva. Forensictransfer: Weaklysupervised domain adaptation for forgery detection. arXiv preprint arXiv:1812.02510, 2018.

144. Xiaoguang Tu, Hengsheng Zhang, Mei Xie, Yao Luo, Yuefei Zhang, and Zheng Ma. Deep transfer across domains for face anti-spoofing. arXiv preprint arXiv:1901.05633, 2019.

145. Paula Fraga-Lamas and Tiago M Fernandez-Carames. Leveraging distributed ledger technologies and blockchain to combat fake news. arXiv preprint arXiv:1904.05386, 2019.

146. Chi-Ying Chen et al. A trusting news ecosystem against fake news from humanity and technology perspectives. In 2019 19th International Conference on Computational Science and Its Applications (ICCSA), pages 132–137. IEEE, 2019.

147. Haya R Hasan and Khaled Salah. Combating deepfake videos using blockchain and smart contracts. IEEE Access, 7:41596–41606, 2019.

148. Yuezun Li, Xin Yang, Baoyuan Wu, and Siwei Lyu. Hiding faces in plain sight: Disrupting ai face synthesis with adversarial perturbations. arXiv preprint arXiv:1906.09288, 2019.

149. Andrew Gully Nick Dufour. Dfd. https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html, 9 2019.

150. Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset. arXiv preprint arXiv:1910.08854, 2019.

151. Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 1646–1654, 2016.

152. Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.

153. Yisroel Mirsky, Tom Mahler, Ilan Shelef, and Yuval Elovici. Ct-gan: Malicious tampering of 3d medical imagery using deep learning. In USENIX Security Symposium 2019, 2019.

154. Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, et al. Transfer learning from speaker verification to multispeaker text-tospeech synthesis. In Advances in neural information processing systems, pages 4480–4490, 2018.

155. Jesse Demiani. A voice deepfake was used to scam a ceo out of $243,000 - forbes. https://bit.ly/38sXb1I, 9 2019.

156. Philip Bontrager, Aditi Roy, Julian Togelius, Nasir Memon, and Arun Ross. Deepmasterprints: Generating masterprints for dictionary attacks via latent variable evolution. In 2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS). IEEE, 2018.

157. Marco Schreyer, Timur Sattarov, Bernd Reimer, and Damian Borth. Adversarial learning of deepfakes in accounting. arXiv preprint arXiv:1910.03810, 2019.

158. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Advances in Neural Information Processing Systems 32, pages 9054–9065. Curran Associates, Inc., 2019.

159. Sakshi Agarwal and Lav R Varshney. Limits of deepfake detection: A robust estimation viewpoint. arXiv preprint arXiv:1905.03493, 2019.

160. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179, 2018.

161. Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja, Pankaj Wasnik, and Christoph Busch. Fake face detection methods: Can they be generalized? In 2018 International Conference of the Biometrics Special Interest Group (BIOSIG), pages 1–6. IEEE, 2018.