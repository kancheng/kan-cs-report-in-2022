# DMSASD - 数字媒体软件与系统开发 - Digital Media Software And System Development

> 2101212850 干皓丞

PKU 2022 個人實驗報告作業

## Details

```
此為個人的論文軍火庫
```

期末工作，深度偽造、檢測、法律、時事 Deepfackes & Detection

近年來人工智能領域的蓬勃發展 ...


## Reference

1. https://zhuanlan.zhihu.com/p/90316297

2. https://zhuanlan.zhihu.com/p/92474937

3. https://zhuanlan.zhihu.com/p/92853899

4. https://zhuanlan.zhihu.com/p/115070797

5. https://github.com/HongguLiu/Deepfake-Detection



## Links

1. Deep Learning for Deepfakes Creation and Detection: A Survey

https://www.researchgate.net/publication/336055871_Deep_Learning_for_Deepfakes_Creation_and_Detection_A_Survey

```
Deep learning has been successfully applied to solvevarious complex problems ranging from big data analytics tocomputer vision and human-level control. 

Deep learning advanceshowever have also been employed to create software that cancause threats to privacy, democracy and national security. 

Oneof those deep learning-powered applications recently emergedis “deepfake”. 

Deepfake algorithms can create fake images andvideos that humans cannot distinguish them from authenticones.

The proposal of technologies that can automatically detectand assess the integrity of digital visual media is thereforeindispensable.

This paper presents a survey of algorithms usedto create deepfakes and, more importantly, methods proposed todetect deepfakes in the literature to date.

We present extensivediscussions on challenges, research trends and directions relatedto deepfake technologies.

By reviewing the background of deep-fakes and state-of-the-art deepfake detection methods, this studyprovides a comprehensive overview of deepfake techniques andfacilitates the development of new and more robust methods todeal with the increasingly challenging deepfakes.
```


2. Deepfakes Detection Techniques Using Deep Learning: A Survey

https://www.scirp.org/journal/paperinformation.aspx?paperid=109149

```
Deep learning is an effective and useful technique that has been widely applied in a variety of fields, including computer vision, machine vision, and natural language processing. 

Deepfakes uses deep learning technology to manipulate images and videos of a person that humans cannot differentiate them from the real one. 

In recent years, many studies have been conducted to understand how deepfakes work and many approaches based on deep learning have been introduced to detect deepfakes videos or images. 

In this paper, we conduct a comprehensive review of deepfakes creation and detection technologies using deep learning approaches. 

In addition, we give a thorough analysis of various technologies and their application in deepfakes detection.

Our study will be beneficial for researchers in this field as it will cover the recent state-of-art methods that discover deepfakes videos or images in social contents.

In addition, it will help comparison with the existing works because of the detailed description of the latest methods and dataset used in this domain.
```

## Laws

```
法律思維與制度的智慧轉型, 李建良．劉靜怡．邱文聰．吳全峰．陳弘儒．陳柏良．何之行．廖貞．黃相博．林勤富．李怡俐．楊岳平．鄭瑞健．沈宗倫．王怡蘋
```
Link : https://www.angle.com.tw/book.asp?BKID=12196

1. 人工智慧時代的法學研究路徑初探，劉靜怡

2. 第二波人工智慧知識學習與生產對法學的挑戰 — 資訊、科技與社會研究及法學的對話，邱文聰

3. 初探人工智慧與生命倫理之關係，吳全峰

4. 初探目的解釋在法律人工智慧系統之運用可能，陳弘儒

5. AI 時代之分裂社會與民主—以美國法之表意自由與觀念市場自由競爭理論為中心，陳柏良

6. AI 個資爭議在英國與歐盟之經驗 — 以Google DeepMind一案為例，何之行、廖貞

Note : Google 醫療體系與歐盟法規之間發生的問題

7. 人工智慧在金融業的應用—論數位金融與一般個人資料保護規則之適用與衝突，黃相博

8. 人工智慧時代下的國際人權法 ─ 規範與制度的韌性探索與再建構，林勤富、李怡俐

9. 人工智慧時代下的金融監理議題 ─ 以理財機器人監理為例，楊岳平

10. 人工智慧時代下的證券監理 ─ 以智能合約在區塊鏈技術的應用出發，鄭瑞健

11. 人工智慧科技對於專利侵權法制的衝擊與因應之道 ─ 以責任歸屬為中心，沈宗倫

12. 人工智慧創作與著作權之相關問題，王怡蘋

## lists


0. Li XR, Ji SL, Wu CM, Liu ZG, Deng SG, Cheng P, Yang M, Kong XW. Survey on deepfakes and detection techniques. Ruan Jian Xue Bao/Journal of Software, 2021,32(2):496−518 (in Chinese). http://www.jos.org.cn/1000-9825/6140.htm 

Link : http://www.jos.org.cn/josen/article/abstract/6140

Note : 深度偽造與檢測綜述

```
Deep learning has achieved great success in the field of computer vision, surpassing many traditional methods. 

However, in recent years, deep learning technology has been abused in the production of fake videos, making fake videos represented by Deepfakes flooding on the Internet. 

This technique produces pornographic movies, fake news, political rumors by tampering or replacing the face information of the original videos and synthesizes fake speech. 

In order to eliminate the negative effects brought by such forgery technologies, many researchers have conducted in-depth research on the identification of fake videos and proposed a series of detection methods to help institutions or communities to identify such fake videos. 

Nevertheless, the current detection technology still has many limitations such as specific distribution data, specific compression ratio, and so on, far behind the generation technology of fake video. 

In addition, different researchers handle the problem from different angles. 

The data sets and evaluation indicators used are not uniform.

So far, the academic community still lacks a unified understanding of deep forgery and detection technology. 

The architecture of deep forgery and detection technology research is not clear.

In this review, the development of deep forgery and detection technologies are reviewed.

Besides, existing research works are systematically summarize and scientifically classified.

Finally, the social risks posed by the spread of Deepfakes technology are discussed, the limitations of detection technology are analyzed, and the challenges and potential research directions of detection technology are discussed, aiming to provide guidance for follow-up researchers to further promote the development and deployment of Deepfakes detection technology. 
```


1. Wu Z, Kinnunen T, Chng ES, Li H, Ambikairajah E. A study on spoofing attack in state-of-the-art speaker verification: The telephone speech case. In: Proc. of the Asia Pacific Signal and Information Processing Association Annual Summit and Conf. IEEE, 2012. 1−5.

Link : https://ieeexplore.ieee.org/document/6411897

Note : 深度偽造語音檢測，Wu 等人提出的歸一化的余弦相位和修改的群延遲

```
Voice conversion technique, which modifies one speaker's (source) voice to sound like another speaker (target), presents a threat to automatic speaker verification. 

In this paper, we first present new results of evaluating the vulnerability of current state-of-the-art speaker verification systems: Gaussian mixture model with joint factor analysis (GMM-JFA) and probabilistic linear discriminant analysis (PLDA) systems, against spoofing attacks. 

The spoofing attacks are simulated by two voice conversion techniques: Gaussian mixture model based conversion and unit selection based conversion. 

To reduce false acceptance rate caused by spoofing attack, we propose a general anti-spoofing attack framework for the speaker verification systems, where a converted speech detector is adopted as a post-processing module for the speaker verification system's acceptance decision. 

The detector decides whether the accepted claim is human speech or converted speech. 

A subset of the core task in the NIST SRE 2006 corpus is used to evaluate the vulnerability of speaker verification system and the performance of converted speech detector. 

The results indicate that both conversion techniques can increase the false acceptance rate of GMM-JFA and PLDA system, while the converted speech detector can reduce the false acceptance rate from 31.54% and 41.25% to 1.64% and 1.71% for GMM-JFA and PLDA system on unit-selection based converted speech, respectively.
```


2. Wu Z, Chng ES, Li H. Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition. In: Proc. of the 13th Annual Conf. of the Int’l Speech Communication Association. 2012. 1700−1703.

Link : https://www.researchgate.net/publication/260343013_Detecting_Converted_Speech_and_Natural_Speech_for_anti-Spoofing_Attack_in_Speaker_Recognition

Note : 深度偽造語音檢測，Wu 等人提出的歸一化的余弦相位和修改的群延遲

```
Voice conversion techniques present a threat to speaker verification systems. 

To enhance the security of speaker verification systems, We study how to automat-ically distinguish natural speech and synthetic/converted speech. 

Motivated by the research on phase spectrum in speech perception, in this study, we propose to use fea-tures derived from phase spectrum to detect converted speech. 

The features are tested under three different train-ing situations of the converted speech detector: a) only Gaussian mixture model (GMM) based converted speech data are available; b) only unit-selection based converted speech data are available; c) no converted speech data are available for training converted speech model. 

Experi-ments conducted on the National Institute of Standards and Technology (NIST) 2006 speaker recognition evalu-ation (SRE) corpus show that the performance of the fea-tures derived from phase spectrum outperform the mel-frequency cepstral coefficients (MFCCs) tremendously: even without converted speech for training, the equal er-ror rate (EER) is reduced from 20.20% of MFCCs to 2.35%.
```

3. Das RK, Yang J, Li H. Long range acoustic and deep features perspective on ASVspoof 2019. In: Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019. 1018−1025.

Link : https://ieeexplore.ieee.org/document/9003845

Note : 1) ASVspoof2019 資料集，2) 從遠程聲學和深度特徵的角度總結了欺騙檢測的發現且對不同類型的欺騙攻擊的性質和系統開發進行了綜合分析。

```
To secure automatic speaker verification (ASV) systems from intruders, robust countermeasures for spoofing attack detection are required.

The ASVspoof series of challenge provides a shared anti-spoofing task.

The recent edition, ASVspoof 2019, focuses on attacks by both synthetic and replay speech that are referred to as logical and physical access attacks, respectively.

In the ASVspoof 2019 submission, we considered novel countermeasures based on long range acoustic features, that are unique in many ways as they are derived using octave power spectrum and subbands, as opposed to the commonly used linear power spectrum.

During the post-challenge study, we further investigate the use of deep features that enhances the discriminative ability between genuine and spoofed speech.

In this paper, we summarize the findings from the perspective of long range acoustic and deep features for spoof detection.

We make a comprehensive analysis on the nature of different kinds of spoofing attacks and system development.
```

4. Zeinali H, Stafylakis T, Athanasopoulou G, Rohdin J, Gkinis I, Burget L, Cernocky JH. Detecting spoofing attacks using VGG and SincNet: BUT-Omilia submission to ASVspoof 2019 challenge. In: Proc. of the 20th Annual Conf. of the Int’l Speech Communication Association. 2019. 1073−1077.


Link : https://arxiv.org/abs/1907.12908

Note : 應用不同架構來應對攻擊

Tag : CVPR

```
In this paper, we present the system description of the joint efforts of Brno University of Technology (BUT) and Omilia - Conversational Intelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge.

The primary submission for Physical access (PA) is a fusion of two VGG networks, trained on single and two-channels features.

For Logical access (LA), our primary system is a fusion of VGG and the recently introduced SincNet architecture.

The results on PA show that the proposed networks yield very competitive performance in all conditions and achieved 86\:\% relative improvement compared to the official baseline.

On the other hand, the results on LA showed that although the proposed architecture and training strategy performs very well on certain spoofing attacks, it fails to generalize to certain attacks that are unseen during training.

```

5. Schörkhuber C, Klapuri A. Constant-Q transform toolbox for music processing. In: Proc. of the 7th Sound and Music Computing Conf. Barcelona, 2010. 3−64.

Link : https://core.ac.uk/download/pdf/144846462.pdf

Note : CQT 特徵，提出了一種計算時域信號恆定 Q 變換 (CQT) 的高效計算方法。

```
This paper proposes a computationally efficient method for computing the constant-Q transform (CQT) of a timedomain signal. 

CQT refers to a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal. 

An inverse transform is proposed which enables a reasonable-quality (around 55dB signal-to-noise ratio) reconstruction of the original signal from its CQT coefficients. 

Here CQTs with high Q-factors, equivalent to 12–96 bins per octave, are of particular interest.

The proposed method is flexible with regard to the number of bins per octave, the applied window function, and the Q-factor, and is particularly suitable for the analysis of music signals.

A reference implementation of the proposed methods is published as a Matlab toolbox.

The toolbox includes user-interface tools that facilitate spectral data visualization and the indexing and working with the data structure produced by the CQT.
```

6. Gomez-Alanis A, Peinado AM, Gonzalez JA, Gomez AM. A light convolutional GRU-RNN deep feature extractor for ASV spoofing detection. In: Proc. of the Interspeech 2019. 2019. 1068−1072.

Link : https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2212.pdf

Note : 1) 工作的目的是開發一個單一的反欺騙系統，該系統可用於有效檢測 ASVspoof 2019 挑戰賽中考慮的所有類型的欺騙攻擊。 2) 深度伪造检测算法在公开数据集上的检测表现之性能上的評估 (LightCNN+RNN \混合光卷积和门递归单元)

```
The aim of this work is to develop a single anti-spoofing system which can be applied to effectively detect all the types of spoofing attacks considered in the ASVspoof 2019 Challenge: text-to-speech, voice conversion and replay based attacks. 

To achieve this, we propose the use of a Light Convolutional Gated Recurrent Neural Network (LC-GRNN) as a deep feature extractor to robustly represent speech signals as utterance-level embeddings, which are later used by a back-end recognizer which performs the final genuine/spoofed classification. 

This novel architecture combines the ability of light convolutional layers for extracting discriminative features at frame level with the capacity of gated recurrent unit based RNNs for learning long-term dependencies of the subsequent deep features. 

The proposed system has been presented as a contribution to the ASVspoof 2019 Challenge, and the results show a significant improvement in comparison with the baseline systems. 

Moreover, experiments were also carried out on the ASVspoof 2015 and 2017 corpora, and the results indicate that our proposal clearly outperforms other popular methods recently proposed and other similar deep feature based systems.
```

7. Chen T, Kumar A, Nagarsheth P, Sivaraman G, Khoury E. Generalization of audio Deepfake detection. In: Proc. of the Odyssey 2020 Speaker and Language Recognition Workshop. 2020. 132−137.

Link : https://www.isca-speech.org/archive_v0/Odyssey_2020/pdfs/29.pdf

Note : 深度伪造检测算法在公开数据集上的检测表现之性能上的評估 (Deep Residual Network + Frequency Masking \大边际距离 & 损失函数)

```
Audio Deepfakes, technically known as logical-access voice spoofing techniques, have become an increased threat on voice interfaces due to the recent breakthroughs in speech synthesis and voice conversion technologies.

Effectively detecting these attacks is critical to many speech applications including automatic speaker verification systems.

As new types of speech synthesis and voice conversion techniques are emerging rapidly, the generalization ability of spoofing countermeasures is becoming an increasingly critical challenge.

This paper focuses on overcoming this issue by using large margin cosine loss function (LMCL) and online frequency masking augmentation to force the neural network to learn more robust feature embeddings.

We evaluate the performance of the proposed system on the ASVspoof 2019 logical access (LA) dataset.

Additionally, we evaluate it on a noisy version of the ASVspoof 2019 dataset using publicly available noises to simulate more realistic scenarios.

Finally, we evaluate the proposed system on a copy of the dataset that is logically replayed through the telephony channel to simulate spoofing attacks in the call center scenario.

Our baseline system is based on residual neural network, and has achieved the lowest equal error rate (EER) of 4.04% among all single-system submissions during the ASVspoof
2019 challenge.

Furthermore, the additional improvements proposed in this paper reduce the EER to 1.26%.
```

8. Li R, Zhao M, Li Z, Li L, Hong Q. Anti-spoofing speaker verification system with multi-feature integration and multi-task learning. In: Proc. of the Interspeech. 2019. 1048−1052.

Link : https://www.researchgate.net/publication/335829363_Anti-Spoofing_Speaker_Verification_System_with_Multi-Feature_Integration_and_Multi-Task_Learning

Note : 深度伪造检测算法在公开数据集上的检测表现之性能上的評估 (Butterfly Unit + Multi-Task \多特征融合 & 多任务学习)

```
Speaker anti-spooﬁng is crucial to prevent security breacheswhen the speaker veriﬁcation systems encounter the spoofed at-tacks from the advanced speech synthesis algorithms and highﬁdelity replay devices. 

In this paper, we propose a frameworkbased on multiple features integration and multi-task learning(MFMT) for improving anti-spooﬁng performance. 

It is im-portant to integrate the complementary information of multi-ple spectral features within the network, such as MFCC, C-QCC, Fbank, etc., as often a single kind of feature is not e-nough to grasp the global spooﬁng cues and it generalizes poor-ly. 

Furthermore, we propose a helpful butterﬂy unit (BU) formulti-task learning to propagate the shared representations be-tween the binary decision task and the other auxiliary task.

The BU can obtain task representations of other branch during for-ward propagation and prevent the gradient from assimilating thebranch during back propagation.

Our proposed system yieldedan EER of 9.01% on ASVspoof 2017, while the best single sys-tem and the average scores fusion obtained the evaluation EERof 2.39% and 0.96% on ASVspoof 2019 PA, respectively.

Index Terms: multi-feature integration, multi-task learning,stitching layer, butterﬂy unit, anti-spooﬁng, speaker veriﬁcation
```

9. Goswami G, Ratha N, Agarwal A, Singh R, Vatsa M. Unravelling robustness of deep learning based face recognition against adversarial attacks. In: Proc. of the 32nd AAAI Conf. on Artificial Intelligence. 2018. 6829−6836.


Link : https://arxiv.org/abs/1803.00401

Note : 該研究發現對人臉圖片的遮擋和加噪，能夠在一定的程度上去欺騙人臉檢測器 VGGface 和 Openface

```
Deep neural network (DNN) architecture based models have high expressive power and learning capacity. 

However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. 

Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities.

In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: 

(i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; 

(ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and 

(iii) making corrections to the processing pipeline to alleviate the problem. 

Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. 

The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. 

Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition.
```

10. Parkhi OM, Vedaldi A, Zisserman A. Deep face recognition. In: Proc. of the British Machine Vision Conf. (BMVC). BMVA Press, 2015. 41.1−41.12.

Link : https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf

Note : VGGface

```
The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video.

Recent progress in this area has been due to two factors:

(i) end to end learning for the task using a convolutional neural network (CNN), and 

(ii) the availability of very large scale training datasets.

We make two contributions: 

first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human
in the loop, and discuss the trade off between data purity and time; 

second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.
```

11. Baltrušaitis T, Robinson P, Morency LP. Openface: An open source facial behavior analysis toolkit. In: Proc. of the IEEE Winter Conf. on Applications of Computer Vision (WACV). IEEE, 2016. 1−10.

Link : https://ieeexplore.ieee.org/document/7477553

Note : Openface

```
Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding.

We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis.

OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.

The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. 

Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware.

Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.
```

12. Li X, Ji S, Han M, Ji J, Ren Z, Liu Y, Wu C. Adversarial examples versus cloud-based detectors: A black-box empirical study. arXiv preprint arXiv:1901.01223, 2019.

Link : https://arxiv.org/abs/1901.01223

Note : 利用查询优化的方式对人脸图片进行加噪, 以此来绕过人脸识别引擎

```
Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. 

While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet.

In this paper, we mainly focus on the security issues of real-world cloud-based image detectors.

Specifically, 

(1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and 

(2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. 

Through the comprehensive evaluations on five major cloud platforms: 

AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100%, and the semantic segmentation based attacks have a success rate over 90% among different detection services, such as violence, politician, and pornography detection. 

We also proposed several possible defense strategies for these security challenges in the real-life situation.
```

13. Dong Y, Su H, Wu B, Li Z, Liu W, Zhang T, Zhu J. Efficient decision-based black-box adversarial attacks on face recognition. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2019. 7714−7722.

Link : https://arxiv.org/abs/1904.04433

Note : 利用查询优化的方式对人脸图片进行加噪, 以此来绕过人脸识别引擎

```
ace recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs).

However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes.

Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed.

In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model.

This attack setting is more practical in real-world face recognition systems.

To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometries of the search directions and reduce the dimension of the search space.

Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries.

We also apply the proposed method to attack a real-world face recognition system successfully.
```

14. Song Q, Wu Y, Yang L. Attacks on state-of-the-art face recognition using attentional adversarial attack generative network. arXiv preprint arXiv:1811.12026, 2018.

Link : https://arxiv.org/abs/1811.12026

Note : 使用注意力机制和生成对抗网络生成指定语义信息的假人脸,使得人脸识别器误判

```
With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked.

So, it is important to study how face recognition networks are subject to attacks.

In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously.

Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images.

For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces.

Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better.

The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.
```

15. Majumdar P, Agarwal A, Singh R, Vatsa M. Evading face recognition via partial tampering of faces. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops. 2019. 11−20.

Link : https://ieeexplore.ieee.org/abstract/document/9025546

Note : 研究发现:对人脸部分区域的修改和变形,可以让人脸识别器有很高的误识率.

```
Advancements in machine learning and deep learning techniques have led to the development of sophisticated and accurate face recognition systems.

However, for the past few years, researchers are exploring the vulnerabilities of these systems towards digital attacks.

Creation of digitally altered images has become an easy task with the availability of various image editing tools and mobile application such as Snapchat.

Morphing based digital attacks are used to elude and gain the identity of legitimate users by fooling the deep networks.

In this research, partial face tampering attack is proposed, where facial regions are replaced or morphed to generate tampered samples.

Face verification experiments performed using two state-of-the-art face recognition systems, VGG-Face and OpenFace on the CMU-MultiPIE dataset indicates the vulnerability of these systems towards the attack.

Further, a Partial Face Tampering Detection (PFTD) network is proposed for the detection of the proposed attack.

The network captures the inconsistencies among the original and tampered images by combining the raw and high-frequency information of the input images for the detection of tampered images.

The proposed network surpasses the performance of the existing baseline deep neural networks for tampered image detection.
```

16. Korshunov P, Marcel S. Vulnerability of face recognition to deep morphing. arXiv preprint arXiv:1910.01933, 2019.

Link : https://arxiv.org/abs/1910.01933

Note : 测试了基于 VGGnet 和 FaceNet 的人脸检测器的安全性,通过输入生成的 Deepfakes 影像,发现这两类人脸检测器分别有 85.62% 和 95.00% 的错误接受率,说明人脸检测器分辨不出深度伪造人脸和
源人脸. 

```
It is increasingly easy to automatically swap faces in images and video or morph two faces into one using generative adversarial networks (GANs).

The high quality of the resulted deep-morph raises the question of how vulnerable the current face recognition systems are to such fake images and videos.

It also calls for automated ways to detect these GAN-generated faces.

In this paper, we present the publicly available dataset of the Deepfake videos with faces morphed with a GAN-based algorithm.

To generate these videos, we used open source software based on GANs, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos.

We show that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to the deep morph videos, with 85.62 and 95.00 false acceptance rates, respectively, which means methods for detecting these videos are necessary.

We consider several baseline approaches for detecting deep morphs and find that the method based on visual quality metrics (often used in presentation attack detection domain) leads to the best performance with 8.97 equal error rate.

Our experiments demonstrate that GAN-generated deep morph videos are challenging for both face recognition systems and existing detection methods, and the further development of deep morphing technologies will make it even more so.
```

17. Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2015. 815−823.

Link : https://arxiv.org/abs/1503.03832

Note : FaceNet

```
Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. 

In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity.

Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.

Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.

To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method.

The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.

On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%.

Our system cuts the error rate in comparison to the best published result by 30% on both datasets.

We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.
```

18. Szegedy C, Zaremba W, Sutskever I, Bruna J. Intriguing properties of neural networks. In: Proc. of the 2nd Int’l Conf. on Leaning Representations (ICLR). 2014.

Link : https://arxiv.org/abs/1312.6199

Note : 神经网络本身存在着对抗样本攻击. 对抗样本攻击是一种对模型输入进行扰动,从而使模型产生误判的技术 !??

```
Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks.

While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties.

In this paper we report two such properties.

First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis.

It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.

Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend.

We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error.

In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
```

19. Goodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. In: Proc. of the 3rd Int’l Conf. on Leaning Representations (ICLR). 2015.

Link : https://arxiv.org/abs/1412.6572

Note : 神经网络本身存在着对抗样本攻击. 对抗样本攻击是一种对模型输入进行扰动,从而使模型产生误判的技术 !??

```
Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence.

Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. 

We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature.

This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets.

Moreover, this view yields a simple and fast method of generating adversarial examples.

Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.
```

20. Kurakin A, Goodfellow I, Bengio S. Adversarial examples in the physical world. In: Proc. of the 5th Int’l Conf. on Leaning Representations (ICLR) Workshop. 2017.

Link : https://arxiv.org/abs/1607.02533

Note : 神经网络本身存在着对抗样本攻击. 对抗样本攻击是一种对模型输入进行扰动,从而使模型产生误判的技术 !??

```
Most existing machine learning classifiers are highly vulnerable to adversarial examples.

An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. 

In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.

Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.

Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier.

This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input.

This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples.

We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system.

We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
```

21. Wang SY, Wang O, Zhang R, Owens A, Efros AA. CNN-generated images are surprisingly easy to spot for now. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020. 8692−8701.

Link : https://arxiv.org/abs/1912.11035

Note : 研究发现不同的 GAN 生成的伪造图像都留下特定的指纹特征,虽然依赖于指纹特征训练的检测器泛化能力不好,但是对训练数据进行预处理,如增加 JPEG 压缩、模糊等操作,大大提高模型的泛化性能,同时在检测时对图片进行后处理,可以增加模型的鲁棒性

```
In this work we ask whether it is possible to create a "universal" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used.

To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark).

We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2).

Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis.

Code and pre-trained networks are available at this https URL .

https://peterwang512.github.io/CNNDetection/
```

22. Neves JC, Tolosana R, Vera-Rodriguez R, Vera-Rodriguez R, Lopes V, Proena H, Fierrez J. Ganprintr: Improved fakes and evaluation of the state-of-the-art in face manipulation detection. IEEE Journal of Selected Topics in Signal Processing, 2020,14(5): 1038−1048.

Link : https://arxiv.org/abs/1911.05351

Note : 设计了一个自动编码器能够将合成的伪造图像移除指纹等信息,让现有的伪造检测系统失效

```
The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. 

Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios.

In this study, we focus on the synthesis of entire facial images, which is a specific type of facial manipulation.

The main contributions of this study are four-fold: 

i) a novel strategy to remove GAN "fingerprints" from synthetic fake images based on autoencoders is described, in order to spoof facial manipulation detection systems while keeping the visual quality of the resulting images;

ii) an in-depth analysis of the recent literature in facial manipulation detection;

iii) a complete experimental assessment of this type of facial manipulation, considering the state-of-the-art fake detection systems (based on holistic deep networks, steganalysis, and local artifacts), remarking how challenging is this task in unconstrained scenarios; and finally

iv) we announce a novel public database, named iFakeFaceDB, yielding from the application of our proposed GAN-fingerprint Removal approach (GANprintR) to already very realistic synthetic fake images.

The results obtained in our empirical evaluation show that additional efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques, such as the one proposed in this study.
```

23. Marra F, Gragnaniello D, Cozzolino D, Verdoliva L. Detection of GAN-generated fake images over social networks. In: Proc. of the IEEE Conf. on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2018. 384−389.

Link : https://ieeexplore.ieee.org/document/8397040

Note : 模拟了篡改图片在社交网络的场景中的检测,结果显示,现有的检测器在现实网络对抗环境下(未知压缩和未知类型等)表现很差

```
The diffusion of fake images and videos on social networks is a fast growing problem.

Commercial media editing tools allow anyone to remove, add, or clone people and objects, to generate fake images.

Many techniques have been proposed to detect such conventional fakes, but new attacks emerge by the day.

Image-to-image translation, based on generative adversarial networks (GANs), appears as one of the most dangerous, as it allows one to modify context and semantics of images in a very realistic way.

In this paper, we study the performance of several image forgery detectors against image-to-image translation, both in ideal conditions, and in the presence of compression, routinely performed upon uploading on social networks.

The study, carried out on a dataset of 36302 images, shows that detection accuracies up to 95% can be achieved by both conventional and deep learning detectors, but only the latter keep providing a high accuracy, up to 89%, on compressed data.
```

24. Zhang X, Karaman S, Chang SF. Detecting and simulating artifacts in GAN fake images. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). 2019. 1−6.

Link : https://arxiv.org/pdf/1907.06515.pdf

Note : 寻找 GAN 的共有痕迹,提高检测器的鲁棒性.现有的检测器对数据依赖强,泛化性不够

```
To detect GAN generated images, conventional supervised machine learning algorithms require collection of a number of real and fake images from the targeted GAN model.

However, the specific model used by the attacker is often unavailable.

To address this, we propose a GAN simulator, AutoGAN, which can simulate the artifacts produced by the common pipeline shared by several popular GAN models.

Additionally, we identify a unique artifact caused by the up-sampling component included in the common GAN pipeline.

We show theoretically such artifacts are manifested as replications of spectra in the frequency domain and thus propose a classifier model based on the spectrum input, rather than the pixel input.

By using the simulated images to train a spectrum based classifier, even without seeing the fake images produced by the targeted GAN model during training, our approach achieves state-of-the-art performances on detecting fake images generated by popular GAN models such as CycleGAN.
```

25. Du M, Pentyala S, Li Y, Hu X. Towards generalizable forgery detection with locality-aware autoencoder. arXiv preprint arXiv: 1909.05999, 2019.

Link : https://arxiv.org/abs/1909.05999

Note : 利用局部性感知的自动编码器实现造检测,使得模型聚焦篡改区域,通用性更强

```
With advancements of deep learning techniques, it is now possible to generate super-realistic images and videos, i.e., deepfakes.

These deepfakes could reach mass audience and result in adverse impacts on our society.

Although lots of efforts have been devoted to detect deepfakes, their performance drops significantly on previously unseen but related manipulations and the detection generalization capability remains a problem.

Motivated by the fine-grained nature and spatial locality characteristics of deepfakes, we propose Locality-Aware AutoEncoder (LAE) to bridge the generalization gap.

In the training process, we use a pixel-wise mask to regularize local interpretation of LAE to enforce the model to learn intrinsic representation from the forgery region, instead of capturing artifacts in the training set and learning superficial correlations to perform detection.

We further propose an active learning framework to select the challenging candidates for labeling, which requires human masks for less than 3% of the training data, dramatically reducing the annotation efforts to regularize interpretations.

Experimental results on three deepfake detection tasks indicate that LAE could focus on the forgery regions to make decisions.

The analysis further shows that LAE outperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on three deepfake detection tasks in terms of generalization accuracy on previously unseen manipulations.
```

26. Huang R, Fang F, Nguyen HH, Yamagishi J, Echizen I. Security of facial forensics models against adversarial attacks. arXiv preprint arXiv:1911.00660, 2019.

Link : https://arxiv.org/abs/1911.00660

Note : 借鉴了对抗样本的思想,对这些基于神经网络的检测器进行对抗性攻击,设计了单个对抗攻击和通用对抗攻击两种方式,使得检测器的篡改分类和定位失效.尽管现在已经存在众多的检测器,在一些数据集上表现很好,但是攻击者依然可以完善生成方法,隐藏一些标志性特征从而绕过检测器,这是一个长期的攻防博弈过程. 

```
Deep neural networks (DNNs) have been used in digital forensics to identify fake facial images.

We investigated several DNN-based forgery forensics models (FFMs) to examine whether they are secure against adversarial attacks.

We experimentally demonstrated the existence of individual adversarial perturbations (IAPs) and universal adversarial perturbations (UAPs) that can lead a well-performed FFM to misbehave.

Based on iterative procedure, gradient information is used to generate two kinds of IAPs that can be used to fabricate classification and segmentation outputs.

In contrast, UAPs are generated on the basis of over-firing.

We designed a new objective function that encourages neurons to over-fire, which makes UAP generation feasible even without using training data.

Experiments demonstrated the transferability of UAPs across unseen datasets and unseen FFMs.

Moreover, we conducted subjective assessment for imperceptibility of the adversarial perturbations, revealing that the crafted UAPs are visually negligible.

These findings provide a baseline for evaluating the adversarial security of FFMs.
```
27. Hall HK. Deepfake videos: When seeing isn’t believing. Catholic University Journal of Law and Technology, 2018,27(1):Article No.51.

Link : https://scholarship.law.edu/jlt/vol27/iss1/4/#:~:text=Videos%2C%20known%20as%20deepfakes%2C%20use,videos%20are%20fact%20or%20fiction.

Note : 深度伪造技术的发展给社会带来了巨大的负面影响,从社会国家领导人到普通的互联网公民,都有被此类技术侵害的可能性

```
Videos, known as deepfakes, use readily available software to create a work that shows people saying and doing things they may never have uttered or engaged in.

The technology making the videos appear very authentic is advancing at such a rate that people may not be able to detect if the videos are fact or fiction.

Given the hasty acceptance of other forms of fake news in society, deepfake videos have the ability to affect the nature of information the public receives about candidates and policies.

This study examines the potential use of deepfake videos in the democratic process, analyzes the challenges in regulating this area due to the First Amendment, questions the practicality of the marketplace of ideas metaphor in today’s news and information environment, and explores possible responses to the spread of deepfakes.
```

28. Hasan HR, Salah K. Combating deepfake videos using blockchain and smart contracts. IEEE Access, 2019,7:41596−41606.

Link : https://ieeexplore.ieee.org/document/8668407

Note : 尝试用区块链技术对互联网上的视频进行追踪

```
With the rise of artificial intelligence (AI) and deep learning techniques, fake digital contents have proliferated in recent years.

Fake footage, images, audios, and videos (known as deepfakes) can be a scary and dangerous phenomenon and can have the potential of altering the truth and eroding trust by giving false reality.

Proof of authenticity (PoA) of digital media is critical to help eradicate the epidemic of forged content.

Current solutions lack the ability to provide history tracking and provenance of digital media.

In this paper, we provide a solution and a general framework using Ethereum smart contracts to trace and track the provenance and history of digital content to its original source even if the digital content is copied multiple times.

The smart contract utilizes the hashes of the interplanetary file system (IPFS) used to store digital content and its metadata.

Our solution focuses on video content, but the solution framework provided in this paper is generic enough and can be applied to any other form of digital content.

Our solution relies on the principle that if the content can be credibly traced to a trusted or reputable source, the content can then be real and authentic.

The full code of the smart contract has been made publicly available at Github.
```

29. The law of California to Deepfake. 2019. https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730

Link : https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730

Note : 加州修法

```
AB 730, Berman. Elections: deceptive audio or visual media.

Existing law prohibits a person or specified entity from, with actual malice, producing, distributing, publishing, or broadcasting campaign material, as defined, that contains (1) a picture or photograph of a person or persons into which the image of a candidate for public office is superimposed or (2) a picture or photograph of a candidate for public office into which the image of another person or persons is superimposed, unless the campaign material contains a specified disclosure.
```

30. Regulations of China Internet Information Office on the control of online content. 2020 (in Chinese). http://www.cac.gov.cn/2019-12/20/c_1578375159509309.htm

Link : http://www.cac.gov.cn/2019-12/20/c_1578375159509309.htm

Note : 网络信息内容生态治理规定

```
国家互联网信息办公室令

第5号

　　《网络信息内容生态治理规定》已经国家互联网信息办公室室务会议审议通过，现予公布，自2020年3月1日起施行。 
...
```

31. Nataraj L, Mohammed TM, Chandrasekaran S, Flenner A, Bappy JH, Roy-Chowdhury AK, Manjunath BS. Detecting GAN generated fake images using co-occurrence matrices. Electronic Imaging, 2019,2019(5):532-1−532-7.

Link : 

Note : 

```

```

32. Li H, Li B, Tan S, Huang J. Identification of deep network generated images using disparities in color components. arXiv preprint arXiv:1808.07276, 2018.

Link : 

Note : 

```

```

33. Xuan X, Peng B, Wang W, Dong J. On the generalization of GAN image forensics. In: Proc. of the Chinese Conf. on Biometric Recognition. Cham: Springer-Verlag, 2019. 134−141.

Link : 

Note : 

```

```

34. McCloskey S, Albright M. Detecting GAN-generated imagery using color cues. arXiv preprint arXiv:1812.08247, 2018.

Link : 

Note : 

```

```

35. Marra F, Gragnaniello D, Verdoliva L, Poggi G. Do GANs leave artificial fingerprints? In: Proc. of the IEEE Conf. on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2019. 506−511.

Link : 

Note : 

```

```

36. Yu N, Davis LS, Fritz M. Attributing fake images to GANs: Learning and analyzing GAN fingerprints. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 7556−7566.

Link : 

Note : 

```

```

37. Wang R, Ma L, Juefei-Xu F, Xie X, Wang J, Liu Y. Fakespotter: A simple baseline for spotting ai-synthesized fake faces. In: Proc. of the 29th Int’l Joint Conf. on Artifical Intelligence (IJCAI). 2020. 3444−3451.

Link : 

Note : 

```

```

38. Chollet F. Xception: Deep learning with depthwise separable convolutions. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2017. 1251−1258.

Link : 

Note : 

```

```

39. Songsri-in K, Zafeiriou S. Complement face forensic detection and localization with faciallandmarks. arXiv preprint arXiv:1910. 05455, 2019.

Link : 

Note : 

```

```

40. Nguyen HH, Yamagishi J, Echizen I. Capsule-forensics: Using capsule networks to detect forged images and videos. In: Proc. of the IEEE Int’l Conf. on Acoustics, Speech and Signal Processing (ICASSP 2019). IEEE, 2019. 2307−2311.

Link : 

Note : 

```

```

41. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: Proc. of the 3rd Int’l Conf. on Learning Representations (ICLR). 2015.

Link : 

Note : 

```

```

42. Mo H, Chen B, Luo W. Fake faces identification via convolutional neural network. In: Proc. of the 6th ACM Workshop on Information Hiding and Multimedia Security. 2018. 43−47.

Link : 

Note : 

```

```

43. Durall R, Keuper M, Pfreundt FJ, Keuper J. Unmasking DeepFakes with simple features. arXiv preprint arXiv:1911.00686, 2019.

Link : 

Note : 

```

```

44. Ding X, Raziei Z, Larson EC, Olinick EV, Krueger PS, Hahsler M. Swapped face detection using deep learning and subjective assessment. EURASIP Journal on Information Security, 2020(2020):Article No.6.

Link : 

Note : 

```

```

45. Cozzolino D, Thies J, Rössler A, Riess C, Niebner M, Verdoliva L. Forensictransfer: Weakly-supervised domain adaptation for forgery detection. arXiv preprint arXiv:1812.02510, 2018.

Link : 

Note : 

```

```

46. Nguyen HH, Fang F, Yamagishi J, Echizen I. Multi-task learning for detecting and segmenting manipulated facial images and videos. arXiv preprint arXiv:1906.06876, 2019.

Link : 

Note : 

```

```

47. Hsu CC, Lee CY, Zhuang YX. Learning to detect fake face images in the wild. In: Proc. of the Int’l Symp. on Computer, Consumer and Control (IS3C). IEEE, 2018. 388−391.

Link : 

Note : 

```

```

48. Hsu CC, Zhuang YX, Lee CY. Deep fake image detection based on pairwise learning. Applied Sciences, 2020,10(1):Article No.370.

Link : 

Note : 

```

```

49. Dang LM, Hassan SI, Im S, Lee J, Lee S, Moon H. Deep learning based computer generated face identification using convolutional neural network. Applied Sciences, 2018,8(12):Article No.2610.

Link : 

Note : 

```

```

50. Bayar B, Stamm MC. A deep learning approach to universal image manipulation detection using a new convolutional layer. In: Proc. of the 4th ACM Workshop on Information Hiding and Multimedia Security. 2016. 5−10.

Link : 

Note : 

```

```

51. Dang H, Liu F, Stehouwer J, Liu X, Jain A. On the detection of digital face manipulation. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020. 5780−5789.

Link : 

Note : 

```

```

52. Rahmouni N, Nozick V, Yamagishi J, Echizen I. Distinguishing computer graphics from natural images using convolution neural networks. In: Proc. of the IEEE Workshop on Information Forensics and Security (WIFS). IEEE, 2017. 1−6.

Link : 

Note : 

```

```

53. Li X, Yu K, Ji S, Wang Y, Wu C, Xue H. Fighting against Deepfake: Patch&Pair convolutional neural networks (PPCNN). In: Proc. of the Companion Web Conf. 2020. 2020. 88−89.

Link : 

Note : 

```

```

54. Brockschmidt J, Shang J, Wu J. On the generality of facial forgery detection. In: Proc. of the IEEE 16th Int’l Conf. on Mobile Ad Hoc and Sensor Systems Workshops (MASSW). IEEE, 2019. 43−47.

Link : 

Note : 

```

```

55. Sohrawardi SJ, Chintha A, Thai B, Seng S, Hickerson A, Ptucha R, Wright M. Poster: Towards robust open-world detection of Deepfakes. In: Proc. of the ACM SIGSAC Conf. on Computer and Communications Security. 2019. 2613−2615.

Link : 

Note : 

```

```

56. Agarwal S, Farid H, Gu Y, He M, Nagano K, Li H. Protecting world leaders against deep fakes. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops. 2019. 38−45.

Link : 

Note : 

```

```

57. Amerini I, Galteri L, Caldelli R, Bimbo AD. Deepfake video detection through optical flow based CNN. In: Proc. of the IEEE Int’l Conf. on Computer Vision Workshops. 2019. 1205−1207.

Link : 

Note : 

```

```

58. Güera D, Delp EJ. Deepfake video detection using recurrent neural networks. In: Proc. of the 15th IEEE Int’l Conf. on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2018. 1−6.

Link : 

Note : 

```

```

59. Sabir E, Cheng J, Jaiswal A, AbdAlmageed W, Masi I, Natarajan P. Recurrent convolutional strategies for face manipulation detection in videos. arXiv preprint arXiv:1905.00582, 2019.

Link : 

Note : 

```

```

60. Todisco M, Delgado H, Evans NWD. A new feature for automatic speaker verification anti-spoofing: Constant Q cepstral coefficients. In: Proc. of the Odyssey. 2016. 283−290.

Link : 

Note : 

```

```

61. Rössler A, Cozzolino D, Verdoliva L, Christian R, Justus T, Matthias N. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179, 2018.

Link : 

Note : 

```

```

62. Rossler A, Cozzolino D, Verdoliva L, Riess C, Thies J, Niessner M. Faceforensics++: Learning to detect manipulated facial images. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 1−11.

Link : 

Note : 

```

```

63. Korshunov P, Marcel S. Deepfakes: A new threat to face recognition? Assessment and detection. arXiv preprint arXiv:1812.08685, 2018.

Link : 

Note : 

```

```

64. VidTIMIT. 2019. http://conradsanderson.id.au/vidtimit/

Link : 

Note : 

```

```

65. Afchar D, Nozick V, Yamagishi J, Echizen I. Mesonet: A compact facial video forgery detection network. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). IEEE, 2018. 1−7.

Link : 

Note : 

```

```

66. Li Y, Yang X, Sun P, Qi H, Lyu S. Celeb-DF: A new dataset for Deepfake forensics. arXiv preprint arXiv:1909.12962, 2019.

Link : 

Note : 

```

```

67. DeepfakeDetection. 2019. https://github.com/ondyari/FaceForensics

Link : 

Note : 

```

```

68. Dolhansky B, Howes R, Pflaum B, Baram N, Ferrer C. The Deepfake detection challenge (DFDC) preview dataset. arXiv preprint arXiv:1910.08854, 2019.

Link : 

Note : 

```

```

69. DFDC. 2020. https://www.kaggle.com/c/deepfake-detection-challenge/data

Link : 

Note : 

```

```

70. Jiang L, Li R, Wu W, Qian C, Loy C. DeeperForensics-1.0: A large-scale dataset for real-world face forgery detection. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020. 2886−2895.

Link : 

Note : 

```

```

71. ASVspoof 2015 database. 2020. https://datashare.is.ed.ac.uk/handle/10283/853

Link : 

Note : 

```

```

72. ASVspoof 2019 database. 2020. https://datashare.is.ed.ac.uk/handle/10283/3336

Link : 

Note : 

```

```

73. Abu-El-Haija S, Kothari N, Lee J, Natsev P, Toderici G, Varadarajan B, Vijayanarasimhan S. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.

Link : 

Note : 

```

```

74. Amerini I, Ballan L, Caldelli R, Bimbo AD, Serra G. A sift-based forensic method for copy-move attack detection and transformation recovery. IEEE Trans. on Information Forensics and Security, 2011,6(3):1099−1110.

Link : 

Note : 

```

```

75. De Carvalho TJ, Riess C, Angelopoulou E, Pedrini H, Rocha A. Exposing digital image forgeries by illumination color classification. IEEE Trans. on Information Forensics and Security, 2013,8(7):1182−1194.

Link : 

Note : 

```

```

76. Lukáš J, Fridrich J, Goljan M. Detecting digital image forgeries using sensor pattern noise. In: Proc. of the Security, Steganography, and Watermarking of Multimedia Contents VIII, Vol.6072. Int’l Society for Optics and Photonics, 2006.

Link : 

Note : 

```

```

77. Chierchia G, Parrilli S, Poggi G, Verdoliva L, Sansone C. PRNU-based detection of small-size image forgeries. In: Proc. of the 17th Int’l Conf. on Digital Signal Processing (DSP). IEEE, 2011. 1−6. 

Link : 

Note : 

```

```

78. Fridrich J, Kodovsky J. Rich models for steganalysis of digital images. IEEE Trans. on Information Forensics and Security, 2012, 7(3):868−882.

Link : 

Note : 

```

```

79. Wang W, Dong J, Tan T. Exploring DCT coefficient quantization effects for local tampering detection. IEEE Trans. on Information Forensics and Security, 2014,9(10):1653−1666.

Link : 

Note : 

```

```

80. Nataraj L, Sarkar A, Manjunath BS. Adding gaussian noise to “denoise” JPEG for detecting image resizing. In: Proc. of the 16th IEEE Int’l Conf. on Image Processing (ICIP). IEEE, 2009. 1493−1496.

Link : 

Note : 

```

```

81. Bianchi T, De Rosa A, Piva A. Improved DCT coefficient analysis for forgery localization in JPEG images. In: Proc. of the IEEE Int’l Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011. 2444−2447.

Link : 

Note : 

```

```

82. Pan X, Zhang X, Lyu S. Exposing image splicing with inconsistent local noise variances. In: Proc. of the IEEE Int’l Conf. on Computational Photography (ICCP). IEEE, 2012. 1−10.

Link : 

Note : 

```

```

83. Ferrara P, Bianchi T, De Rosa A, Piva A. Image forgery localization via fine-grained analysis of CFA artifacts. IEEE Trans. on Information Forensics and Security, 2012,7(5):1566−1577.

Link : 

Note : 

```

```

84. Cozzolino D, Verdoliva L. Noiseprint: A CNN-based camera model fingerprint. IEEE Trans. on Information Forensics and Security, 2019,15:144−159.

Link : 

Note : 

```

```

85. Zhou P, Han X, Morariu VI, Davis LS. Learning rich features for image manipulation detection. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2018. 1053−1061.

Link : 

Note : 

```

```

86. Rao Y, Ni J. A deep learning approach to detection of splicing and copy-move forgeries in images. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). IEEE, 2016. 1−6.

Link : 

Note : 

```

```

87. Liu B, Pun CM. Deep fusion network for splicing forgery localization. In: Proc. of the European Conf. on Computer Vision (ECCV). 2018. 237−251.

Link : 

Note : 

```

```

88. Huh M, Liu A, Owens A, Efros A. Fighting fake news: Image splice detection via learned self-consistency. In: Proc. of the European Conf. on Computer Vision (ECCV). 2018. 101−117.

Link : 

Note : 

```

```

89. Cun X, Pun CM. Image splicing localization via semi-global network and fully connected conditional random fields. In: Proc. of the European Conf. on Computer Vision (ECCV). 2018. 252−266.

Link : 

Note : 

```

```

90. Cozzolino D, Poggi G, Verdoliva L. Recasting residual-based local descriptors as convolutional neural networks: An application to image forgery detection. In: Proc. of the 5th ACM Workshop on Information Hiding and Multimedia Security. 2017. 159−164.

Link : 

Note : 

```

```

91. Chen C, McCloskey S, Yu J. Focus manipulation detection via photometric histogram analysis. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2018. 1674−1682.

Link : 

Note : 

```

```

92. Zhou P, Han X, Morariu VI, Davis LS. Two-stream neural networks for tampered face detection. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE, 2017. 1831−1839.

Link : 

Note : 

```

```

93. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2015. 1−9.

Link : 

Note : 

```

```

94. Yang X, Li Y, Lyu S. Exposing deep fakes using inconsistent head poses. In: Proc. of the IEEE Int’l Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019. 8261−8265.

Link : 

Note : 

```

```

95. Yang X, Li Y, Qi H, Lyu S. Exposing GAN-synthesized faces using landmark locations. In: Proc. of the ACM Workshop on Information Hiding and Multimedia Security. 2019. 113−118.

Link : 

Note : 

```

```

96. Li Y, Chang MC, Lyu S. In ictu oculi: Exposing AI created fake videos by detecting eye blinking. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). IEEE, 2018. 1−7.

Link : 

Note : 

```

```

97. Ciftci UA, Demir I. FakeCatcher: Detection of synthetic portrait videos using biological signals. arXiv preprint arXiv:1901.02212, 2019.

Link : 

Note : 

```

```

98. Fernandes S, Raj S, Ortiz E, Vintila I, Salter M, Urosevic G, Jha S. Predicting heart rate variations of Deepfake videos using neural ODE. In: Proc. of the IEEE Int’l Conf. on Computer Vision Workshops. 2019. 1721−1729.

Link : 

Note : 

```

```

99. Li Y, Lyu S. Exposing Deepfake videos by detecting face warping artifacts. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops, 2019. 46−52.

Link : 

Note : 

```

```

100. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2016. 770−778. 

Link : 

Note : 

```

```

101. Deepfakes. 2019. https://github.com/deepfakes/faceswap

Link : 

Note : 

```

```

102. Zao app. 2019. https://zao-app.com/

Link : 

Note : 

```

```

103. Deepfake detection challenge. 2020. https://www.kaggle.com/c/deepfake-detection-challenge

Link : 

Note : 

```

```

104. Girish N, Nandini C. A review on digital video forgery detection techniques in cyber forensics. Science, Technology and Development, 2019,3(6):235−239.

Link : 

Note : 

```

```

105. Nguyen TT, Nguyen CM, Nguyen DT, Nguyen DT, Nahavandi S. Deep learning for Deepfakes creation and detection. arXiv preprint arXiv:1909.11573, 2019.

Link : 

Note : 

```

```

106. Zollhöfer M, Thies J, Garrido P, Bradley D, Beeler T, Perez P, Stamminger M, Niessner M, Theobalt C. State of the art on monocular 3D face reconstruction, tracking, and applications. Computer Graphics Forum, 2018,37(2):523−550.

Link : 

Note : 

```

```

107. FaceSwap. 2019. https://github.com/MarekKowalski/FaceSwap/

Link : 

Note : 

```

```

108. Dale K, Sunkavalli K, Johnson MK, Vlasic D, Matusik W, Pfister H. Video face replacement. In: Proc. of the SIGGRAPH Asia Conf. 2011. 1−10.

Link : 

Note : 

```

```

109. Garrido P, Valgaerts L, Rehmsen O, Thormae T, Perez P, Theobalt C. Automatic face reenactment. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2014. 4217−4224.

Link : 

Note : 

```

```

110. Garrido P, Valgaerts L, Sarmadi H, Steiner I, Varanasi K, Perez P, Theobalt C. VDub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. Computer Graphics Forum, 2015,34(2):193−204.

Link : 

Note : 

```

```

111. Nirkin Y, Masi I, Tuan AT, Hassner T, Medioni G. On face segmentation, face swapping, and face perception. In: Proc. of the 13th IEEE Int’l Conf. on Automatic Face and Gesture Recognition (FG 2018). IEEE, 2018. 98−105.

Link : 

Note : 

```

```

112. Lu Z, Li Z, Cao J, He R, Sun Z. Recent progress of face image synthesis. In: Proc. of the 4th IAPR Asian Conf. on Pattern Recognition (ACPR). IEEE, 2017. 7−12.

Link : 

Note : 

```

```

113. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courvile A, Bengio Y. Generative adversarial nets. In: Proc. of the Advances in Neural Information Processing Systems. 2014. 2672−2680.

Link : 

Note : 

```

```

114. Faceswap-GAN. 2019. https://github.com/shaoanlu/faceswap-GAN

Link : 

Note : 

```

```

115. Korshunova I, Shi W, Dambre J, Theis L. Fast face-swap using convolutional neural networks. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2017. 3677−3685.

Link : 

Note : 

```

```

116. Nirkin Y, Keller Y, Hassner T. FSGAN: Subject agnostic face swapping and reenactment. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 7184−7193.

Link : 

Note : 

```

```

117. Choi Y, Choi M, Kim M, Ha J, Kin S, Choo J. StarGAN: Unified generative adversarial networks for multi-domain image-toimage translation. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2018. 8789−8797.

Link : 

Note : 

```

```

118. Zhang H, Xu T, Li H, Zhang S, Wang X, Huang X, Netaxas D. StackGAN++: Realistic image synthesis with stacked generative adversarial networks. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2018,41(8):1947−1962.

Link : 

Note : 

```

```

119. Karras T, Aila T, Laine S, Lehtinen J. Progressive growing of GANs for improved quality, stability, and variation. In: Proc. of the 6th Int’l Conf. on Learning Representations (ICLR). 2018.

Link : 

Note : 

```

```

120. Antipov G, Baccouche M, Dugelay JL. Face aging with conditional generative adversarial networks. In: Proc. of the IEEE Int’l Conf. on Image Processing (ICIP). IEEE, 2017. 2089−2093.

Link : 

Note : 

```

```

121. Mirza M, Osindero S. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.

Link : 

Note : 

```

```

122. Huang R, Zhang S, Li T, He R. Beyond face rotation: Global and local perception GAN for photorealistic and identity preserving frontal view synthesis. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2017. 2439−2448.

Link : 

Note : 

```

```

123. Thies J, Zollhöfer M, Nießner M, Valgaerts L, Stamminger M, Theobalt C. Real-time expression transfer for facial reenactment. ACM Trans. on Graphics (TOG), 2015,34(6):Article No.183.

Link : 

Note : 

```

```

124. Thies J, Zollhofer M, Stamminger M, Theobalt C, Niebner M. Face2face: Real-time face capture and reenactment of RGB videos. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2016. 2387−2395.

Link : 

Note : 

```

```

125. Thies J, Zollhöfer M, Theobalt C, Stamminger M, Niessner M. Headon: Real-time reenactment of human portrait videos. ACM Trans. on Graphics (TOG), 2018,37(4):1−13.

Link : 

Note : 

```

```

126. Kim H, Garrido P, Tewari A, Xu W, Thies J, Niessner M, Perez P, Richardt C, Zollhofer M, Theobalt C. Deep video portraits. ACM Trans. on Graphics (TOG), 2018,37(4):1−14.

Link : 

Note : 

```

```

127. Thies J, Zollhöfer M, Nießner M. Deferred neural rendering: Image synthesis using neural textures. ACM Trans. on Graphics (TOG), 2019,38(4):1−12.

Link : 

Note : 

```

```

128. Suwajanakorn S, Seitz SM, Kemelmacher-Shlizerman I. Synthesizing Obama: Learning lip sync from audio. ACM Trans. on Graphics (TOG), 2017,36(4):1−13.

Link : 

Note : 

```

```

129. Zakharov E, Shysheya A, Burkov E, Lempitsky V. Few-shot adversarial learning of realistic neural talking head models. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 9459−9468.

Link : 

Note : 

```

```

130. Fried O, Tewari A, Zollhöfer M, Finkelstein A, Shechtman E, Goldman D, Genova K, Jin Z, Theobalt C, Agrawala M. Text-based editing of talking-head video. ACM Trans. on Graphics (TOG), 2019,38(4):1−14.

Link : 

Note : 

```

```

131. Averbuch-Elor H, Cohen-Or D, Kopf J, Cohen M. Bringing portraits to life. ACM Trans. on Graphics (TOG), 2017,36(6):Article No.196.

Link : 

Note : 

```

```

132. Lample G, Zeghidour N, Usunier N, Bordes A, Denoyer L, Ranzato M. Fader networks: Manipulating images by sliding attributes. In: Proc. of the Advances in Neural Information Processing Systems. 2017. 5967−5976. 

Link : 

Note : 

```

```

133. Van Den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, Senior AW, Kavukcuoglu K. Wavenet: A generative model for raw audio. In: Proc. of the 9th Speech Synthesis Workshop. 2016.

Link : 

Note : 

```

```

134. Arik S, Chrzanowski M, Coates A, Diamos G, Kang Y, Li X, Miller J, Ng A, Raiman J, Sengupta S, Shoeybi M. Deep voice: Real-time neural text-to-speech. In: Proc. of the 34th Int’l Conf. on Machine Learning. 2017. 195−204.

Link : 

Note : 

```

```

135. Wang Y, Skerry-Ryan RJ, Stanton D, Wu Y, Weiss R, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S, Le Q, Agiomyrgiannakis Y, Clark B, Saurous R. Tacotron: Towards end-to-end speech synthesis. In: Proc. of the Interspeech 2017, 18th Annual Conf. of the Int’l Speech Communication Association. 2017. 4006−4010.

Link : 

Note : 

```

```

136. Arik S, Diamos G, Gibiansky A, Miller J, Peng K, Ping W, Raiman J, Zhou Y. Deep voice 2: Multi-speaker neural text-to-speech. In: Proc. of the Advances in Neural Information Processing Systems. 2017. 2962−2970.

Link : 

Note : 

```

```

137. Ping W, Peng K, Gibiansky A, Arik S, Kannan A, Narang S. Deep voice 3: 2000-speaker neural text-to-speech. In: Proc. of the ICLR. 2018. 214−217.

Link : 

Note : 

```

```

138. Pascual S, Bonafonte A, Serra J. SEGAN: Speech enhancement generative adversarial network. In: Proc. of the Interspeech 2017, 18th Annual Conf. of the Int’l Speech Communication Association. 2017. 3642−3646.

Link : 

Note : 

```

```

139. Donahue C, McAuley J, Puckette M. Adversarial audio synthesis. In: Proc. of the 7th Int’l Conf. on Learning Representations (ICLR). 2019.

Link : 

Note : 

```

```

140. Li XR, Yu K. A Deepfakes detection technique based on two-stream network. Journal of Cyber Security, 2020,5(2):84−91 (in Chinese with English abstract).

Link : 

Note : 

```

```

141. FakeApp. 2019. https://www.deepfakescn.com

Link : 

Note : 

```

```

142. Faceapp. 2019. https://www.faceapp.com/

Link : 

Note : 

```

```

143. DeepFaceLab. 2019. https://github.com/iperov/DeepFaceLab

Link : 

Note : 

```

```

144. Dfaker. 2019. https://github.com/dfaker/df

Link : 

Note : 

```

```

145. DeepFake-tf. 2019. https://github.com/StromWine/DeepFake-tf

Link : 

Note : 

```

```

146. Faceswap-Deepfake-Pytorch. 2019. https://github.com/Oldpan/Faceswap-Deepfake-Pytorch

Link : 

Note : 

```

```

147. Deep-voice-conversion. 2020. https://github.com/andabi/deep-voice-conversion

Link : 

Note : 

```

```

148. MelNet. 2020. https://sjvasquez.github.io/blog/melnet/

Link : 

Note : 

```

```

149. Matern F, Riess C, Stamminger M. Exploiting visual artifacts to expose Deepfakes and face manipulations. In: Proc. of the IEEE Winter Applications of Computer Vision Workshops (WACVW). IEEE, 2019. 83−92.

Link : 

Note : 

```

```


