# DMSASD - 数字媒体软件与系统开发 - Digital Media Software And System Development

> 2101212850 干皓丞

PKU 2022 個人實驗報告作業

## Details

Arsenal 中文素材

## Survey & New Works

0. Deep Learning for Deepfakes Creation and Detection: A Survey

https://www.researchgate.net/publication/336055871_Deep_Learning_for_Deepfakes_Creation_and_Detection_A_Survey

有持續更新，但是該版本是早期的版本。

- list1 - 105 ,2022 v4, https://arxiv.org/pdf/1909.11573.pdf

```
Deep learning has been successfully applied to solvevarious complex problems ranging from big data analytics tocomputer vision and human-level control. 

Deep learning advanceshowever have also been employed to create software that cancause threats to privacy, democracy and national security. 

Oneof those deep learning-powered applications recently emergedis “deepfake”. 

Deepfake algorithms can create fake images andvideos that humans cannot distinguish them from authenticones.

The proposal of technologies that can automatically detectand assess the integrity of digital visual media is thereforeindispensable.

This paper presents a survey of algorithms usedto create deepfakes and, more importantly, methods proposed todetect deepfakes in the literature to date.

We present extensivediscussions on challenges, research trends and directions relatedto deepfake technologies.

By reviewing the background of deep-fakes and state-of-the-art deepfake detection methods, this studyprovides a comprehensive overview of deepfake techniques andfacilitates the development of new and more robust methods todeal with the increasingly challenging deepfakes.
```

深度學習已成功應用於解決從大數據分析到計算機視覺和人類水平控制的各種複雜問題，其技術也可能會被用於創造出可能對隱私、民主體制和國家安全造成威脅的軟體應用。 而近來出現的基於深度學習的應用程序則是 “deepfake”，其演算法可以做出人類用肉眼無法將它們與真品區分開來的假影像和影片。因此，擁有能夠自動檢測和評估數位視覺媒體完整性等技術的提議是必不可少的。本研究介紹了用於創造深度偽造的演算法的調查，更重要的是，介紹了迄今為止文獻中提出的用於檢測深度偽造的方法。同時對與深度偽造技術相關的挑戰、研究趨勢和方向進行了廣泛的討論。通過回顧深造假的背景和最先進的深造假檢測方法，本研究提供了深造假技術的全面概述，並有助於開發新的、更強大的方法來應對日益具有挑戰性的深度偽造局面。

Bibliography

```
@unknown{unknown,
author = {Nguyen, Thanh and Nguyen, Cuong M. and Nguyen, Tien and Nguyen, Duc and Nahavandi, Saeid},
year = {2019},
month = {09},
pages = {},
title = {Deep Learning for Deepfakes Creation and Detection: A Survey}
}
```

> 疑似與某篇一致


1. Deepfakes Detection Techniques Using Deep Learning: A Survey

Almars, A. M. (2021). Deepfakes detection techniques using deep learning: a survey. Journal of Computer and Communications, 9(5), 20-35.

https://www.scirp.org/journal/paperinformation.aspx?paperid=109149

```
Deep learning is an effective and useful technique that has been widely applied in a variety of fields, including computer vision, machine vision, and natural language processing. 

Deepfakes uses deep learning technology to manipulate images and videos of a person that humans cannot differentiate them from the real one. 

In recent years, many studies have been conducted to understand how deepfakes work and many approaches based on deep learning have been introduced to detect deepfakes videos or images. 

In this paper, we conduct a comprehensive review of deepfakes creation and detection technologies using deep learning approaches. 

In addition, we give a thorough analysis of various technologies and their application in deepfakes detection.

Our study will be beneficial for researchers in this field as it will cover the recent state-of-art methods that discover deepfakes videos or images in social contents.

In addition, it will help comparison with the existing works because of the detailed description of the latest methods and dataset used in this domain.
```

深度學習是一種有效且已廣泛應用於各種領域的有用技術，其領域包括計算機視覺、機器視覺和自然語言處理，Deepfakes 則是使用深度學習技術來處理人類無法將其與真人區分開來的人的圖像和影片。近年來，已經有許多研究來了解深度偽造的工作原理，並且引入了許多基於深度學習的方法來檢測深度偽造的影像或圖像。在該研究中，研究者使用深度學習方法對深度偽造創建和檢測技術進行了全面審視，同時對各種技術及其在深度偽造檢測中的應用進行了深入分析，其研究成果將對該領域的研究人員有所幫助，因為它將涵蓋最近發現社交內容中的深度偽造影像或圖像的最先進的方法。此外，由於對該領域使用的最新方法和資料集的詳細描述，它將有助於與現有工作進行比較。

Bibliography

```
@article{almars2021deepfakes,
  title={Deepfakes detection techniques using deep learning: a survey},
  author={Almars, Abdulqader M},
  journal={Journal of Computer and Communications},
  volume={9},
  number={5},
  pages={20--35},
  year={2021},
  publisher={Scientific Research Publishing}
}
```


2. M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection

Wang, J., Wu, Z., Chen, J., & Jiang, Y. G. (2021). M2tr: Multi-modal multi-scale transformers for deepfake detection. arXiv preprint arXiv:2104.09770.

https://arxiv.org/abs/2104.09770

```
The widespread dissemination of forged images generated by Deepfake techniques has posed a serious threat to the trustworthiness of digital information. 

This demands effective approaches that can detect perceptually convincing Deepfakes generated by advanced manipulation techniques. 

Most existing approaches combat Deepfakes with deep neural networks by mapping the input image to a binary prediction without capturing the consistency among different pixels. 

In this paper, we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection. 

We achieve this with transformer models, which have recently demonstrated superior performance in modeling dependencies between pixels for a variety of recognition tasks in computer vision. 

In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale transformer that operates on patches of different sizes to detect the local inconsistency at different spatial levels. 

To improve the detection results and enhance the robustness of our method to image compression, M2TR also takes frequency information, which is further combined with RGB features using a cross modality fusion module. 

Developing and evaluating Deepfake detection methods requires large-scale datasets. However, we observe that samples in existing benchmarks contain severe artifacts and lack diversity. 

This motivates us to introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. 

On three Deepfake datasets, we conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods.
```

Deepfake 技術所產生的偽造圖像廣泛傳播對數位資訊的可信度構成了嚴重威脅，這需要有效的方法來檢測由先進技術所生成具有感知力的 Deepfake 成果。大多數現有方法通過將輸入圖像對應到二進制預測而不捕獲不同像素之間的一致性來使用深度神經網絡來對抗 Deepfakes 技術。在該研究中，研究者旨在為 Deepfake 檢測捕獲不同尺度的細微操作偽影，並通過轉換器模型實現了這一點，該模型最近在為計算機視覺中的各種識別任務建模像素之間的依賴關係方面表現出卓越的性能。同時研究者介紹了一種多模態多尺度變換器（M2TR），它使用多尺度變換器對不同大小的補丁進行操作，以檢測不同空間級別的局部不一致性，為了改善檢測結果並增強我們方法對圖像壓縮的魯棒性，M2TR 還獲取頻率信息，並使用交叉模態融合模塊將其與 RGB 特徵進一步結合。開發和評估 Deepfake 檢測方法需要大規模的數據集。此研究觀察到現有基準中的樣本包含嚴重的偽影並且缺乏多樣性，這促使此研究引入了一個高品質的 Deepfake 資料集 SR-DF，它由 4,000 個由最先進的面部交換和面部重演方法去生成的 DeepFake 組成影像，最後在三個 Deepfake 資料集上，研究者進行了廣泛的實驗來驗證所提出方法的有效性，該方法優於最先進的 Deepfake 檢測方法。

Bibliography

```
@article{wang2021m2tr,
  title={M2tr: Multi-modal multi-scale transformers for deepfake detection},
  author={Wang, Junke and Wu, Zuxuan and Chen, Jingjing and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2104.09770},
  year={2021}
}
```


3. Combining EfficientNet and Vision Transformers for Video Deepfake Detection

Coccomini, D., Messina, N., Gennaro, C., & Falchi, F. (2021). Combining efficientnet and vision transformers for video deepfake detection. arXiv preprint arXiv:2107.02612.

https://arxiv.org/abs/2107.02612

```
Deepfakes are the result of digital manipulation to forge realistic yet fake imagery. 

With the astonishing advances in deep generative models, fake images or videos are nowadays obtained using variational autoencoders (VAEs) or Generative Adversarial Networks (GANs). 

These technologies are becoming more accessible and accurate, resulting in fake videos that are very difficult to be detected. 

Traditionally, Convolutional Neural Networks (CNNs) have been used to perform video deepfake detection, with the best results obtained using methods based on EfficientNet B7. 

In this study, we focus on video deep fake detection on faces, given that most methods are becoming extremely accurate in the generation of realistic human faces. 

Specifically, we combine various types of Vision Transformers with a convolutional EfficientNet B0 used as a feature extractor, obtaining comparable results with some very recent methods that use Vision Transformers. 

Differently from the state-of-the-art approaches, we use neither distillation nor ensemble methods. 

Furthermore, we present a straightforward inference procedure based on a simple voting scheme for handling multiple faces in the same video shot. 

The best model achieved an AUC of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the DeepFake Detection Challenge (DFDC).
```

Deepfakes 是用數位的方式去操作跟偽造出逼真但虛假圖像的結果，而隨著深度生成模型的驚人進步，現在可以使用變分自動編碼器 (VAE) 或生成對抗網絡 (GAN) 兩種技術去獲得虛假圖像或影片，而這些技術變得越來越容易獲得和準確，導致很難檢測到假影像。

傳統上，卷積神經網絡 (CNN) 已被用於執行影像深度偽造檢測，使用基於 EfficientNet B7 的方法獲得了最佳結果，在此項研究中，研究者專注於人臉的視頻深度偽造檢測，因為大多數方法在生成逼真的人臉方面變得非常準確。

具體來說，該研究將各種類型的視覺轉換器與用作特徵提取器的卷積 EfficientNet B0 相結合，同時與一些使用視覺轉換器的最新方法獲得了可比較的結果。而本研究最先進的方法不同，研究者既不使用蒸餾也不使用集成方法。研究者也提出了一個基於簡單投票方案的簡單推理程序，用於處理同一影像鏡頭中的多個人臉。同時最佳模型的 AUC 為 0.951，F1 得分為 88.0%，非常接近 DeepFake 檢測挑戰賽 (DFDC) 的最新水平。

Bibliography

```
@article{coccomini2021combining,
  title={Combining efficientnet and vision transformers for video deepfake detection},
  author={Coccomini, Davide and Messina, Nicola and Gennaro, Claudio and Falchi, Fabrizio},
  journal={arXiv preprint arXiv:2107.02612},
  year={2021}
}
```


4. Video Transformer for Deepfake Detection with Incremental Learning

Khan, S. A., & Dai, H. (2021, October). Video Transformer for Deepfake Detection with Incremental Learning. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 1821-1828).

https://dl.acm.org/doi/abs/10.1145/3474085.3475332?sid=SCITRUS

```
Face forgery by deepfake is widely spread over the internet and this raises severe societal concerns. 

In this paper, we propose a novel video transformer with incremental learning for detecting deepfake videos. 

To better align the input face images, we use a 3D face reconstruction method to generate UV texture from a single input face image. 

The aligned face image can also provide pose, eyes blink and mouth movement information that cannot be perceived in the UV texture image, so we use both face images and their UV texture maps to extract the image features. 

We present an incremental learning strategy to fine-tune the proposed model on a smaller amount of data and achieve better deepfake detection performance. 

The comprehensive experiments on various public deepfake datasets demonstrate that the proposed video transformer model with incremental learning achieves state-of-the-art performance in the deepfake video detection task with enhanced feature learning from the sequenced data.
```

Deepfake 的面部偽造在互聯網上廣泛傳播，這引起了嚴重的社會擔憂，在該研究中，研究者提出了一種具有增量學習功能的新型影像轉換器，用於檢測深度偽造影像，而為了更好地對齊輸入人臉圖像，我們使用 3D 人臉重建方法從單個輸入人臉圖像生成 UV 紋理。其對齊的人臉圖像還可以提供在 UV 紋理圖像中無法感知的姿勢、眨眼和嘴巴運動資訊，因此研究者使用人臉圖像及其 UV 紋理圖來提取圖像特徵。最後研究者提出了一種增量學習策略，可以在更少量的資料上微調所提出的模型，並實現更好的深度偽造檢測性能。對各種公共 deepfake 數據集的綜合實驗表明，所提出的具有增量學習的視頻轉換器模型通過對序列數據的增強特徵學習，在 deepfake 視頻檢測任務中實現了最先進的性能。

Bibliography

```
@inproceedings{khan2021video,
  title={Video Transformer for Deepfake Detection with Incremental Learning},
  author={Khan, Sohail Ahmed and Dai, Hang},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={1821--1828},
  year={2021}
}
```

5. Deep Alignment Network: A convolutional neural network for robust face alignment

https://www.researchgate.net/publication/317378240_Deep_Alignment_Network_A_convolutional_neural_network_for_robust_face_alignment

```
In this paper, we propose Deep Alignment Network (DAN), a robust face alignment method based on a deep neural network architecture. 

DAN consists of multiple stages, where each stage improves the locations of the facial landmarks estimated by the previous stage. 

Our method uses entire face images at all stages, contrary to the recently proposed face alignment methods that rely on local patches. 

This is possible thanks to the use of landmark heatmaps which provide visual information about landmark locations estimated at the previous stages of the algorithm. 

The use of entire face images rather than patches allows DAN to handle face images with large variation in head pose and difficult initializations. 

An extensive evaluation on two publicly available datasets shows that DAN reduces the state-of-the-art failure rate by up to 70%. 

Our method has also been submitted for evaluation as part of the Menpo challenge.
```

該研究提出了深度對齊網絡（DAN），這是一種基於深度神經網絡架構的魯棒人臉對齊方法，其 DAN 由多個階段組成，每個階段都改進了前一階段估計的面部標誌的位置。而研究者們的方法在所有階段都使用整個人臉圖像，這與最近提出的依賴局部補丁的人臉對齊方法相反。這要歸功於地標熱圖的使用，它提供了在算法的前一階段估計的地標位置的視覺信息。使用整個人臉圖像而不是補丁允許 DAN 處理頭部姿勢變化很大且初始化困難的人臉圖像。對兩個公開可用數據集的廣泛評估表明，DAN 可將最先進的故障率降低多達 70%。


6. A deep learning approach to face swap detection

http://injoit.org/index.php/j1/article/viewFile/1152/1131


7. AdaBins Depth Estimation using Adaptive Bins

> adabins & CVPR 21

https://arxiv.org/abs/2011.14141

8. DFDT: An End-to-End DeepFake Detection Framework Using Vision Transformer

Aminollah Khormali and Jiann-Shiun Yuan



## Laws

```
法律思維與制度的智慧轉型, 李建良．劉靜怡．邱文聰．吳全峰．陳弘儒．陳柏良．何之行．廖貞．黃相博．林勤富．李怡俐．楊岳平．鄭瑞健．沈宗倫．王怡蘋
```
Link : https://www.angle.com.tw/book.asp?BKID=12196

0. 人工智慧時代的法學研究路徑初探，劉靜怡

人工智慧的應用,是最近十年來日漸受到瞩目的趨势。這也使得应該如何討諭人工智慧和機器學習影響力及其所衍生之倫理與法律議题的研究需求,日益追切。本文將以人工智慧應有的治理架構與法學研究路為探討核心,嘗試描出人工智慧與機器學習對於法學研究所形成的挑戰圖像。

關鍵詞:人工智慧、資料驅動、機器學習、禁止授權原則、正當法律程序

1. 第二波人工智慧知識學習與生產對法學的挑戰 — 資訊、科技與社會研究及法學的對話，邱文聰

常今人工智還欠缺自我意與意向性而不能成為法律權利主體,也尚無跨領域適用之能力,但「強/弱AI」或「通用/限定AI」的區分，已不足以描述人工智慧從1950年代演化至今的轉變。科技與社研究學者對於人類如何學習、傳遞知識與專業知能的相關研究成果，是理解人工智慧是否能真正取代人類智能的重要切入點。本文考察 Dreyfus 與 Collins 兩位重要學者相關理論,指出只要機器習的實作先例涵蓋夠廣,各種身體性默知識及集體社會性默會知識 就能被第二波人工智慧的學習模式所涵化,與人工智慧是否已具有流利的自然語言能力無關。

真正的挑戰在於,依照不同機器學習方法,人工智慧既可能承襲默會知識中的積習與可能的不理性與錯,也可能引發人類利用尚無法為現有知識所驗證的預測能力以進行各種控制的慾望。以規範論証為核心活動的法學 則至少在三個面向上可能面臨第二波人工智能的模仿或參與而遭受衝擊:在外部證立上創造新的差異製造事實、模仿規範論證行為、模仿法律效果裁量行為:法學也將在「效率」、「法與社會的典範轉移」與「控制」三條軸線上，不斷地進行自我辦證。

關鍵詞:機器學習、監督式學習、非監督式學習、直覺、默會知識、專業知能、規範論證、涵攝、內部證立、外部證立

Bibliography

```
@Article{law01,
title = {第二波人工智慧知識學習與生產對法學的挑戰 — 資訊、科技與社會研究及法學的對話},
author = {邱文聰},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


2. 初探人工智慧與生命倫理之關係，吳全峰

人工智慧(artificial intelligence, Al)在醫療産業之應用,不論被视為協助完善醫療專業人員判斷之工具·或被視為可能取代醫師並自主執行診斷與治療之醫療機器人 (medical robots)，均可能引起醫療本質之改變，甚至影響生命倫理之價值；

但相關事議往往被視為傳统生命倫理議題之延伸或變形，並未得到應之重視與討論。本文主張醫療人工智慧之影響 - 尤其是醫療機器人之發展 - 已超過傳統生命倫理架構所預設處理之議題範園，甚至可能涉及社會對醫病關係之重新定義，並分別從自主(autonomy)、受益 (beneticence)、正義(justice)、隱私(privacy)等面向出發，分析生命倫理與機器倫理間、醫療專業與資訊專業間、專業人員與病人間之價值衡突,並重新思考生命倫理在醫療人工智慧年代所應具備之規範內容。

關鍵詞:人工智、醫療機器人、生命倫理

Bibliography

```
@Article{law02,
title = {初探人工智慧與生命倫理之關係},
author = {吳全峰},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


3. 初探目的解釋在法律人工智慧系統之運用可能，陳弘儒

本文說明法律推理的計算機模型在設計上的可能困境。

文章指出,在目的被給定的條件下,雖然可能運用CMLRs進行目的推論，但仍舊無法消除規範性的質疑,因為有限範圍之目的推論是對於未來可能結果之預測，因此，可能世界如何與目的之規範事態相符合是一個關鍵問題。此外，本文直視「目的如何具有規範性」的議題,並指出法律推理的不確定性。在實踐層面，則透過「目的」之概念分析為 CMLRs 提供哲學反省，釐清其運用界線以及邊界問題。

關鍵詞 :人工智慧與法律、目的解釋、法律推理的電腦模型、規範性、工具理性

Bibliography

```
@Article{law03,
title = {初探目的解釋在法律人工智慧系統之運用可能},
author = {陳弘儒},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


4. AI 時代之分裂社會與民主 — 以美國法之表意自由與觀念市場自由競爭理論為中心，陳柏良

美國聯邦最高法院自 1920 年代起所形塑之之表意自由權,建立於三個法社會基礎:一、資訊稀缺性:二、閱聽者具備充分的資訊審議時間；三、政府是意見自由競爭市場的主要威脅。

然自 1990 年代起·随著 AI 與網路科技發展及資訊產業結構變遷,言論與閱聽者關注的相對稀缺關係，出現結構性地逆轉。廉價言論的大量出現，造成資訊氾濫 (information flood) 的現象、閱聽者關注産業 (attention industry) 崛起，雖能協助使用者快過濾資訊, 但其詳細蒐集並處理使用者習慣後，呈現閱聽人所好之資訊，也產生過濾氣泡現象(同溫層效應)。依此，個體在虛擬空間中的各小群體內，習於接受特定來源含特定值觀之訊息，進而造成社會共識分歧與政治社群碎片化,使人民對民主產生系統性信任危機。

本文認為:若從保障閱聽人的角度出發, 得在特定脈絡下, 肯認 AI 的演算輸出,屬於機器表意，受到憲法表意自由權保障。然為避免公民之政治偏好與世界觀形成過程，遭到大量且真偽難辨之資訊所曲，應立法強制揭露 AI 為機器表意之發言者身分與來源，提供閱聽者辨識資訊之初步線索，以保障閱聽者的資訊辨識利益，確保民主憲政秩序中的核心元素: 公民政治偏好的形成與審議程序，不致在動態的資訊流動結構中遭到系統性扭曲，以促進社會多元發展，鞏固民主價值!

關鍵詞:社會變遷與法律、人工智慧(AI)、資訊社育、廉價言論、表意自由權、觀念市場自由競爭理論、審議式民主、共和主義圖像

Bibliography

```
@Article{law04,
title = {AI 時代之分裂社會與民主 — 以美國法之表意自由與觀念市場自由競爭理論為中心},
author = {陳柏良},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


5. AI 個資爭議在英國與歐盟之經驗 — 以Google DeepMind 一案為例，何之行、廖貞

本文分析英國國家健保局(NHS)下轄之 Royl Free 公立醫院於 2016 年 1 月起薪由身 Google DecpMnd 公司所簽署之
資料分享協議·移轉超過 160 萬未去識別化之病患個資于 Google DeepMind 作為急性腎衰竭診断軟體開發之用，從而引發個資保護之爭議。文章首先剖析 Google DeepMind 一案之案例事及背景，而探討人工智演算法所仰賴大量資料之應用與個資保護間文衝突。最後則藉由 Google DeepMind 一案，進一步分析甫於 2018 年 5 月施行之歐盟一般資料保護規則(EUGeneral Data Protection Regulation, GDPR)，其就巨量資料發展所訂定關於科學例外條款及拒絕自動化決策之適用範圍及其限制，盼能由 Google DeepMind 一案之探討，就展人工慧醫療領域所可能面臨之相關個資保護議題以為參酌和借鏡。

關鍵詞: Google DeepMind、人工智能 (AI)、英國資訊委員辦公室 (ICO)、歐盟一般資料保護規則 (GDPR) 科學例外條款 (Scientific Exemptions)

Note : Google 醫療體系與歐盟法規之間發生的問題

Bibliography

```
@Article{law05,
title = {AI 個資爭議在英國與歐盟之經驗 — 以Google DeepMind 一案為例},
author = {何之行、廖貞},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


6. 人工智慧在金融業的應用—論數位金融與一般個人資料保護規則之適用與衝突，黃相博

由於數位科技的發展，對個人資料保護帶來新的挑戰，一方面，因為數位化而產生巨大商業契機，另一方面又要保護數位化的個人資料，兩者密不可分的緊張關係，有必要制定一個法律框架，以確保在個人資料主權與使用個人資料的合法經濟利益之間保持一個適當的平衡。歐盟針對金融機構所發布第二號支付服務指令，有助於降低第三方支付服務提供者進入金融市場的成本，讓更多人得以享受到便利的金融服務。但在 2018 年實施的一般個人資料保護規則，在某些個人資料保護核心關鍵點上的要求，會比第二號支付服務指令更嚴格。而且一般個人資料保護規則是以拥有資料主權的客戶角度進行規範，無疑地對於個人資料有更周全的保護。但是，其中某些新增賦予的當事人權利，例如匿名化、被遺忘及拒絕權等，在金融服務上恐難一體適用，而有「原則適用」，「法定例外排除」之必要。因此，在接受金融服務時，個人資料保護必须有某個程度的退讓，以求取在個人資料保護與金融服務間之平衡。

關鍵詞:數位金融、普惠金融、開放銀行、個資主權、第二號支付服務指令、一般個人資料保護規則

Bibliography

```
@Article{law06,
title = {人工智慧在金融業的應用 — 論數位金融與一般個人資料保護規則之適用與衝突},
author = {黃相博},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


7. 人工智慧時代下的國際人權法 ─ 規範與制度的韌性探索與再建構，林勤富、李怡俐

隨著數據分析、計算能力和機器學習的快速發展,以人工智慧(artificial intelligence)或複雑演算法為基礎的預測、分類或決策系統越來越受各國政府機關運用(如社會福利部門、執法單位和法院等)以協助甚至取代公部門的日常功能，或制定得影響人民基本權利與義務之決策。如此公部門的「演算法化」(algorithmization) 趨势，涉及諸多社會、經濟、政治與法律層面之制度選擇與基本人權問題,往往引起高度關注與爭議。例如，由於複雜演算法系統缺乏透明性以及對弱勢族群之系統性偏見，美國數州刑事司法系統所使用之 COMPAS 風險評估工具演算法即引起若干法律辦論，在 State v. Loomis 案中威斯康辛州最高法院認為，即使 COMPAS 的演算法受營業秘密保護而從未對外公開，初審法院據其報告所探納之量刑決定未侵害被告正當法律程序的憲法權利。然，此判決顯示刑事司法程序越發依賴欠缺透明性的演算法系統時，可能带來刑享基本人權有被犧牲或剝奪的疑慮。此外，中國政府近年所建立之社會信用系统，亦有以人工智慧與其他新興科技強化其社會控制與個人監控的色彩，對言論自由、隱私保護和反歧視等基本人權甚至法治原則造成嚴重侵害。

同樣地，私部門亦大量應用人工智慧以提升業務産能與效率,如 Googl蒐集分析個人數據、網頁搜尋及座標歴史等資料，投放客製化廣告及資訊；又如 Facebook 過濾新聞源與分析使用者行為的社交生態系統，或中國芝麻信用以評分系统提供貸款與電子商務優惠等。然而，這些私部門人工智慧系統之運作亦可能引發歧視、隱私侵害及分配正義等隱憂。人工智慧發展與應用上之潛在人爭議亦引起聯合國人權機構之關注。聯合國人權事務高級專具辨事處、聯合國人權理事會和聯合國言論自由權特別調查事務官接連提出相關報告，警示 AI 運用可能造成各種人權侵犯現象，特别是深化歧視或侵害隱私與言論自由等權利，有鑑此國际法層次之發展，本文試圖探討國際人權規範與機制在人工智慧規範議題上的治理功能與潜在挑戰本文認為，當代國際人權規範框架與機制擁有諸多治理功能，能在人工智慧所衍生之人權議題上作為具國際共識的風險評估與權利害認定具體標準，

明確建立公、私部門人權保障之義務，更能作為審議平台以提供不同利害关际者對话与參与之共通语言。基此，當前國際人權法的規範框架與機制係回應人工智慧相關人權優害之有力治理工具。
然而，本文亦發現此國際公認之法律框架仍有其內在規範缺陷與不足，如缺乏承認數位權利之積極規範變遷,以及過於以國家為中心之治理局限。

為回應人工智慧科技對國際人規範所帶來的新一代衝擊與挑戰，本文認為有權解釋的相關人權機制或司法機關除可引入區域人權法院近年的創新解釋方法外，亦應適當擁抱並承認新型態的人權類型與內涵，蓋唯有重新形塑與理解國際人權法的規範形式與內涵，方足以因應人工智慧時代下科技所带來的各項挑戰。

關鍵詞:人工智慧、演算法、國際人權法、全球治理、跨國風險管制

Bibliography

```
@Article{law07,
title = {人工智慧時代下的國際人權法 ─ 規範與制度的韌性探索與再建構},
author = {林勤富、李怡俐},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


8. 人工智慧時代下的金融監理議題 ─ 以理財機器人監理為例，楊岳平

理財機器人利用人工智慧取代傳統理財專員提供投資顧問服務，衍生出不同於傳統證期投顧服務的金融消費者保護與金融系统性風險的監管考量，同時也需要不同的監理模式以確保監管的有效執行，因此對我國的證券投資願問与期貨投資顧問法制帶來一定挑戰。本文以人工智慧的特性為基礎，指出理財機器人業者需要有別於現行證期投顧法制的内部组織要求、技術測試要求、受托人責任追究重點、資本市場穩定控管的監理法制，並建議在理財機器人發展初期，透過加強外部稽核的方式，循序漸進強化對理財機器人的監管。

關鍵詞:人工智慧、理財機器人、演算法、投資顧問、普惠金融

Bibliography

```
@Article{law08,
title = {人工智慧時代下的金融監理議題 ─ 以理財機器人監理為例},
author = {楊岳平},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


9. 人工智慧時代下的證券監理 ─ 以智能合約在區塊鏈技術的應用出發，鄭瑞健

由中本聰最早所發布的區塊白皮書論之，區塊鏈應用技術的主要功能在於借重其分散式帳本的功能解決雙重消費的難題，以建立比特幣的虚擬通貨體系。
後人在其基礎上不斷創新，甚至結合智能合約的功能，讓區塊鏈技術的應用提升至另一層次:例如創造証券型代幣以為投資工具、然此賦予區塊無限可能的應用技術卻很可能因智能合約自身的設計缺陷，而讓不可逆且無法竄改的區塊技術產生破口; 2016 年著名之 DAO 事件即為適例，此促使了美國聯邦證券監督管理委員於 2017 年發布 DAO 報告，以 1946 年聯邦最高法院作成之 Howey Test 檢視購買 DAO 代幣是否為投資契約，遂正式將此議題檯面化，復引發學術界和實務界的熱烈討論與持續關注。

關鍵詞:人工智慧、區塊鏈、智能合約、Howey測試、投資契約、初次代幣發行、證券代幣發行、證券監理

Bibliography

```
@Article{law09,
title = {人工智慧時代下的證券監理 ─ 以智能合約在區塊鏈技術的應用出發},
author = {鄭瑞健},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


10. 人工智慧科技對於專利侵權法制的衝擊與因應之道 ─ 以責任歸屬為中心，沈宗倫

人工智慧在一定的自主性，與人類共同執行相關產業活動，若有專利侵權發生，若不承認人工智慧為責任主體，且人工智慧之侵權行為乃人類參與者不可預見時，則縱使專利侵權由人工智慧所促成，且人類參與者確由專利侵權下獲致專利權價值之支配，將產生無人須對侵權結果負之情形，亦牴觸專利法之立法價值。本文是以現行專利法與民法侵權行為相關規範出發，探討人類使用人工智慧執行產業事務時，一旦有專利侵權行為發生，人類對於人工智慧之行為的可歸責問题。本文嘗試由歸責主體以及注意義之內容與範圍出發，就可歸責問題詳細論述。希望能提出一符合人工智慧科技屬性，及專利法與侵權行為相關法理之歸責模式，解決現行法下人工智慧參與專利權之責任屬爭議。

關鍵詞:人工智慧、專利侵權、可預見性、專利權價值、責任歸屬、全要件原則

Bibliography

```
@Article{law10,
title = {人工智慧科技對於專利侵權法制的衝擊與因應之道 ─ 以責任歸屬為中心},
author = {沈宗倫},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```


11. 人工智慧創作與著作權之相關問題，王怡蘋

近年來人工智慧的科技發展突飛猛進，研究團隊致力於將其運用於各種領域，其中一項則是發展得以獨立創作的人工智慧。然而人工智慧獨立創作之成果是否受到保護，卻挑戰著著作權法的基本原則，即著作權法保護人類精神創作。本文嘗試從著作權法的立法精神出發，探討以著作權法保護人工智慧創作之可能性，並在此基礎之上，進一步討論保護制度之設計。其次是訓練人工智慧創作需先建置學習資料庫，而建置過程需要將非數位資料數位，並將數化資料儲存於資料庫中，此種行為均屬於著作權法意義下之重製，而應得著作權人之同意，由此行生出的問題在於此行為在評價上是否屬於權利限制之範疇。最後則是人工智慧可能產生與既有創作雷同之創作，此種現象是否應該避免，防免義務與義務人應如何建構,亦有討論之必要。

關鍵詞:人類精神創作、人工智慧創作、學習資料庫、權利限制、抄襲、自我審查系統

Bibliography

```
@Article{law11,
title = {人工智慧創作與著作權之相關問題},
author = {王怡蘋},
 year = {2021},
 month = {11},
 publisher = {元照出版公司}
}
```

## lists 1.

0. Li XR, Ji SL, Wu CM, Liu ZG, Deng SG, Cheng P, Yang M, Kong XW. Survey on deepfakes and detection techniques. Ruan Jian Xue Bao/Journal of Software, 2021,32(2):496−518 (in Chinese). http://www.jos.org.cn/1000-9825/6140.htm 

Link : http://www.jos.org.cn/josen/article/abstract/6140

Note : `*` 深度偽造與檢測綜述

```
Deep learning has achieved great success in the field of computer vision, surpassing many traditional methods. 

However, in recent years, deep learning technology has been abused in the production of fake videos, making fake videos represented by Deepfakes flooding on the Internet. 

This technique produces pornographic movies, fake news, political rumors by tampering or replacing the face information of the original videos and synthesizes fake speech. 

In order to eliminate the negative effects brought by such forgery technologies, many researchers have conducted in-depth research on the identification of fake videos and proposed a series of detection methods to help institutions or communities to identify such fake videos. 

Nevertheless, the current detection technology still has many limitations such as specific distribution data, specific compression ratio, and so on, far behind the generation technology of fake video. 

In addition, different researchers handle the problem from different angles. 

The data sets and evaluation indicators used are not uniform.

So far, the academic community still lacks a unified understanding of deep forgery and detection technology. 

The architecture of deep forgery and detection technology research is not clear.

In this review, the development of deep forgery and detection technologies are reviewed.

Besides, existing research works are systematically summarize and scientifically classified.

Finally, the social risks posed by the spread of Deepfakes technology are discussed, the limitations of detection technology are analyzed, and the challenges and potential research directions of detection technology are discussed, aiming to provide guidance for follow-up researchers to further promote the development and deployment of Deepfakes detection technology. 
```

雖然深度學習在計算機視覺領域超越了許多傳統方法取得了巨大成功，但近年深度學習技術被濫用於製作假影像與圖像等多媒體領域，使得以 Deepfakes 為代表的假影片在網絡上氾濫成災，該技術通過篡改或替換原始影像的的面部資訊並合成虛假語音資料。其成果被用來產生色情電影散佈、假新聞、政治謠言等非法用途，而為了消除此類偽造技術帶來的負面影響，許多研究人員對假視頻的識別進行了深入研究，並提出了一系列檢測方法，以幫助機構或社區識別此類假影片。儘管如此，目前的檢測技術仍存在特定分佈資料、特定壓縮比等諸多限制，遠遠落後於假影片的生成技術。

同時不同的研究人員也從不同的角度處理這個問題，但各方使用的資料集和評價指標並不統一，至目前為止，多數人對深度偽造和檢測技術仍然缺乏統一的認識且深度偽造和檢測技術研究的架構尚不清楚。在這篇綜述中，對深度偽造和檢測技術的發展進行了綜述，並對現有研究工作進行了系統總結和科學分類。最後，討論了 Deepfakes 技術傳播帶來的社會風險，分析了檢測技術的局限性，探討了檢測技術面臨的挑戰和潛在的研究方向，旨在為後續研究人員進一步推動發展提供指導。並部署 Deepfakes 檢測技術。

Bibliography

```
@Article{2021496,
title = {深度伪造与检测技术综述},
author = {李旭嵘,纪守领,吴春明,刘振广,邓水光,程鹏,杨珉,孔祥维},
 journal = {软件学报},
 volume = {32},
 number = {2},
 pages = {496},
 numpages = {23.0000},
 year = {2021},
 month = {},
 doi = {10.13328/j.cnki.jos.006140},
 publisher = {科学出版社}
}
```

1. Wu Z, Kinnunen T, Chng ES, Li H, Ambikairajah E. A study on spoofing attack in state-of-the-art speaker verification: The telephone speech case. In: Proc. of the Asia Pacific Signal and Information Processing Association Annual Summit and Conf. IEEE, 2012. 1−5.

Link : https://ieeexplore.ieee.org/document/6411897

Note : 深度偽造語音檢測，Wu 等人提出的歸一化的余弦相位和修改的群延遲

```
Voice conversion technique, which modifies one speaker's (source) voice to sound like another speaker (target), presents a threat to automatic speaker verification. 

In this paper, we first present new results of evaluating the vulnerability of current state-of-the-art speaker verification systems: Gaussian mixture model with joint factor analysis (GMM-JFA) and probabilistic linear discriminant analysis (PLDA) systems, against spoofing attacks. 

The spoofing attacks are simulated by two voice conversion techniques: Gaussian mixture model based conversion and unit selection based conversion. 

To reduce false acceptance rate caused by spoofing attack, we propose a general anti-spoofing attack framework for the speaker verification systems, where a converted speech detector is adopted as a post-processing module for the speaker verification system's acceptance decision. 

The detector decides whether the accepted claim is human speech or converted speech. 

A subset of the core task in the NIST SRE 2006 corpus is used to evaluate the vulnerability of speaker verification system and the performance of converted speech detector. 

The results indicate that both conversion techniques can increase the false acceptance rate of GMM-JFA and PLDA system, while the converted speech detector can reduce the false acceptance rate from 31.54% and 41.25% to 1.64% and 1.71% for GMM-JFA and PLDA system on unit-selection based converted speech, respectively.
```

語音轉換技術將一個說話者的聲音修改為聽起來像另一個說話者，這對自動說話者驗證功能構成了威脅。該研究首先展示了評估當前最先進的說話人驗證系統的脆弱性的新結果：具有聯合因子分析 (GMM-JFA) 和概率線性判別分析 (PLDA) 系統的高斯混合模型，以防止欺騙攻擊。而所謂的欺騙攻擊則是通過兩種語音轉換技術模擬：基於高斯混合模型的轉換和基於單元選擇的轉換。為了降低由欺騙攻擊引起的錯誤接受率，研究者提出了一種用於說話人驗證系統的通用反欺騙攻擊框架，其中採用轉換後的語音檢測器作為說話人驗證系統接受決策的後處理模塊，其檢測器決定接受的聲明是人類語音還是轉換後的語音。NIST SRE 2006 語料庫中的核心任務子集用於評估說話人驗證系統的脆弱性和轉換後的語音檢測器的性能。其研究結果表明，兩種轉換技術都可以提高 GMM-JFA 和 PLDA 系統的誤認率，而轉換後的語音檢測器可以將 GMM-JFA 和 PLDA 的誤認率從 31.54% 和 41.25% 降低到 1.64% 和 1.71%基於單元選擇的轉換語音系統。

Bibliography

```
@inproceedings{wu2012study,
  title={A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case},
  author={Wu, Zhizheng and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou and Ambikairajah, Eliathamby},
  booktitle={Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference},
  pages={1--5},
  year={2012},
  organization={IEEE}
}
```


2. Wu Z, Chng ES, Li H. Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition. In: Proc. of the 13th Annual Conf. of the Int’l Speech Communication Association. 2012. 1700−1703.

Link : https://www.researchgate.net/publication/260343013_Detecting_Converted_Speech_and_Natural_Speech_for_anti-Spoofing_Attack_in_Speaker_Recognition

Note : 深度偽造語音檢測，Wu 等人提出的歸一化的余弦相位和修改的群延遲

```
Voice conversion techniques present a threat to speaker verification systems. 

To enhance the security of speaker verification systems, We study how to automat-ically distinguish natural speech and synthetic/converted speech. 

Motivated by the research on phase spectrum in speech perception, in this study, we propose to use fea-tures derived from phase spectrum to detect converted speech. 

The features are tested under three different train-ing situations of the converted speech detector: a) only Gaussian mixture model (GMM) based converted speech data are available; b) only unit-selection based converted speech data are available; c) no converted speech data are available for training converted speech model. 

Experi-ments conducted on the National Institute of Standards and Technology (NIST) 2006 speaker recognition evalu-ation (SRE) corpus show that the performance of the fea-tures derived from phase spectrum outperform the mel-frequency cepstral coefficients (MFCCs) tremendously: even without converted speech for training, the equal er-ror rate (EER) is reduced from 20.20% of MFCCs to 2.35%.
```

語音轉換技術對說話人驗證系統構成威脅，此研究為了增強說話人驗證系統的安全性，研究者進行如何自動區分自然語音和合成/轉換後的語音。受語音感知中相位譜研究的啟發，在該研究中，研究者建議使用從相位譜中獲得的特徵來檢測轉換後的語音。這些特徵在轉換後的語音檢測器的三種不同訓練情況下進行測試： a) 只有基於高斯混合模型 (GMM) 的轉換後的語音數據可用； b) 只有基於單元選擇的轉換語音數據可用； c) 沒有轉換後的語音數據可用於訓練轉換後的語音模型。而在美國國家標準與技術研究院 (NIST) 2006 說話人識別評估 (SRE) 語料庫上進行的實驗表明，從相位譜派生的特徵的性能大大優於梅爾頻率倒譜係數 (MFCC)：即使沒有經過轉換的語音進行訓練，等錯誤率 (EER) 從 MFCC 的 20.20% 降低到 2.35%。

Bibliography

```
@inproceedings{wu2012detecting,
  title={Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition},
  author={Wu, Zhizheng and Chng, Eng Siong and Li, Haizhou},
  booktitle={Thirteenth Annual Conference of the International Speech Communication Association},
  year={2012}
}
```


3. Das RK, Yang J, Li H. Long range acoustic and deep features perspective on ASVspoof 2019. In: Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019. 1018−1025.

Das, R. K., Yang, J., & Li, H. (2019, December). Long range acoustic and deep features perspective on ASVspoof 2019. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) (pp. 1018-1025). IEEE.

Link : https://ieeexplore.ieee.org/document/9003845

Note : 1) ASVspoof2019 資料集，2) 從遠程聲學和深度特徵的角度總結了欺騙檢測的發現且對不同類型的欺騙攻擊的性質和系統開發進行了綜合分析。

```
To secure automatic speaker verification (ASV) systems from intruders, robust countermeasures for spoofing attack detection are required.

The ASVspoof series of challenge provides a shared anti-spoofing task.

The recent edition, ASVspoof 2019, focuses on attacks by both synthetic and replay speech that are referred to as logical and physical access attacks, respectively.

In the ASVspoof 2019 submission, we considered novel countermeasures based on long range acoustic features, that are unique in many ways as they are derived using octave power spectrum and subbands, as opposed to the commonly used linear power spectrum.

During the post-challenge study, we further investigate the use of deep features that enhances the discriminative ability between genuine and spoofed speech.

In this paper, we summarize the findings from the perspective of long range acoustic and deep features for spoof detection.

We make a comprehensive analysis on the nature of different kinds of spoofing attacks and system development.
```

為了保護自動說話者驗證 (ASV) 系統免受入侵者的侵害，需要針對欺騙攻擊檢測採取穩健的對策，而 ASVspoof 系列挑戰提供了一個共享的反欺騙工作。其中最近的版本 ASVspoof 2019 側重於合成語音和重放語音的攻擊，分別稱為邏輯訪問攻擊和物理訪問攻擊，在 ASVspoof 2019 提交的論文中，研究者考慮了基於遠程聲學特徵的新對策，這些對策在許多方面都是獨一無二的，因為它們是使用倍頻程功率譜和子帶得出的，而不是常用的線性功率譜，在挑戰後研究中，研究者進一步研究了使用深度特徵來增強真實和欺騙性語音之間的區分能力。而該研究在從遠程聲學和深度特徵的角度總結了欺騙檢測的發現，並從不同類型的欺騙攻擊的性質和系統開發進行了綜合分析。

Bibliography

```
@inproceedings{das2019long,
  title={Long range acoustic and deep features perspective on ASVspoof 2019},
  author={Das, Rohan Kumar and Yang, Jichen and Li, Haizhou},
  booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={1018--1025},
  year={2019},
  organization={IEEE}
}
```


4. Zeinali H, Stafylakis T, Athanasopoulou G, Rohdin J, Gkinis I, Burget L, Cernocky JH. Detecting spoofing attacks using VGG and SincNet: BUT-Omilia submission to ASVspoof 2019 challenge. In: Proc. of the 20th Annual Conf. of the Int’l Speech Communication Association. 2019. 1073−1077.

Zeinali, H., Stafylakis, T., Athanasopoulou, G., Rohdin, J., Gkinis, I., Burget, L., & Černocký, J. (2019). Detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge. arXiv preprint arXiv:1907.12908.

Link : https://arxiv.org/abs/1907.12908

Note : 應用不同架構來應對攻擊

Tag : CVPR

```
In this paper, we present the system description of the joint efforts of Brno University of Technology (BUT) and Omilia - Conversational Intelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge.

The primary submission for Physical access (PA) is a fusion of two VGG networks, trained on single and two-channels features.

For Logical access (LA), our primary system is a fusion of VGG and the recently introduced SincNet architecture.

The results on PA show that the proposed networks yield very competitive performance in all conditions and achieved 86\:\% relative improvement compared to the official baseline.

On the other hand, the results on LA showed that although the proposed architecture and training strategy performs very well on certain spoofing attacks, it fails to generalize to certain attacks that are unseen during training.

```

該研究介紹了布爾諾理工大學 (BUT) 和 Omilia 共同努力的系統描述 — ASVSpoof2019 Spoofing and Countermeasures Challenge 的對話智能，其物理訪問（PA）的主要提交是兩個 VGG 網絡的融合，並在單通道和雙通道特徵上進行了訓練。對於邏輯訪問 (LA)，研究者的主要系統是 VGG 和最近引入的 SincNet 架構的融合，其 PA 上的結果表明，所提出的網絡在所有條件下都產生了非常有競爭力的性能，並且與官方基線相比實現了 86\:\% 的相對改進。另一方面，LA 上的結果表明，儘管所提出的架構和訓練策略在某些欺騙攻擊上表現得非常好，但它無法推廣到在訓練期間看不見的某些攻擊下。

Bibliography

```
@article{zeinali2019detecting,
  title={Detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge},
  author={Zeinali, Hossein and Stafylakis, Themos and Athanasopoulou, Georgia and Rohdin, Johan and Gkinis, Ioannis and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and others},
  journal={arXiv preprint arXiv:1907.12908},
  year={2019}
}
```


5. Schörkhuber C, Klapuri A. Constant-Q transform toolbox for music processing. In: Proc. of the 7th Sound and Music Computing Conf. Barcelona, 2010. 3−64.

Schörkhuber, C., & Klapuri, A. (2010, July). Constant-Q transform toolbox for music processing. In 7th sound and music computing conference, Barcelona, Spain (pp. 3-64).

Link : https://core.ac.uk/download/pdf/144846462.pdf

Note : CQT 特徵，提出了一種計算時域信號恆定 Q 變換 (CQT) 的高效計算方法。

```
This paper proposes a computationally efficient method for computing the constant-Q transform (CQT) of a timedomain signal. 

CQT refers to a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal. 

An inverse transform is proposed which enables a reasonable-quality (around 55dB signal-to-noise ratio) reconstruction of the original signal from its CQT coefficients. 

Here CQTs with high Q-factors, equivalent to 12–96 bins per octave, are of particular interest.

The proposed method is flexible with regard to the number of bins per octave, the applied window function, and the Q-factor, and is particularly suitable for the analysis of music signals.

A reference implementation of the proposed methods is published as a Matlab toolbox.

The toolbox includes user-interface tools that facilitate spectral data visualization and the indexing and working with the data structure produced by the CQT.
```

本文提出了一種計算時域信號恆定 Q 變換 (CQT) 的高效計算方法。CQT 指的是一種時頻表示，其中頻率區間是幾何間隔的，並且所有區間的 Q 因子（中心頻率與帶寬的比率）相等，該研究提出了一種逆變換，它能夠根據其 CQT 係數對原始信號進行合理質量（大約 55dB 信噪比）的重構。在這裡，具有高 Q 因子的 CQT（相當於每倍頻程 12-96 個 bin）特別令人感興趣。所提出的方法在每倍頻程的 bin 數量、應用的窗口函數和 Q 因子方面是靈活的，並且特別適用於音樂信號的分析。而提出方法的參考實現作為 Matlab 工具箱發布，該工具箱包括用戶界面工具，可促進光譜數據可視化以及索引和使用 CQT 生成的數據結構。

Bibliography

```
@inproceedings{schorkhuber2010constant,
  title={Constant-Q transform toolbox for music processing},
  author={Sch{\"o}rkhuber, Christian and Klapuri, Anssi},
  booktitle={7th sound and music computing conference, Barcelona, Spain},
  pages={3--64},
  year={2010}
}
```


6. Gomez-Alanis A, Peinado AM, Gonzalez JA, Gomez AM. A light convolutional GRU-RNN deep feature extractor for ASV spoofing detection. In: Proc. of the Interspeech 2019. 2019. 1068−1072.

Gomez-Alanis, A., Peinado, A. M., Gonzalez, J. A., & Gomez, A. M. (2019, September). A light convolutional GRU-RNN deep feature extractor for ASV spoofing detection. In Proc. Interspeech (Vol. 2019, pp. 1068-1072).

Link : https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2212.pdf

Note : 1) 工作的目的是開發一個單一的反欺騙系統，該系統可用於有效檢測 ASVspoof 2019 挑戰賽中考慮的所有類型的欺騙攻擊。 2) 深度伪造检测算法在公开数据集上的检测表现之性能上的評估 (LightCNN+RNN \混合光卷积和门递归单元)

```
The aim of this work is to develop a single anti-spoofing system which can be applied to effectively detect all the types of spoofing attacks considered in the ASVspoof 2019 Challenge: text-to-speech, voice conversion and replay based attacks. 

To achieve this, we propose the use of a Light Convolutional Gated Recurrent Neural Network (LC-GRNN) as a deep feature extractor to robustly represent speech signals as utterance-level embeddings, which are later used by a back-end recognizer which performs the final genuine/spoofed classification. 

This novel architecture combines the ability of light convolutional layers for extracting discriminative features at frame level with the capacity of gated recurrent unit based RNNs for learning long-term dependencies of the subsequent deep features. 

The proposed system has been presented as a contribution to the ASVspoof 2019 Challenge, and the results show a significant improvement in comparison with the baseline systems. 

Moreover, experiments were also carried out on the ASVspoof 2015 and 2017 corpora, and the results indicate that our proposal clearly outperforms other popular methods recently proposed and other similar deep feature based systems.
```

這項工作的目的是開發一個單一的反欺騙系統，該系統可用於有效檢測 ASVspoof 2019 挑戰賽中考慮的所有類型的欺騙攻擊：從文本到語音、語音轉換和基於重放的攻擊，而該研究為了實現這一點，研究者們建議使用輕卷積門控循環神經網絡 (LC-GRNN) 作為深度特徵提取器，以穩健地將語音信號表示為話語級嵌入，稍後由後端識別器使用，該識別器執行最終的真實/欺騙分類，這種新穎的架構結合了輕卷積層在幀級別提取判別特徵的能力與基於門控循環單元的 RNN 學習後續深度特徵的長期依賴關係的能力。所提出的系統已作為對 ASVspoof 2019 挑戰賽的貢獻而提出，與基線系統相比，結果顯示出顯著改進。此外，還在 ASVspoof 2015 和 2017 語料庫上進行了實驗，結果表明我們的提議明顯優於最近提出的其他流行方法和其他類似的基於深度特徵的系統。

Bibliography

```
@inproceedings{gomez2019light,
  title={A light convolutional GRU-RNN deep feature extractor for ASV spoofing detection},
  author={Gomez-Alanis, Alejandro and Peinado, Antonio M and Gonzalez, Jose A and Gomez, Angel M},
  booktitle={Proc. Interspeech},
  volume={2019},
  pages={1068--1072},
  year={2019}
}
```


7. Chen T, Kumar A, Nagarsheth P, Sivaraman G, Khoury E. Generalization of audio Deepfake detection. In: Proc. of the Odyssey 2020 Speaker and Language Recognition Workshop. 2020. 132−137.

Chen, T., Kumar, A., Nagarsheth, P., Sivaraman, G., & Khoury, E. (2020, November). Generalization of audio deepfake detection. In Proc. Odyssey 2020 The Speaker and Language Recognition Workshop (pp. 132-137).

Link : https://www.isca-speech.org/archive_v0/Odyssey_2020/pdfs/29.pdf

Note : 深度伪造检测算法在公开数据集上的检测表现之性能上的評估 (Deep Residual Network + Frequency Masking \大边际距离 & 损失函数)

```
Audio Deepfakes, technically known as logical-access voice spoofing techniques, have become an increased threat on voice interfaces due to the recent breakthroughs in speech synthesis and voice conversion technologies.

Effectively detecting these attacks is critical to many speech applications including automatic speaker verification systems.

As new types of speech synthesis and voice conversion techniques are emerging rapidly, the generalization ability of spoofing countermeasures is becoming an increasingly critical challenge.

This paper focuses on overcoming this issue by using large margin cosine loss function (LMCL) and online frequency masking augmentation to force the neural network to learn more robust feature embeddings.

We evaluate the performance of the proposed system on the ASVspoof 2019 logical access (LA) dataset.

Additionally, we evaluate it on a noisy version of the ASVspoof 2019 dataset using publicly available noises to simulate more realistic scenarios.

Finally, we evaluate the proposed system on a copy of the dataset that is logically replayed through the telephony channel to simulate spoofing attacks in the call center scenario.

Our baseline system is based on residual neural network, and has achieved the lowest equal error rate (EER) of 4.04% among all single-system submissions during the ASVspoof 2019 challenge.

Furthermore, the additional improvements proposed in this paper reduce the EER to 1.26%.
```

由於最近語音合成和語音轉換技術的突破，Audio Deepfakes，在技術上被稱為邏輯訪問語音欺騙技術，已經成為語音接口上越來越大的威脅。為了有效檢測這些攻擊對於包括自動說話人驗證系統在內的許多語音應用程序至關重要，同時隨著新型語音合成和語音轉換技術的迅速出現，欺騙對策的泛化能力正成為越來越關鍵的挑戰。該研究重點通過使用大餘量餘弦損失函數 (LMCL) 和在線頻率掩蔽增強來強制神經網絡學習更穩健的特徵嵌入來克服這個問題，而研究者在 ASVspoof 2019 邏輯訪問 (LA) 數據集上評估了擬議系統的性能。此外，研究者使用公開可用的噪聲在 ASVspoof 2019 數據集的噪聲版本上對其進行評估，以模擬更真實的場景，最後在通過電話通道邏輯重放的資料集副本上評估所提出的系統，以模擬呼叫中心場景中的欺騙攻擊。研究者們的基線系統基於殘差神經網絡，並在 ASVspoof 2019 挑戰期間的所有單系統提交中實現了 4.04% 的最低等錯誤率 (EER)，此外，該研究提出的額外改進將 EER 降低到 1.26%。

Bibliography

```
@inproceedings{chen2020generalization,
  title={Generalization of audio deepfake detection},
  author={Chen, Tianxiang and Kumar, Avrosh and Nagarsheth, Parav and Sivaraman, Ganesh and Khoury, Elie},
  booktitle={Proc. Odyssey 2020 The Speaker and Language Recognition Workshop},
  pages={132--137},
  year={2020}
}
```


8. Li R, Zhao M, Li Z, Li L, Hong Q. Anti-spoofing speaker verification system with multi-feature integration and multi-task learning. In: Proc. of the Interspeech. 2019. 1048−1052.

Li, R., Zhao, M., Li, Z., Li, L., & Hong, Q. (2019, September). Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning. In Interspeech (pp. 1048-1052).

Link : https://www.researchgate.net/publication/335829363_Anti-Spoofing_Speaker_Verification_System_with_Multi-Feature_Integration_and_Multi-Task_Learning

Note : 深度伪造检测算法在公开数据集上的检测表现之性能上的評估 (Butterfly Unit + Multi-Task \多特征融合 & 多任务学习)

```
Speaker anti-spooﬁng is crucial to prevent security breacheswhen the speaker veriﬁcation systems encounter the spoofed at-tacks from the advanced speech synthesis algorithms and highﬁdelity replay devices. 

In this paper, we propose a frameworkbased on multiple features integration and multi-task learning(MFMT) for improving anti-spooﬁng performance. 

It is im-portant to integrate the complementary information of multi-ple spectral features within the network, such as MFCC, C-QCC, Fbank, etc., as often a single kind of feature is not e-nough to grasp the global spooﬁng cues and it generalizes poor-ly. 

Furthermore, we propose a helpful butterﬂy unit (BU) formulti-task learning to propagate the shared representations be-tween the binary decision task and the other auxiliary task.

The BU can obtain task representations of other branch during for-ward propagation and prevent the gradient from assimilating thebranch during back propagation.

Our proposed system yieldedan EER of 9.01% on ASVspoof 2017, while the best single sys-tem and the average scores fusion obtained the evaluation EERof 2.39% and 0.96% on ASVspoof 2019 PA, respectively.

Index Terms: multi-feature integration, multi-task learning,stitching layer, butterﬂy unit, anti-spooﬁng, speaker veriﬁcation
```

當說話人驗證系統遇到來自高級語音合成算法和高保真回放設備的欺騙攻擊時，說話人反欺騙對於防止安全漏洞至關重要。在該研究中中，研究者提出了一種基於多特徵集成和多任務學習（MFMT）的框架，用於提高反欺騙性能。同時因為整合網絡內多個光譜特徵的互補信息非常重要，例如 MFCC、C-QCC、Fbank 等，因為通常單一的特徵不足以掌握全局欺騙線索，它概括得很差。此外，研究者提出了一個有用的蝴蝶單元（BU），用於多任務學習，以傳播二元決策任務和其他輔助任務之間的共享表示，其 BU 可以在前向傳播過程中獲得其他分支的任務表示，並防止梯度在反向傳播過程中同化該分支。該研究所提出的系統在 ASVspoof 2017 上的 EER 為 9.01%，而最佳單系統和平均分數融合在 ASVspoof 2019 PA 上分別獲得了 2.39% 和 0.96% 的評估 EER。

Bibliography

```
@inproceedings{li2019anti,
  title={Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning.},
  author={Li, Rongjin and Zhao, Miao and Li, Zheng and Li, Lin and Hong, Qingyang},
  booktitle={Interspeech},
  pages={1048--1052},
  year={2019}
}
```


9. Goswami G, Ratha N, Agarwal A, Singh R, Vatsa M. Unravelling robustness of deep learning based face recognition against adversarial attacks. In: Proc. of the 32nd AAAI Conf. on Artificial Intelligence. 2018. 6829−6836.

Goswami, G., Ratha, N., Agarwal, A., Singh, R., & Vatsa, M. (2018, April). Unravelling robustness of deep learning based face recognition against adversarial attacks. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1).

Link : https://arxiv.org/abs/1803.00401

Note : 該研究發現對人臉圖片的遮擋和加噪，能夠在一定的程度上去欺騙人臉檢測器 VGGface 和 Openface

```
Deep neural network (DNN) architecture based models have high expressive power and learning capacity. 

However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. 

Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities.

In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: 

(i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; 

(ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and 

(iii) making corrections to the processing pipeline to alleviate the problem. 

Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. 

The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. 

Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition.
```

基於深度神經網絡 (DNN) 架構的模型具有很高的表達能力和學習能力，然而，它們本質上是一種黑盒方法，因為在其多層表示中學習到的函數在數學上表示並不容易。意識到這一點後，許多研究人員已經開始設計方法來利用基於深度學習的演算法的缺點，質疑它們的魯棒性並暴露它們的奇點。在在此研究中，研究者們試圖解開與 DNN 在人臉識別方面的魯棒性相關的三個方面：(i) 評估深度架構對人臉識別的影響，以應對攻擊的脆弱性，這些攻擊受到現實世界中普遍觀察到的扭曲的啟發，淺層學習方法和基於學習的對手可以很好地處理這些扭曲；(ii) 通過表徵深層網絡隱藏層中的異常濾波器響應行為來檢測奇異點；和(iii) 對處理管道進行更正以緩解問題。該研究則使用多個開源的基於 DNN 的人臉識別網絡（包括 OpenFace 和 VGG-Face）以及兩個公開可用的資料庫（MEDS 和 PaSC）的實驗評估表明，基於深度學習的人臉識別算法的性能在存在這樣的扭曲。同時該方法還與現有的檢測算法進行了比較，結果表明，通過使用網絡中隱藏層的響應適當地設計分類器，它能夠以非常高的精度檢測攻擊。最後，研究提出了幾種有效的對策來減輕對抗性攻擊的影響並提高基於 DNN 的人臉識別的整體魯棒性。

Bibliography

```
@inproceedings{goswami2018unravelling,
  title={Unravelling robustness of deep learning based face recognition against adversarial attacks},
  author={Goswami, Gaurav and Ratha, Nalini and Agarwal, Akshay and Singh, Richa and Vatsa, Mayank},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}
```


10. Parkhi OM, Vedaldi A, Zisserman A. Deep face recognition. In: Proc. of the British Machine Vision Conf. (BMVC). BMVA Press, 2015. 41.1−41.12.

Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015). Deep face recognition.

Link : https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf

Note : VGGface

```
The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video.

Recent progress in this area has been due to two factors:

(i) end to end learning for the task using a convolutional neural network (CNN), and 

(ii) the availability of very large scale training datasets.

We make two contributions: 

first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human
in the loop, and discuss the trade off between data purity and time; 

second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.
```

本文的目標是人臉識別——來自單張照片或視頻中跟踪的一組人臉，該領域的最新進展歸因於兩個因素：(i) 使用卷積神經網絡 (CNN) 對任務進行端到端學習，以及(ii) 超大規模訓練數據集的可用性。該研究做出了兩個貢獻：首先，研究者們展示瞭如何通過自動化和人工的組合來組裝一個非常大規模的資料集（260 萬張圖像，超過 260 萬人），同時在循環中，並討論數據純度和時間之間的權衡；其次，該研究遍歷了深度網絡訓練和人臉識別的複雜性，提出了在標準 LFW 和 YTF 人臉基准上實現可比的最先進結果的方法和程序。

Bibliography

```
@article{parkhi2015deep,
  title={Deep face recognition},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
  year={2015},
  publisher={British Machine Vision Association}
}
```


11. Baltrušaitis T, Robinson P, Morency LP. Openface: An open source facial behavior analysis toolkit. In: Proc. of the IEEE Winter Conf. on Applications of Computer Vision (WACV). IEEE, 2016. 1−10.

Baltrušaitis, T., Robinson, P., & Morency, L. P. (2016, March). Openface: an open source facial behavior analysis toolkit. In 2016 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 1-10). IEEE.

Link : https://ieeexplore.ieee.org/document/7477553

Note : Openface

```
Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding.

We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis.

OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.

The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. 

Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware.

Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.
```

在過去的幾年裡中，人們對自動面部行為分析和理解越來越感興趣。其研究者展示了一種名為 OpenFace 的開源工具，適用於計算機視覺和機器學習研究人員、情感計算社區以及對基於面部行為分析構建交互式應用程序感興趣的人們。OpenFace 是第一個能夠進行面部標誌檢測、頭部姿勢估計、面部動作單元識別和眼睛注視估計的開源工具。代表 OpenFace 核心的計算機視覺演算法在上述在所有任務中展示了最先進的結果。此外，該研究的工具具有實時性能，無需任何專業硬件即可從簡單的網絡攝像頭運行。最後，OpenFace 允許通過輕量級消息傳遞系統與其他應用程序和設備輕鬆集成。

Bibliography

```
@inproceedings{baltruvsaitis2016openface,
  title={Openface: an open source facial behavior analysis toolkit},
  author={Baltru{\v{s}}aitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
  booktitle={2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1--10},
  year={2016},
  organization={IEEE}
}
```


12. Li X, Ji S, Han M, Ji J, Ren Z, Liu Y, Wu C. Adversarial examples versus cloud-based detectors: A black-box empirical study. arXiv preprint arXiv:1901.01223, 2019.

Li, X., Ji, S., Han, M., Ji, J., Ren, Z., Liu, Y., & Wu, C. (2019). Adversarial examples versus cloud-based detectors: A black-box empirical study. IEEE Transactions on Dependable and Secure Computing, 18(4), 1933-1949.

Link : https://arxiv.org/abs/1901.01223

Note : 利用查询优化的方式对人脸图片进行加噪, 以此来绕过人脸识别引擎

```
Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. 

While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet.

In this paper, we mainly focus on the security issues of real-world cloud-based image detectors.

Specifically, 

(1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and 

(2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. 

Through the comprehensive evaluations on five major cloud platforms: 

AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100%, and the semantic segmentation based attacks have a success rate over 90% among different detection services, such as violence, politician, and pornography detection. 

We also proposed several possible defense strategies for these security challenges in the real-life situation.
```

深度學習已被谷歌、AWS和百度等主要雲提供商廣泛利用，提供各種計算機視覺相關服務，包括圖像分類、對象識別、非法圖像檢測等。雖然最近的工作廣泛表明深度學習分類模型容易受到對抗性示例的攻擊，但比分類器更複雜的基於雲的圖像檢測模型也可能具有類似的安全問題，但尚未得到足夠的關注。在該研究中，研究者主要關注現實世界基於雲的圖像檢測器的安全問題。具體來說，(1) 基於有效的語義分割，研究者提出了四種攻擊，僅通過與黑盒 API 交互來生成語義感知的對抗樣本；和 (2) 研究者首次嘗試對針對現實世界基於雲的圖像檢測器的黑盒攻擊進行廣泛的實證研究。通過對 5 大雲平台的綜合評價：AWS、Azure、谷歌云、百度雲和阿里雲，研究者證明了該研究基於圖像處理的攻擊可以達到大約 100% 的成功率，基於語義分割的攻擊在不同的檢測服務中成功率超過 90%，例如暴力、政治和色情檢測。同時研究者還針對現實生活中的這些安全挑戰提出了幾種可能的防禦策略。

Bibliography

```
@article{li2019adversarial,
  title={Adversarial examples versus cloud-based detectors: A black-box empirical study},
  author={Li, Xurong and Ji, Shouling and Han, Meng and Ji, Juntao and Ren, Zhenyu and Liu, Yushan and Wu, Chunming},
  journal={IEEE Transactions on Dependable and Secure Computing},
  volume={18},
  number={4},
  pages={1933--1949},
  year={2019},
  publisher={IEEE}
}
```


13. Dong Y, Su H, Wu B, Li Z, Liu W, Zhang T, Zhu J. Efficient decision-based black-box adversarial attacks on face recognition. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2019. 7714−7722.

Dong, Y., Su, H., Wu, B., Li, Z., Liu, W., Zhang, T., & Zhu, J. (2019). Efficient decision-based black-box adversarial attacks on face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7714-7722).

Link : https://arxiv.org/abs/1904.04433

Note : 利用查询优化的方式对人脸图片进行加噪, 以此来绕过人脸识别引擎

```
Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs).

However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes.

Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed.

In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model.

This attack setting is more practical in real-world face recognition systems.

To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometries of the search directions and reduce the dimension of the search space.

Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries.

We also apply the proposed method to attack a real-world face recognition system successfully.
```

由於深度卷積神經網絡（CNN）的巨大改進，人臉識別近年來取得了顯著進展。然而，深度 CNN 容易受到對抗樣本的攻擊，這可能會在具有安全敏感目的的現實世界人臉識別應用中造成致命的後果，因為對抗性攻擊被廣泛研究之故，有心人可以在部署模型之前識別模型的漏洞。在該研究中，研究者評估了基於決策的黑盒攻擊設置中最先進的人臉識別模型的魯棒性，其中攻擊者無法訪問模型參數和梯度，但只能獲取硬標籤通過向目標模型發送查詢來進行預測，這種攻擊設置在現實世界的人臉識別系統中更實用。為了提高先前方法的效率，研究者提出了一種進化攻擊演算法，該演算法可以對搜索方向的局部幾何進行建模並降低搜索空間的維度。大量實驗證明了所提出的方法的有效性，該方法可以通過較少的查詢對輸入人臉圖像產生最小的擾動，同時研究者還應用所提出的方法成功地攻擊了現實世界的人臉識別系統。

Bibliography

```
@inproceedings{dong2019efficient,
  title={Efficient decision-based black-box adversarial attacks on face recognition},
  author={Dong, Yinpeng and Su, Hang and Wu, Baoyuan and Li, Zhifeng and Liu, Wei and Zhang, Tong and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7714--7722},
  year={2019}
}
```


14. Song Q, Wu Y, Yang L. Attacks on state-of-the-art face recognition using attentional adversarial attack generative network. arXiv preprint arXiv:1811.12026, 2018.

Yang, L., Song, Q., & Wu, Y. (2021). Attacks on state-of-the-art face recognition using attentional adversarial attack generative network. Multimedia Tools and Applications, 80(1), 855-875.

Link : https://arxiv.org/abs/1811.12026

Note : 使用注意力机制和生成对抗网络生成指定语义信息的假人脸,使得人脸识别器误判

```
With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked.

So, it is important to study how face recognition networks are subject to attacks.

In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously.

Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images.

For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces.

Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better.

The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.
```

隨著人臉識別的廣泛應用，它的弱點也逐漸暴露出來，即容易被攻擊。因此研究人臉識別網絡如何受到攻擊非常重要。在研究中，研究者專注於一種對人臉識別網絡進行攻擊的新穎方法，該方法會誤導網絡將某人識別為目標人，而不是不明顯地錯誤分類，同時，因為此緣故，研究者引入了一個特定的注意力對抗攻擊生成網絡來生成假人臉圖像。為了捕獲目標人的語義信息，這項工作添加了條件變分自動編碼器和注意模塊來學習人臉之間的實例級對應關係。與傳統的雙人 GAN 不同，這項工作引入了人臉識別網絡作為第三個參與者參與生成器和判別器之間的競爭，這使得攻擊者可以更好地模仿目標人。生成的結果難以引起旁觀者註意的人臉可以逃避最先進網絡的識別，並且大多數人都被識別為目標人。

Bibliography

```
@article{yang2021attacks,
  title={Attacks on state-of-the-art face recognition using attentional adversarial attack generative network},
  author={Yang, Lu and Song, Qing and Wu, Yingqi},
  journal={Multimedia Tools and Applications},
  volume={80},
  number={1},
  pages={855--875},
  year={2021},
  publisher={Springer}
}
```


15. Majumdar P, Agarwal A, Singh R, Vatsa M. Evading face recognition via partial tampering of faces. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops. 2019. 11−20.

Majumdar, P., Agarwal, A., Singh, R., & Vatsa, M. (2019). Evading face recognition via partial tampering of faces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (pp. 0-0).

Link : https://ieeexplore.ieee.org/abstract/document/9025546

Note : 研究发现:对人脸部分区域的修改和变形,可以让人脸识别器有很高的误识率.

```
Advancements in machine learning and deep learning techniques have led to the development of sophisticated and accurate face recognition systems.

However, for the past few years, researchers are exploring the vulnerabilities of these systems towards digital attacks.

Creation of digitally altered images has become an easy task with the availability of various image editing tools and mobile application such as Snapchat.

Morphing based digital attacks are used to elude and gain the identity of legitimate users by fooling the deep networks.

In this research, partial face tampering attack is proposed, where facial regions are replaced or morphed to generate tampered samples.

Face verification experiments performed using two state-of-the-art face recognition systems, VGG-Face and OpenFace on the CMU-MultiPIE dataset indicates the vulnerability of these systems towards the attack.

Further, a Partial Face Tampering Detection (PFTD) network is proposed for the detection of the proposed attack.

The network captures the inconsistencies among the original and tampered images by combining the raw and high-frequency information of the input images for the detection of tampered images.

The proposed network surpasses the performance of the existing baseline deep neural networks for tampered image detection.
```

機器學習和深度學習技術的進步導致了複雜和準確的人臉識別系統的發展。然而，在過去的幾年裡中研究人員們正探索這些系統對數位攻擊的脆弱性，而隨著各種圖像編輯工具和移動應用程序（如 Snapchat）的出現，創造出經過數位修改的圖像已成為一項簡單的任務。基於變形的數位攻擊用於通過欺騙深層網絡來逃避和獲取合法用戶的身份。在該項研究中，研究者們提出了部分面部篡改攻擊，其中面部區域被替換或變形以生成篡改樣本，而在 CMU-MultiPIE 數據集上使用兩個最先進的人臉識別系統 VGG-Face 和 OpenFace 進行的人臉驗證實驗表明了這些系統對攻擊的脆弱性。此外，該研究提出了一種部分人臉篡改檢測（PFTD）網絡來檢測所提出的攻擊，該網絡通過結合輸入圖像的原始信息和高頻信息來捕獲原始圖像和篡改圖像之間的不一致性，以檢測篡改圖像。所提出的網絡在篡改圖像檢測方面超越了現有基線深度神經網絡的性能。

Bibliography

```
@inproceedings{majumdar2019evading,
  title={Evading face recognition via partial tampering of faces},
  author={Majumdar, Puspita and Agarwal, Akshay and Singh, Richa and Vatsa, Mayank},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={0--0},
  year={2019}
}
```


16. Korshunov P, Marcel S. Vulnerability of face recognition to deep morphing. arXiv preprint arXiv:1910.01933, 2019.

Korshunov, P., & Marcel, S. (2019). Vulnerability of face recognition to deep morphing. arXiv preprint arXiv:1910.01933.

Link : https://arxiv.org/abs/1910.01933

Note : 测试了基于 VGGnet 和 FaceNet 的人脸检测器的安全性,通过输入生成的 Deepfakes 影像,发现这两类人脸检测器分别有 85.62% 和 95.00% 的错误接受率,说明人脸检测器分辨不出深度伪造人脸和
源人脸. 

```
It is increasingly easy to automatically swap faces in images and video or morph two faces into one using generative adversarial networks (GANs).

The high quality of the resulted deep-morph raises the question of how vulnerable the current face recognition systems are to such fake images and videos.

It also calls for automated ways to detect these GAN-generated faces.

In this paper, we present the publicly available dataset of the Deepfake videos with faces morphed with a GAN-based algorithm.

To generate these videos, we used open source software based on GANs, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos.

We show that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to the deep morph videos, with 85.62 and 95.00 false acceptance rates, respectively, which means methods for detecting these videos are necessary.

We consider several baseline approaches for detecting deep morphs and find that the method based on visual quality metrics (often used in presentation attack detection domain) leads to the best performance with 8.97 equal error rate.

Our experiments demonstrate that GAN-generated deep morph videos are challenging for both face recognition systems and existing detection methods, and the further development of deep morphing technologies will make it even more so.
```

使用生成對抗網絡 (GAN) 自動交換圖像和視頻中的人臉或將兩張人臉變成一張人臉變得越來越容易，由此產生的深度變形的高質量提出了一個問題，即當前的人臉識別系統對此類虛假圖像和影像前是脆弱的。有很高的需求，需要採用自動化方法來檢測這些 GAN 生成的人臉。在此研究中，研究者展示了 Deepfake 視頻的公開數據集，其中的人臉使用基於 GAN 的算法變形，同時為了生成這些影像，研究者使用了基於 GAN 的開源軟件，並且我們強調訓練和混合參數可以顯著影響生成影像的品質，該研究表明，基於 VGG 和 Facenet 神經網絡的最先進的人臉識別系統容易受到深度變形視頻的影響，錯誤接受率分別為 85.62 和 95.00，這意味著檢測這些視頻的方法是必要的。同時研究也考慮了幾種檢測深度變形的基線方法，並發現基於視覺品質指標的方法（通常用於演示攻擊檢測領域）導致最佳性能，錯誤率為 8.97。而最後研究者的實驗表明，GAN 生成的深度變形視頻對人臉識別系統和現有檢測方法都具有挑戰性，而深度變形技術的進一步發展將使其更加如此。

Bibliography

```
@article{korshunov2019vulnerability,
  title={Vulnerability of face recognition to deep morphing},
  author={Korshunov, Pavel and Marcel, S{\'e}bastien},
  journal={arXiv preprint arXiv:1910.01933},
  year={2019}
}
```


17. Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2015. 815−823.

Schroff, F., Kalenichenko, D., & Philbin, J. (2015). Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 815-823).

Link : https://arxiv.org/abs/1503.03832

Note : FaceNet

```
Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. 

In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity.

Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.

Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.

To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method.

The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.

On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%.

Our system cuts the error rate in comparison to the best published result by 30% on both datasets.

We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.
```

儘管人臉識別領域最近取得了重大進展，但當下大規模有效地實施人臉驗證和識別對當前方法對此提出了嚴峻挑戰，在該研究中，研究者們提出了一個稱為 FaceNet 的系統，它直接學習從人臉圖像到緊湊歐幾里得空間的映射，其中距離直接對應於人臉相似度的度量。

一旦產生了這個空間，就可以使用將 FaceNet 嵌入作為特徵向量的標準技術輕鬆實現人臉識別、驗證和聚類等任務，研究者的方法使用經過訓練的深度卷積網絡直接優化嵌入本身，而不是像以前的深度學習方法那樣使用中間瓶頸層。

為了訓練，研究者使用了使用一種新穎的在線三元組挖掘方法生成的大致對齊匹配/非匹配面塊的三元組，其方法的好處是更高的表示效率：我們實現了最先進的人臉識別性能，每張人臉僅使用 128 字節。在廣泛使用的野外標記人臉 (LFW) 數據集上，研究者的系統達到了 99.63% 的新記錄準確率。在 YouTube Faces DB 上，它達到了 95.12%。其系統在兩個數據集上都將錯誤率與發布的最佳結果相比降低了 30%。同時研究者還介紹了諧波嵌入的概念和諧波三元組損失，它們描述了相互兼容的不同版本的人臉嵌入（由不同的網絡產生），並允許相互直接比較。

Bibliography

```
@inproceedings{schroff2015facenet,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}
```

18. Szegedy C, Zaremba W, Sutskever I, Bruna J. Intriguing properties of neural networks. In: Proc. of the 2nd Int’l Conf. on Leaning Representations (ICLR). 2014.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013). Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.

Link : https://arxiv.org/abs/1312.6199

Note : 神经网络本身存在着对抗样本攻击. 对抗样本攻击是一种对模型输入进行扰动,从而使模型产生误判的技术 !??

```
Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks.

While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties.

In this paper we report two such properties.

First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis.

It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.

Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend.

We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error.

In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
```

深度神經網絡是高度表達模型，最近在語音和視覺識別任務上取得了最先進的性能，雖然他們的表現力是他們成功的原因，但它也使他們學習無法解釋的解決方案，這些解決方案可能具有違反直覺的特性。在該研究中，研究者們報告了兩個這樣的屬性。首先，根據單元分析的各種方法，該研究發現單個高級單元和高級單元的隨機線性組合之間沒有區別。其研究表明，在神經網絡的高層中，包含語義信息的是空間，而不是單個單元。其次，研究者們發現深度神經網絡學習的輸入-輸出映射在很大程度上是不連續的。可以通過應用某種不可察覺的擾動來導致網絡對圖像進行錯誤分類，這種擾動是通過最大化網絡的預測誤差來發現的。此外，這些擾動的具體性質並不是學習的隨機偽影：相同的擾動可能導致在數據集的不同子集上訓練的不同網絡對相同的輸入進行錯誤分類。

Bibliography

```
@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}
```

19. Goodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. In: Proc. of the 3rd Int’l Conf. on Leaning Representations (ICLR). 2015.

Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.

Link : https://arxiv.org/abs/1412.6572

Note : 神经网络本身存在着对抗样本攻击. 对抗样本攻击是一种对模型输入进行扰动,从而使模型产生误判的技术 !??

```
Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence.

Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. 

We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature.

This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets.

Moreover, this view yields a simple and fast method of generating adversarial examples.

Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.
```

包括神經網絡在內的一些機器學習模型一直錯誤分類對抗性示例輸入，這些輸入是通過對數據集中的示例應用小的但故意最壞情況的擾動形成的，這樣擾動的輸入會導致模型以高置信度輸出不正確的答案。解釋這種現象的早期嘗試集中在非線性和過度擬合上。相反，該研究的研究者們認為神經網絡易受對抗性擾動影響的主要原因是它們的線性性質，這種解釋得到了新的定量結果的支持，同時給出了關於它們最有趣的事實的第一個解釋：它們在架構和訓練集上的泛化。此外，這種觀點產生了一種簡單而快速的生成對抗樣本的方法。使用這種方法為對抗性訓練提供示例，該研究減少了 MNIST 數據集上 maxout 網絡的測試集誤差。

Bibliography

```
@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}
```


20. Kurakin A, Goodfellow I, Bengio S. Adversarial examples in the physical world. In: Proc. of the 5th Int’l Conf. on Leaning Representations (ICLR) Workshop. 2017.

Kurakin, A., Goodfellow, I. J., & Bengio, S. (2018). Adversarial examples in the physical world. In Artificial intelligence safety and security (pp. 99-112). Chapman and Hall/CRC.

Link : https://arxiv.org/abs/1607.02533

Note : 神经网络本身存在着对抗样本攻击. 对抗样本攻击是一种对模型输入进行扰动,从而使模型产生误判的技术 !??

```
Most existing machine learning classifiers are highly vulnerable to adversarial examples.

An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. 

In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.

Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.

Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier.

This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input.

This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples.

We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system.

We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
```

大多數現有的機器學習分類器都非常容易受到對抗性示例的影響，對抗性示例是輸入數據的樣本，該樣本經過了非常輕微的修改，旨在導致機器學習分類器對其進行錯誤分類。在許多情況下，這些修改可能非常微妙，以至於人類觀察者甚至根本沒有註意到修改，但分類器仍然會出錯。對抗性示例會帶來安全問題，因為它們可用於對機器學習系統進行攻擊，即使對手無法訪問底層模型。到目前為止，所有先前的工作都假設了一個威脅模型，其中攻擊者可以將數據直接輸入機器學習分類器。在物理世界中運行的系統並非總是如此，例如那些使用來自相機和其他傳感器的信號作為輸入的系統。該研究表明，即使在這樣的物理世界場景中，機器學習系統也容易受到對抗性示例的影響。研究者們通過將從手機攝像頭獲得的對抗性圖像饋送到 ImageNet Inception 分類器並測量系統的分類精度來證明這一點，最後研究者也發現，即使通過攝像頭感知，大部分對抗性示例也被錯誤分類。

Bibliography

```
@incollection{kurakin2018adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={Artificial intelligence safety and security},
  pages={99--112},
  year={2018},
  publisher={Chapman and Hall/CRC}
}
```


21. Wang SY, Wang O, Zhang R, Owens A, Efros AA. CNN-generated images are surprisingly easy to spot for now. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020. 8692−8701.

Kurakin, A., Goodfellow, I. J., & Bengio, S. (2018). Adversarial examples in the physical world. In Artificial intelligence safety and security (pp. 99-112). Chapman and Hall/CRC.

Link : https://arxiv.org/abs/1912.11035

Note : 研究发现不同的 GAN 生成的伪造图像都留下特定的指纹特征,虽然依赖于指纹特征训练的检测器泛化能力不好,但是对训练数据进行预处理,如增加 JPEG 压缩、模糊等操作,大大提高模型的泛化性能,同时在检测时对图片进行后处理,可以增加模型的鲁棒性

```
In this work we ask whether it is possible to create a "universal" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used.

To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark).

We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2).

Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis.

Code and pre-trained networks are available at this https URL .

https://peterwang512.github.io/CNNDetection/
```

在這項工作中，研究嘗試是否有可能創造一個“通用”檢測器，用於將真實圖像與 CNN 生成的圖像區分開來，無論使用何種架構或數據集。為了測試這一點，研究者們收集了一個數據集，該數據集由 11 種不同的基於 CNN 的圖像生成器模型生成的假圖像組成，這些模型跨越當今常用架構的空間（ProGAN、StyleGAN、BigGAN、CycleGAN、StarGAN、GauGAN、DeepFakes、級聯細化網絡，隱式最大似然估計，二階注意力超分辨率，在黑暗中看到）。其研究者證明，通過仔細的預處理和後處理以及數據增強，僅在一個特定的 CNN 生成器 (ProGAN) 上訓練的標準圖像分類器能夠很好地泛化到看不見的架構、數據集和訓練方法（包括剛剛發布的風格GAN2）。而研究者的研究結果表明，當今 CNN 生成的圖像存在一些常見的系統缺陷，從而阻止它們實現逼真的圖像合成，這一有趣的可能性。

Bibliography

```
@incollection{kurakin2018adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={Artificial intelligence safety and security},
  pages={99--112},
  year={2018},
  publisher={Chapman and Hall/CRC}
}
```


22. Neves JC, Tolosana R, Vera-Rodriguez R, Vera-Rodriguez R, Lopes V, Proena H, Fierrez J. Ganprintr: Improved fakes and evaluation of the state-of-the-art in face manipulation detection. IEEE Journal of Selected Topics in Signal Processing, 2020,14(5): 1038−1048.

Neves, J. C., Tolosana, R., Vera-Rodriguez, R., Lopes, V., Proença, H., & Fierrez, J. (2020). Ganprintr: Improved fakes and evaluation of the state of the art in face manipulation detection. IEEE Journal of Selected Topics in Signal Processing, 14(5), 1038-1048.

Link : https://arxiv.org/abs/1911.05351

Note : 设计了一个自动编码器能够将合成的伪造图像移除指纹等信息,让现有的伪造检测系统失效

```
The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. 

Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios.

In this study, we focus on the synthesis of entire facial images, which is a specific type of facial manipulation.

The main contributions of this study are four-fold: 

i) a novel strategy to remove GAN "fingerprints" from synthetic fake images based on autoencoders is described, in order to spoof facial manipulation detection systems while keeping the visual quality of the resulting images;

ii) an in-depth analysis of the recent literature in facial manipulation detection;

iii) a complete experimental assessment of this type of facial manipulation, considering the state-of-the-art fake detection systems (based on holistic deep networks, steganalysis, and local artifacts), remarking how challenging is this task in unconstrained scenarios; and finally

iv) we announce a novel public database, named iFakeFaceDB, yielding from the application of our proposed GAN-fingerprint Removal approach (GANprintR) to already very realistic synthetic fake images.

The results obtained in our empirical evaluation show that additional efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques, such as the one proposed in this study.
```

大規模面部數據庫的可用性，以及深度學習技術的顯著進步，特別是生成對抗網絡 (GAN)，導致了極其逼真的虛假面部內容的產生，引發了對濫用可能性的明顯擔憂，這種擔憂促進了對操縱檢測方法的研究，這些方法與人類相反，已經在各種場景中取得了驚人的成果。在這項研究中，研究者們專注於整個面部圖像的合成，這是一種特定類型的面部操作。該研究的主要貢獻有四方面：i) 描述了一種從基於自動編碼器的合成假圖像中去除 GAN“指紋”的新策略，以欺騙面部操作檢測系統，同時保持結果圖像的視覺質量；ii) 對近期面部操作檢測文獻的深入分析；iii) 對這種類型的面部操作進行完整的實驗評估，考慮到最先進的假檢測系統（基於整體深度網絡、隱寫分析和局部偽影），並指出這項任務在不受約束的場景中的挑戰性；最後 iv) 研究者們宣布了一個名為 iFakeFaceDB 的新型公共數據庫，該數據庫將該研究所提出的 GAN 指紋去除方法 (GANprintR) 應用於已經非常逼真的合成假圖像。在該研究的實證評估中獲得的結果表明，需要額外的努力來開發針對看不見的條件和欺騙技術的強大的面部操作檢測系統，例如本研究中提出的技術。

Bibliography

```
@article{neves2020ganprintr,
  title={Ganprintr: Improved fakes and evaluation of the state of the art in face manipulation detection},
  author={Neves, Joao C and Tolosana, Ruben and Vera-Rodriguez, Ruben and Lopes, Vasco and Proen{\c{c}}a, Hugo and Fierrez, Julian},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={14},
  number={5},
  pages={1038--1048},
  year={2020},
  publisher={IEEE}
}
```


23. Marra F, Gragnaniello D, Cozzolino D, Verdoliva L. Detection of GAN-generated fake images over social networks. In: Proc. of the IEEE Conf. on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2018. 384−389.

Marra, F., Gragnaniello, D., Cozzolino, D., & Verdoliva, L. (2018, April). Detection of gan-generated fake images over social networks. In 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR) (pp. 384-389). IEEE.

Link : https://ieeexplore.ieee.org/document/8397040

Note : 模拟了篡改图片在社交网络的场景中的检测,结果显示,现有的检测器在现实网络对抗环境下(未知压缩和未知类型等)表现很差

```
The diffusion of fake images and videos on social networks is a fast growing problem.

Commercial media editing tools allow anyone to remove, add, or clone people and objects, to generate fake images.

Many techniques have been proposed to detect such conventional fakes, but new attacks emerge by the day.

Image-to-image translation, based on generative adversarial networks (GANs), appears as one of the most dangerous, as it allows one to modify context and semantics of images in a very realistic way.

In this paper, we study the performance of several image forgery detectors against image-to-image translation, both in ideal conditions, and in the presence of compression, routinely performed upon uploading on social networks.

The study, carried out on a dataset of 36302 images, shows that detection accuracies up to 95% can be achieved by both conventional and deep learning detectors, but only the latter keep providing a high accuracy, up to 89%, on compressed data.
```

社交網絡上虛假圖像和視頻的傳播是一個快速增長的問題，商業媒體編輯工具允許任何人刪除、添加或克隆人和對象，以生成虛假圖像。已經提出了許多技術來檢測這種傳統的偽造品，但新的攻擊每天都在出現。基於生成對抗網絡 (GAN) 的圖像到圖像轉換似乎是最危險的方法之一，因為它允許人們以非常現實的方式修改圖像的上下文和語義。在該研究中，研究者們研究了幾種圖像偽造檢測器對圖像到圖像轉換的性能，無論是在理想條件下，還是在存在壓縮的情況下，通常在上傳到社交網絡時執行。該研究在 36302 張圖像的數據集上進行，表明傳統和深度學習檢測器都可以實現高達 95% 的檢測精度，但只有後者在壓縮數據上保持高達 89% 的高精度。

Bibliography

```
@inproceedings{marra2018detection,
  title={Detection of gan-generated fake images over social networks},
  author={Marra, Francesco and Gragnaniello, Diego and Cozzolino, Davide and Verdoliva, Luisa},
  booktitle={2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
  pages={384--389},
  year={2018},
  organization={IEEE}
}
```


24. Zhang X, Karaman S, Chang SF. Detecting and simulating artifacts in GAN fake images. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). 2019. 1−6.

Zhang, X., Karaman, S., & Chang, S. F. (2019, December). Detecting and simulating artifacts in gan fake images. In 2019 IEEE International Workshop on Information Forensics and Security (WIFS) (pp. 1-6). IEEE.

Link : https://arxiv.org/pdf/1907.06515.pdf

Note : 寻找 GAN 的共有痕迹,提高检测器的鲁棒性.现有的检测器对数据依赖强,泛化性不够

```
To detect GAN generated images, conventional supervised machine learning algorithms require collection of a number of real and fake images from the targeted GAN model.

However, the specific model used by the attacker is often unavailable.

To address this, we propose a GAN simulator, AutoGAN, which can simulate the artifacts produced by the common pipeline shared by several popular GAN models.

Additionally, we identify a unique artifact caused by the up-sampling component included in the common GAN pipeline.

We show theoretically such artifacts are manifested as replications of spectra in the frequency domain and thus propose a classifier model based on the spectrum input, rather than the pixel input.

By using the simulated images to train a spectrum based classifier, even without seeing the fake images produced by the targeted GAN model during training, our approach achieves state-of-the-art performances on detecting fake images generated by popular GAN models such as CycleGAN.
```

為了檢測 GAN 生成的圖像，傳統的監督機器學習算法需要從目標 GAN 模型中收集大量真實和虛假圖像。但是，攻擊者使用的特定模型通常是不可用的。為了解決這個問題，該研究提出了一個 GAN 模擬器 AutoGAN，它可以模擬由幾個流行的 GAN 模型共享的公共管道產生的偽影，此外，研究者確定了由通用 GAN 管道中包含的上採樣組件引起的獨特偽影。該研究從理論上表明，這種偽影表現為頻域中光譜的複制，因此提出了一種基於光譜輸入而不是像素輸入的分類器模型。通過使用模擬圖像來訓練基於頻譜的分類器，即使在訓練期間沒有看到目標 GAN 模型產生的假圖像，此研究的方法在檢測由流行 GAN 模型（如 CycleGAN）生成的假圖像方面也取得了最先進的性能 .

Bibliography

```
@inproceedings{zhang2019detecting,
  title={Detecting and simulating artifacts in gan fake images},
  author={Zhang, Xu and Karaman, Svebor and Chang, Shih-Fu},
  booktitle={2019 IEEE International Workshop on Information Forensics and Security (WIFS)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}
```


25. Du M, Pentyala S, Li Y, Hu X. Towards generalizable forgery detection with locality-aware autoencoder. arXiv preprint arXiv: 1909.05999, 2019.

Du, M., Pentyala, S., Li, Y., & Hu, X. (2019). Towards generalizable forgery detection with locality-aware autoencoder.

Link : https://arxiv.org/abs/1909.05999

Note : 利用局部性感知的自动编码器实现造检测,使得模型聚焦篡改区域,通用性更强

```
With advancements of deep learning techniques, it is now possible to generate super-realistic images and videos, i.e., deepfakes.

These deepfakes could reach mass audience and result in adverse impacts on our society.

Although lots of efforts have been devoted to detect deepfakes, their performance drops significantly on previously unseen but related manipulations and the detection generalization capability remains a problem.

Motivated by the fine-grained nature and spatial locality characteristics of deepfakes, we propose Locality-Aware AutoEncoder (LAE) to bridge the generalization gap.

In the training process, we use a pixel-wise mask to regularize local interpretation of LAE to enforce the model to learn intrinsic representation from the forgery region, instead of capturing artifacts in the training set and learning superficial correlations to perform detection.

We further propose an active learning framework to select the challenging candidates for labeling, which requires human masks for less than 3% of the training data, dramatically reducing the annotation efforts to regularize interpretations.

Experimental results on three deepfake detection tasks indicate that LAE could focus on the forgery regions to make decisions.

The analysis further shows that LAE outperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on three deepfake detection tasks in terms of generalization accuracy on previously unseen manipulations.
```

隨著深度學習技術的進步，現在可以生成超逼真的圖像和影像，即深度偽造。這些深度偽造可能會接觸到大量受眾，並對我們的社會產生不利影響。儘管已經付出了很多努力來檢測 deepfake，但它們的性能在以前看不見但相關的操作和檢測泛化能力上顯著下降，仍然是一個問題。受 deepfakes 的細粒度性質和空間局部性特徵的啟發，研究者們提出了 Locality-Aware AutoEncoder (LAE) 來彌補泛化差距。在訓練過程中，研究者使用像素級掩碼來規範 LAE 的局部解釋，以強制模型從偽造區域學習內在表示，而不是在訓練集中捕獲偽影並學習表面相關性來執行檢測。而該研究進一步提出了一個主動學習框架來選擇具有挑戰性的標籤候選者，該框架需要不到 3% 的訓練數據使用人工蒙版，從而大大減少了規範化解釋的註釋工作。三個 deepfake 檢測任務的實驗結果表明，LAE 可以專注於偽造區域來做出決策。分析進一步表明，就先前未見過的操作的泛化精度而言，LAE 在三個深度偽造檢測任務上的性能分別優於現有技術 6.52%、12.03% 和 3.08%。

Bibliography

```
@article{du2019towards,
  title={Towards generalizable forgery detection with locality-aware autoencoder},
  author={Du, Mengnan and Pentyala, Shiva and Li, Yuening and Hu, Xia},
  year={2019}
}
```


26. Huang R, Fang F, Nguyen HH, Yamagishi J, Echizen I. Security of facial forensics models against adversarial attacks. arXiv preprint arXiv:1911.00660, 2019.

Huang, R., Fang, F., Nguyen, H. H., Yamagishi, J., & Echizen, I. (2020, October). Security of facial forensics models against adversarial attacks. In 2020 IEEE International Conference on Image Processing (ICIP) (pp. 2236-2240). IEEE.

Link : https://arxiv.org/abs/1911.00660

Note : 借鉴了对抗样本的思想,对这些基于神经网络的检测器进行对抗性攻击,设计了单个对抗攻击和通用对抗攻击两种方式,使得检测器的篡改分类和定位失效.尽管现在已经存在众多的检测器,在一些数据集上表现很好,但是攻击者依然可以完善生成方法,隐藏一些标志性特征从而绕过检测器,这是一个长期的攻防博弈过程. 

```
Deep neural networks (DNNs) have been used in digital forensics to identify fake facial images.

We investigated several DNN-based forgery forensics models (FFMs) to examine whether they are secure against adversarial attacks.

We experimentally demonstrated the existence of individual adversarial perturbations (IAPs) and universal adversarial perturbations (UAPs) that can lead a well-performed FFM to misbehave.

Based on iterative procedure, gradient information is used to generate two kinds of IAPs that can be used to fabricate classification and segmentation outputs.

In contrast, UAPs are generated on the basis of over-firing.

We designed a new objective function that encourages neurons to over-fire, which makes UAP generation feasible even without using training data.

Experiments demonstrated the transferability of UAPs across unseen datasets and unseen FFMs.

Moreover, we conducted subjective assessment for imperceptibility of the adversarial perturbations, revealing that the crafted UAPs are visually negligible.

These findings provide a baseline for evaluating the adversarial security of FFMs.
```

深度神經網絡 (DNN) 已在數字取證中用於識別虛假的面部圖像，該研究研究了幾個基於 DNN 的偽造取證模型 (FFM)，以檢查它們是否能夠抵禦對抗性攻擊。研究者們通過實驗證明了個體對抗性擾動 (IAP) 和普遍對抗性擾動 (UAP) 的存在，它們可能導致表現良好的 FFM 行為不端。基於迭代過程，梯度信息用於生成兩種可用於製造分類和分割輸出的 IAP。相比之下，UAP 是在過火的基礎上生成的。研究者們設計了一個新的目標函數，鼓勵神經元過度激發，即使不使用訓練數據也可以生成 UAP，實驗證明了 UAP 在未見數據集和未見 FFM 之間的可轉移性。此外，研究者對對抗性擾動的不可察覺性進行了主觀評估，表明精心製作的 UAP 在視覺上可以忽略不計。這些發現為評估 FFM 的對抗性安全性提供了基準。

Bibliography

```
@inproceedings{huang2020security,
  title={Security of facial forensics models against adversarial attacks},
  author={Huang, Rong and Fang, Fuming and Nguyen, Huy H and Yamagishi, Junichi and Echizen, Isao},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)},
  pages={2236--2240},
  year={2020},
  organization={IEEE}
}
```


27. Hall HK. Deepfake videos: When seeing isn’t believing. Catholic University Journal of Law and Technology, 2018,27(1):Article No.51.

Hall, H. K. (2018). Deepfake videos: When seeing isn't believing. Cath. UJL & Tech, 27, 51.

Link : https://scholarship.law.edu/jlt/vol27/iss1/4/#:~:text=Videos%2C%20known%20as%20deepfakes%2C%20use,videos%20are%20fact%20or%20fiction.

Note : 深度伪造技术的发展给社会带来了巨大的负面影响,从社会国家领导人到普通的互联网公民,都有被此类技术侵害的可能性

```
Videos, known as deepfakes, use readily available software to create a work that shows people saying and doing things they may never have uttered or engaged in.

The technology making the videos appear very authentic is advancing at such a rate that people may not be able to detect if the videos are fact or fiction.

Given the hasty acceptance of other forms of fake news in society, deepfake videos have the ability to affect the nature of information the public receives about candidates and policies.

This study examines the potential use of deepfake videos in the democratic process, analyzes the challenges in regulating this area due to the First Amendment, questions the practicality of the marketplace of ideas metaphor in today’s news and information environment, and explores possible responses to the spread of deepfakes.
```

被稱為 deepfakes 的影像使用現成的軟件來製作作品，展示人們所說和所做的事情，他們可能從未說過或參與過。使視頻看起來非常真實的技術正在以這樣的速度發展，人們可能無法檢測到視頻是事實還是虛構。鑑於社會對其他形式的假新聞的倉促接受，deepfake 視頻有能力影響公眾收到的有關候選人和政策的信息的性質。該研究考察了深度偽造視頻在民主進程中的潛在用途，分析了第一修正案在規範這一領域所面臨的挑戰，質疑思想市場隱喻在當今新聞和信息環境中的實用性，並探討了對傳播的可能反應。

Bibliography

```
@article{hall2018deepfake,
  title={Deepfake videos: When seeing isn't believing},
  author={Hall, Holly Kathleen},
  journal={Cath. UJL \& Tech},
  volume={27},
  pages={51},
  year={2018},
  publisher={HeinOnline}
}
```


28. Hasan HR, Salah K. Combating deepfake videos using blockchain and smart contracts. IEEE Access, 2019,7:41596−41606.

Hasan, H. R., & Salah, K. (2019). Combating deepfake videos using blockchain and smart contracts. Ieee Access, 7, 41596-41606.

Link : https://ieeexplore.ieee.org/document/8668407

Note : 尝试用区块链技术对互联网上的视频进行追踪

```
With the rise of artificial intelligence (AI) and deep learning techniques, fake digital contents have proliferated in recent years.

Fake footage, images, audios, and videos (known as deepfakes) can be a scary and dangerous phenomenon and can have the potential of altering the truth and eroding trust by giving false reality.

Proof of authenticity (PoA) of digital media is critical to help eradicate the epidemic of forged content.

Current solutions lack the ability to provide history tracking and provenance of digital media.

In this paper, we provide a solution and a general framework using Ethereum smart contracts to trace and track the provenance and history of digital content to its original source even if the digital content is copied multiple times.

The smart contract utilizes the hashes of the interplanetary file system (IPFS) used to store digital content and its metadata.

Our solution focuses on video content, but the solution framework provided in this paper is generic enough and can be applied to any other form of digital content.

Our solution relies on the principle that if the content can be credibly traced to a trusted or reputable source, the content can then be real and authentic.

The full code of the smart contract has been made publicly available at Github.
```

隨著人工智能 (AI) 和深度學習技術的興起，近年來虛假數字內容激增。假鏡頭、圖像、音頻和視頻（稱為 deepfakes）可能是一種可怕且危險的現象，並且有可能通過提供虛假現實來改變真相並削弱信任，數位媒體的真實性證明 (PoA) 對於幫助消除偽造內容的流行至關重要，而當前的解決方案缺乏提供數字媒體的歷史跟踪和出處的能力。

在該研究中，研究者們提供了一個解決方案和一個通用框架，使用以太坊智能合約來追踪和跟踪數字內容的出處和歷史到其原始來源，即使數字內容被複製多次，其智能合約利用用於存儲數字內容及其元數據的行星際文件系統 (IPFS) 的哈希值。解決方案專注於視頻內容，且此研究提供的解決方案框架足夠通用，可以應用於任何其他形式的數位內容。其解決方案依賴於這樣一個原則，即如果內容可以可靠地追溯到可信或有信譽的來源，那麼內容就可以是真實的和真實的。

Bibliography

```
@article{hasan2019combating,
  title={Combating deepfake videos using blockchain and smart contracts},
  author={Hasan, Haya R and Salah, Khaled},
  journal={Ieee Access},
  volume={7},
  pages={41596--41606},
  year={2019},
  publisher={IEEE}
}
```


29. The law of California to Deepfake. 2019. https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730

Link : https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730

Note : 加州修法

```
AB 730, Berman. Elections: deceptive audio or visual media.

Existing law prohibits a person or specified entity from, with actual malice, producing, distributing, publishing, or broadcasting campaign material, as defined, that contains (1) a picture or photograph of a person or persons into which the image of a candidate for public office is superimposed or (2) a picture or photograph of a candidate for public office into which the image of another person or persons is superimposed, unless the campaign material contains a specified disclosure.
```

Bibliography

```
@online{list1029,
    title     = "The law of California to Deepfake.",
    url       = "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730"
}
```


30. Regulations of China Internet Information Office on the control of online content. 2020 (in Chinese). http://www.cac.gov.cn/2019-12/20/c_1578375159509309.htm

Link : http://www.cac.gov.cn/2019-12/20/c_1578375159509309.htm

Note : 网络信息内容生态治理规定

```
国家互联网信息办公室令

第5号

　　《网络信息内容生态治理规定》已经国家互联网信息办公室室务会议审议通过，现予公布，自2020年3月1日起施行。 
...
```

Bibliography

```
@online{list1030,
    title     = "Regulations of China Internet Information Office on the control of online content.",
    url       = "http://www.cac.gov.cn/2019-12/20/c_1578375159509309.htm"
}
```


31. Nataraj L, Mohammed TM, Chandrasekaran S, Flenner A, Bappy JH, Roy-Chowdhury AK, Manjunath BS. Detecting GAN generated fake images using co-occurrence matrices. Electronic Imaging, 2019,2019(5):532-1−532-7.

Nataraj, L., Mohammed, T. M., Manjunath, B. S., Chandrasekaran, S., Flenner, A., Bappy, J. H., & Roy-Chowdhury, A. K. (2019). Detecting GAN generated fake images using co-occurrence matrices. Electronic Imaging, 2019(5), 532-1.

Link : https://arxiv.org/abs/1903.06836

Note : : GAN 生成技术改变了图像的像素和色度空间统计特征,通过对特征共生矩阵的学习来区分生成图像的差异

```
The advent of Generative Adversarial Networks (GANs) has brought about completely novel ways of transforming and manipulating pixels in digital images. 

GAN based techniques such as Image-to-Image translations, DeepFakes, and other automated methods have become increasingly popular in creating fake images. 

In this paper, we propose a novel approach to detect GAN generated fake images using a combination of co-occurrence matrices and deep learning. 

We extract co-occurrence matrices on three color channels in the pixel domain and train a model using a deep convolutional neural network (CNN) framework. 

Experimental results on two diverse and challenging GAN datasets comprising more than 56,000 images based on unpaired image-to-image translations (cycleGAN [1]) and facial attributes/expressions (StarGAN [2]) show that our approach is promising and achieves more than 99% classification accuracy in both datasets. 

Further, our approach also generalizes well and achieves good results when trained on one dataset and tested on the other.
```

生成對抗網絡 (GAN) 的出現帶來了全新的方式來轉換和操縱數位圖像中的像素，而基於 GAN 的技術，如圖像到圖像的轉換、DeepFakes 和其他自動化方法在創建假圖像方面變得越來越流行。在該研究中，研究者們提出了一種結合共現矩陣和深度學習來檢測 GAN 生成的假圖像的新方法，也就是在像素域中的三個顏色通道上提取共現矩陣，並使用深度卷積神經網絡 (CNN) 框架訓練模型。基於未配對的圖像到圖像轉換（cycleGAN ）和面部屬性/表情（StarGAN ）的兩個多樣化且具有挑戰性的 GAN 數據集包含超過 56,000 張圖像的實驗結果表明，該研究的方法很有前景，並取得了超過兩個數據集中的分類準確率為 99%。此外，當在一個數據集上訓練並在另一個數據集上進行測試時，此研究的方法也可以很好地泛化並取得良好的結果。

Bibliography

```
@article{nataraj2019detecting,
  title={Detecting GAN generated fake images using co-occurrence matrices},
  author={Nataraj, Lakshmanan and Mohammed, Tajuddin Manhar and Manjunath, BS and Chandrasekaran, Shivkumar and Flenner, Arjuna and Bappy, Jawadul H and Roy-Chowdhury, Amit K},
  journal={Electronic Imaging},
  volume={2019},
  number={5},
  pages={532--1},
  year={2019},
  publisher={Society for Imaging Science and Technology}
}
```


32. Li H, Li B, Tan S, Huang J. Identification of deep network generated images using disparities in color components. arXiv preprint arXiv:1808.07276, 2018.

Li, H., Li, B., Tan, S., & Huang, J. (2020). Identification of deep network generated images using disparities in color components. Signal Processing, 174, 107616.

Link : https://arxiv.org/abs/1808.07276

Note : GAN 生成技术改变了图像的像素和色度空间统计特征,通过对特征共生矩阵的学习来区分生成图像的差异

```
With the powerful deep network architectures, such as generative adversarial networks, one can easily generate photorealistic images. 

Although the generated images are not dedicated for fooling human or deceiving biometric authentication systems, research communities and public media have shown great concerns on the security issues caused by these images. 

This paper addresses the problem of identifying deep network generated (DNG) images. 

Taking the differences between camera imaging and DNG image generation into considerations, we analyze the disparities between DNG images and real images in different color components. 

We observe that the DNG images are more distinguishable from real ones in the chrominance components, especially in the residual domain. 

Based on these observations, we propose a feature set to capture color image statistics for identifying DNG images. 

Additionally, we evaluate several detection situations, including the training-testing data are matched or mismatched in image sources or generative models and detection with only real images. 

Extensive experimental results show that the proposed method can accurately identify DNG images and outperforms existing methods when the training and testing data are mismatched. 

Moreover, when the GAN model is unknown, our methods also achieves good performance with one-class classification by using only real images for training.
```

借助強大的深度網絡架構，例如生成對抗網絡，人們可以輕鬆生成逼真的圖像。儘管生成的圖像並非專門用於欺騙人類或欺騙生物特徵認證系統，但研究界和公共媒體對這些圖像引起的安全問題表示了極大的關注。該研究解決了識別深度網絡生成 (DNG) 圖像的問題，考慮到相機成像和DNG圖像生成之間的差異，研究者分析了不同顏色分量的DNG圖像和真實圖像之間的差異。其研究觀察到 DNG 圖像在色度分量中與真實圖像更容易區分，尤其是在殘差域中。基於這些觀察，研究者們提出了一個特徵集來捕獲用於識別 DNG 圖像的彩色圖像統計信息。此外，研究者評估了幾種檢測情況，包括訓練測試數據在圖像源或生成模型中匹配或不匹配，以及僅使用真實圖像進行檢測。大量實驗結果表明，該方法可以準確識別 DNG 圖像，並且在訓練和測試數據不匹配時優於現有方法。此外，當 GAN 模型未知時，研究者的方法也通過僅使用真實圖像進行訓練，實現了一類分類的良好性能。

Bibliography

```
@article{li2020identification,
  title={Identification of deep network generated images using disparities in color components},
  author={Li, Haodong and Li, Bin and Tan, Shunquan and Huang, Jiwu},
  journal={Signal Processing},
  volume={174},
  pages={107616},
  year={2020},
  publisher={Elsevier}
}
```


33. Xuan X, Peng B, Wang W, Dong J. On the generalization of GAN image forensics. In: Proc. of the Chinese Conf. on Biometric Recognition. Cham: Springer-Verlag, 2019. 134−141.

Xuan, X., Peng, B., Wang, W., & Dong, J. (2019, October). On the generalization of GAN image forensics. In Chinese conference on biometric recognition (pp. 134-141). Springer, Cham.

Link : https://paperswithcode.com/paper/on-the-generalization-of-gan-image-forensics

Note : Xuan 等人使用图像预处理,如滤波、噪音等预处理方法破坏 GAN 图像低级别的生成缺陷,迫使模型学习高级别的固有的线索.S

```
Recently the GAN generated face images are more and more realistic with high-quality, even hard for human eyes to detect. 

On the other hand, the forensics community keeps on developing methods to detect these generated fake images and try to guarantee the credibility of visual contents.

Although researchers have developed some methods to detect generated images, few of them explore the important problem of generalization ability of forensics model.

As new types of GANs are emerging fast, the generalization ability of forensics models to detect new types of GAN images is absolutely an essential research topic.

In this paper, we explore this problem and propose to use preprocessed images to train a forensic CNN model. 

By applying similar image level preprocessing to both real and fake training images, the forensics model is forced to learn more intrinsic features to classify the generated and real face images.

Our experimental results also prove the effectiveness of the proposed method.
```

最近 GAN 生成的人臉圖像越來越逼真，質量越來越高，甚至人眼也很難檢測到，另一方面，取證社區不斷開發檢測這些生成的虛假圖像的方法，並試圖保證視覺內容的可信度。儘管研究人員已經開發了一些檢測生成圖像的方法，但很少有人探索取證模型泛化能力的重要問題。隨著新型 GAN 的快速湧現，取證模型檢測新型 GAN 圖像的泛化能力絕對是一個必不可少的研究課題。在該研究中，研究者們探討了這個問題，並建議使用預處理圖像來訓練取證 CNN 模型。通過對真實和虛假的訓練圖像應用相似的圖像級預處理，取證模型被迫學習更多的內在特徵來對生成的和真實的人臉圖像進行分類。而其的實驗結果也證明了所提方法的有效性。

Bibliography

```
@inproceedings{xuan2019generalization,
  title={On the generalization of GAN image forensics},
  author={Xuan, Xinsheng and Peng, Bo and Wang, Wei and Dong, Jing},
  booktitle={Chinese conference on biometric recognition},
  pages={134--141},
  year={2019},
  organization={Springer}
}
```


34. McCloskey S, Albright M. Detecting GAN-generated imagery using color cues. arXiv preprint arXiv:1812.08247, 2018.

McCloskey, S., & Albright, M. (2018). Detecting gan-generated imagery using color cues. arXiv preprint arXiv:1812.08247.

Link : https://arxiv.org/abs/1812.08247

Note : 发现:GAN 生成器的中间值通常通过归一化来限制输出,这一定程度上也会限制饱和像素的频率.

```
Image forensics is an increasingly relevant problem, as it can potentially address online disinformation campaigns and mitigate problematic aspects of social media. 

Of particular interest, given its recent successes, is the detection of imagery produced by Generative Adversarial Networks (GANs), e.g. `deepfakes'. 

Leveraging large training sets and extensive computing resources, recent work has shown that GANs can be trained to generate synthetic imagery which is (in some ways) indistinguishable from real imagery. 

We analyze the structure of the generating network of a popular GAN implementation, and show that the network's treatment of color is markedly different from a real camera in two ways. 

We further show that these two cues can be used to distinguish GAN-generated imagery from camera imagery, demonstrating effective discrimination between GAN imagery and real camera images used to train the GAN.
```

圖像取證是一個越來越相關的問題，因為它可以潛在地解決在線虛假信息活動並減輕社交媒體的問題方面。鑑於其最近的成功，特別令人感興趣的是由生成對抗網絡 (GAN) 生成的圖像的檢測，例如 '深度偽造'，其利用大型訓練集和廣泛的計算資源，最近的工作表明，可以訓練 GAN 生成合成圖像，這（在某些方面）與真實圖像無法區分，研究者們分析了一個流行的 GAN 實現的生成網絡的結構，並表明該網絡對顏色的處理在兩個方面與真實相機明顯不同。該研究進一步表明，這兩個線索可用於區分 GAN 生成的圖像和相機圖像，證明了 GAN 圖像和用於訓練 GAN 的真實相機圖像之間的有效區分。

Bibliography

```
@article{mccloskey2018detecting,
  title={Detecting gan-generated imagery using color cues},
  author={McCloskey, Scott and Albright, Michael},
  journal={arXiv preprint arXiv:1812.08247},
  year={2018}
}
```


35. Marra F, Gragnaniello D, Verdoliva L, Poggi G. Do GANs leave artificial fingerprints? In: Proc. of the IEEE Conf. on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2019. 506−511.

Marra, F., Gragnaniello, D., Verdoliva, L., & Poggi, G. (2019, March). Do gans leave artificial fingerprints?. In 2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR) (pp. 506-511). IEEE.

Link : https://arxiv.org/abs/1812.11842

Note : 指纹来区分伪造,不同的 GAN 生成的图片在中间分类层具有唯一的特征,可以作为 GAN 生成器的辨别指纹.

```
In the last few years, generative adversarial networks (GAN) have shown tremendous potential for a number of applications in computer vision and related fields. 

With the current pace of progress, it is a sure bet they will soon be able to generate high-quality images and videos, virtually indistinguishable from real ones. 

Unfortunately, realistic GAN-generated images pose serious threats to security, to begin with a possible flood of fake multimedia, and multimedia forensic countermeasures are in urgent need. 

In this work, we show that each GAN leaves its specific fingerprint in the images it generates, just like real-world cameras mark acquired images with traces of their photo-response non-uniformity pattern. 

Source identification experiments with several popular GANs show such fingerprints to represent a precious asset for forensic analyses.
```

在過去的幾年裡，生成對抗網絡（GAN）在計算機視覺和相關領域的許多應用中顯示出巨大的潛力，以目前的發展速度，可以肯定的是，GAN 很快就能生成與真實圖像和視頻幾乎無法區分的高質量圖像和視頻。不幸的是，真實的 GAN 生成的圖像對安全構成了嚴重威脅，首先可能出現大量虛假多媒體，因此迫切需要多媒體取證對策。在該項工作中，研究者們展示了每個 GAN 在其生成的圖像中留下其特定的指紋，就像現實世界的相機用它們的照片響應非均勻模式的痕跡標記所獲取的圖像一樣。幾種流行的 GAN 的源識別實驗表明，這種指紋代表了類似於法醫分析的寶貴資產。

Bibliography

```
@inproceedings{marra2019gans,
  title={Do gans leave artificial fingerprints?},
  author={Marra, Francesco and Gragnaniello, Diego and Verdoliva, Luisa and Poggi, Giovanni},
  booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
  pages={506--511},
  year={2019},
  organization={IEEE}
}
```


36. Yu N, Davis LS, Fritz M. Attributing fake images to GANs: Learning and analyzing GAN fingerprints. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 7556−7566.

Yu, N., Davis, L. S., & Fritz, M. (2019). Attributing fake images to gans: Learning and analyzing gan fingerprints. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 7556-7566).

Link : https://arxiv.org/abs/1811.08180

Note : 指纹来区分伪造,不同的 GAN 生成的图片在中间分类层具有唯一的特征,可以作为 GAN 生成器的辨别指纹.

```
Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. 

But they also raise challenges to visual forensics and model attribution. 

We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. 

For GAN-generated images, we further identify their sources. 

Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; 

(2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; 

(3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; 

(4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations;

 and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.
```

生成對抗網絡 (GAN) 的最新進展表明，在生成逼真的圖像方面取得了越來越大的成功，但 GAN 也對視覺取證和模型歸因提出了挑戰。該研究提出了學習 GAN 指紋對圖像屬性的第一項研究，並使用它們將圖像分類為真實圖像或 GAN 生成的圖像，而對於 GAN 生成的圖像，研究者進一步識別它們的來源，其研究的實驗表明（1）GAN 帶有不同的模型指紋，並在其生成的圖像中留下穩定的指紋，從而支持圖像屬性；(2) GAN 訓練中即使是微小的差異也會導致不同的指紋，從而實現細粒度的模型認證；(3) 指紋在不同的圖像頻率和補丁上持續存在，並且不受 GAN 偽影的影響； (4) 指紋微調對五種對抗性圖像擾動有效免疫；(5) 比較還表明，研究者的學習的指紋在各種設置中始終優於幾個基線。

Bibliography

```
@inproceedings{yu2019attributing,
  title={Attributing fake images to gans: Learning and analyzing gan fingerprints},
  author={Yu, Ning and Davis, Larry S and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7556--7566},
  year={2019}
}
```


37. Wang R, Ma L, Juefei-Xu F, Xie X, Wang J, Liu Y. Fakespotter: A simple baseline for spotting ai-synthesized fake faces. In: Proc. of the 29th Int’l Joint Conf. on Artifical Intelligence (IJCAI). 2020. 3444−3451.

Wang, R., Ma, L., Juefei-Xu, F., Xie, X., Wang, J., & Liu, Y. Fakespotter: A simple baseline for spotting ai-synthesized fake faces. arXiv 2019. arXiv preprint arXiv:1909.06122.

Link : https://arxiv.org/abs/1909.06122

Note : 提出了 FakeSpotter,利用神经元监控的方法来进行分类.使用神经元覆盖的方法观察真假图像经过人脸识别器中的神经元激活变化情况,用 SVM 去学习神经元激活的差异,而假脸在神经
元覆盖的行为上表示相似.

```
In recent years, generative adversarial networks (GANs) and its variants have achieved unprecedented success in image synthesis. 

They are widely adopted in synthesizing facial images which brings potential security concerns to humans as the fakes spread and fuel the misinformation. 

However, robust detectors of these AI-synthesized fake faces are still in their infancy and are not ready to fully tackle this emerging challenge. 

In this work, we propose a novel approach, named FakeSpotter, based on monitoring neuron behaviors to spot AI-synthesized fake faces. 

The studies on neuron coverage and interactions have successfully shown that they can be served as testing criteria for deep learning systems, especially under the settings of being exposed to adversarial attacks. 

Here, we conjecture that monitoring neuron behavior can also serve as an asset in detecting fake faces since layer-by-layer neuron activation patterns may capture more subtle features that are important for the fake detector. 

Experimental results on detecting four types of fake faces synthesized with the state-of-the-art GANs and evading four perturbation attacks show the effectiveness and robustness of our approach.
```

近年來，生成對抗網絡（GAN）及其變體在圖像合成方面取得了前所未有的成功，它們被廣泛用於合成面部圖像，這會給人類帶來潛在的安全問題，因為其假的成果會傳播並助長錯誤信息。然而，這些人工智能合成的假人臉的強大檢測器仍處於起步階段，還沒有準備好完全應對這一新興挑戰。在這項工作中，研究者們提出了一種名為 FakeSpotter 的新方法，該方法基於監視神經元行為來發現 AI 合成的假臉，對神經元覆蓋和交互的研究已經成功地表明，它們可以作為深度學習系統的測試標準，尤其是在暴露於對抗性攻擊的情況下。在這裡，研究者們推測監控神經元行為也可以作為檢測假臉的資產，因為逐層神經元激活模式可能會捕獲對假人檢測器很重要的更細微的特徵。檢測用最先進的 GAN 合成的四種假人臉並避免四種擾動攻擊的實驗結果表明了該研究方法的有效性和魯棒性。

Bibliography

```
@article{wang1909fakespotter,
  title={Fakespotter: A simple baseline for spotting ai-synthesized fake faces. arXiv 2019},
  author={Wang, R and Ma, L and Juefei-Xu, F and Xie, X and Wang, J and Liu, Y},
  journal={arXiv preprint arXiv:1909.06122}
}
```


38. Chollet F. Xception: Deep learning with depthwise separable convolutions. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2017. 1251−1258.

Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1251-1258).

Link : https://arxiv.org/abs/1610.02357

Note : 利用 Xception 架构对视频的全帧和人脸分别训练.结果显示,基于人脸训练的模型效果远远好于全帧模型.同时,实验结果显示:在面对高度压缩的图片时,模型的训练难度会上升且检测率会下降

```
We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution).

In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. 

This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. 

We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. 

Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.
```

該研究將捲積神經網絡中的 Inception 模塊解釋為介於常規卷積和深度可分離卷積操作（深度卷積後跟點卷積）之間的中間步驟，從這個角度來看，深度可分離卷積可以理解為具有最大數量的塔的 Inception 模塊，這一觀察使研究者提出了一種受 Inception 啟發的新型深度卷積神經網絡架構，其中 Inception 模塊已被深度可分離卷積取代。其研究表明，這種被稱為 Xception 的架構在 ImageNet 數據集（Inception V3 的設計目標）上略微優於 Inception V3，並且在包含 3.5 億張圖像和 17,000 個類別的更大圖像分類數據集上顯著優於 Inception V3。由於 Xception 架構與 Inception V3 具有相同數量的參數，因此性能提升不是由於容量增加，而是更有效地使用模型參數。

Bibliography

```
@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}
```


39. Songsri-in K, Zafeiriou S. Complement face forensic detection and localization with faciallandmarks. arXiv preprint arXiv:1910. 05455, 2019.

Songsri-in, K., & Zafeiriou, S. (2019). Complement face forensic detection and localization with faciallandmarks. arXiv preprint arXiv:1910.05455.

Link : https://arxiv.org/abs/1910.05455

Note : 利用人脸关键点信息提升性能的结论也被 Songsri-in 等人实验证实

```
Recently, Generative Adversarial Networks (GANs) and image manipulating methods are becoming more powerful and can produce highly realistic face images beyond human recognition which have raised significant concerns regarding the authenticity of digital media. 

Although there have been some prior works that tackle face forensic classification problem, it is not trivial to estimate edited locations from classification predictions. 

In this paper, we propose, to the best of our knowledge, the first rigorous face forensic localization dataset, which consists of genuine, generated, and manipulated face images. 

In particular, the pristine parts contain face images from CelebA and FFHQ datasets. The fake images are generated from various GANs methods, namely DCGANs, LSGANs, BEGANs, WGAN-GP, ProGANs, and StyleGANs. Lastly, the edited subset is generated from StarGAN and SEFCGAN based on free-form masks. 

In total, the dataset contains about 1.3 million facial images labelled with corresponding binary masks.

Based on the proposed dataset, we demonstrated that explicit adding facial landmarks information in addition to input images improves the performance. 

In addition, our proposed method consists of two branches and can coherently predict face forensic detection and localization to outperform the previous state-of-the-art techniques on the newly proposed dataset as well as the faceforecsic++ dataset especially on low-quality videos.
```

最近，生成對抗網絡 (GAN) 和圖像處理方法變得越來越強大，可以產生超出人類識別範圍的高度逼真的人臉圖像，這引起了人們對數字媒體真實性的嚴重關注，儘管有一些先前的工作可以解決人臉取證分類問題，但從分類預測中估計編輯位置並非易事。在該研究中的研究者們所知，此研究是提出了第一個嚴格的人臉取證定位數據集，該數據集由真實的、生成的和經過處理的人臉圖像組成。特別是，原始部分包含來自 CelebA 和 FFHQ 數據集的人臉圖像。假圖像是由各種 GANs 方法生成的，即 DCGANs、LSGANs、BEGANs、WGAN-GP、ProGANs 和 StyleGANs。最後，編輯的子集是基於自由形式掩碼從 StarGAN 和 SEFCGAN 生成的。該數據集總共包含大約 130 萬張用相應的二進制掩碼標記的面部圖像。基於所提出的數據集，研究者們證明了在輸入圖像之外顯式添加面部標誌信息可以提高性能。此外，該研究提出的方法由兩個分支組成，可以連貫地預測人臉取證檢測和定位，以優於以前在新提出的數據集以及 faceforecsic++ 數據集上的最新技術，尤其是在低質量視頻上。

Bibliography

```
@article{songsri2019complement,
  title={Complement face forensic detection and localization with faciallandmarks},
  author={Songsri-in, Kritaphat and Zafeiriou, Stefanos},
  journal={arXiv preprint arXiv:1910.05455},
  year={2019}
}
```


40. Nguyen HH, Yamagishi J, Echizen I. Capsule-forensics: Using capsule networks to detect forged images and videos. In: Proc. of the IEEE Int’l Conf. on Acoustics, Speech and Signal Processing (ICASSP 2019). IEEE, 2019. 2307−2311.

Nguyen, H. H., Yamagishi, J., & Echizen, I. (2019, May). Capsule-forensics: Using capsule networks to detect forged images and videos. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2307-2311). IEEE.

Link : https://arxiv.org/pdf/1810.11215.pdf?ref=https://githubhelp.com

Note : Nguyen 等人设计了胶囊网络来判别造假的图片或视频,通过抽取人脸,用 VGG-19 提取特征编码,然后输入胶囊网络进行分类.

```
Recent advances in media generation techniques have made it easier for attackers to create forged images and videos. 

Stateof-the-art methods enable the real-time creation of a forged version of a single video obtained from a social network. 

Although numerous methods have been developed for detecting forged images and videos, they are generally targeted at certain domains and quickly become obsolete as new kinds of attacks appear. 

The method introduced in this paper uses a capsule network to detect various kinds of spoofs, from replay attacks using printed images or recorded videos to computergenerated videos using deep convolutional neural networks.

It extends the application of capsule networks beyond their original intention to the solving of inverse graphics problems.
```

媒體生成技術的最新進展使攻擊者更容易創建偽造的圖像和視頻，其最先進的方法可以實時創建從社交網絡獲得的單個視頻的偽造版本。儘管已經開發了許多用於檢測偽造圖像和視頻的方法，但它們通常針對某些領域，並且隨著新型攻擊的出現很快就過時了。該研究介紹的方法使用膠囊網絡來檢測各種欺騙，從使用打印圖像或錄製視頻的重放攻擊到使用深度卷積神經網絡的計算機生成視頻，它將膠囊網絡的應用擴展到解決逆圖形問題的初衷之外。

Bibliography

```
@inproceedings{nguyen2019capsule,
  title={Capsule-forensics: Using capsule networks to detect forged images and videos},
  author={Nguyen, Huy H and Yamagishi, Junichi and Echizen, Isao},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2307--2311},
  year={2019},
  organization={IEEE}
}
```


41. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: Proc. of the 3rd Int’l Conf. on Learning Representations (ICLR). 2015.

Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

Link : https://arxiv.org/abs/1409.1556

Note : VGG-19

```
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. 

Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. 

These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. 

We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. 

We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
```

在這項工作中，研究者們研究了卷積網絡深度對其在大規模圖像識別設置中的準確性的影響，該研究的主要貢獻是使用具有非常小的 (3x3) 卷積濾波器的架構對深度增加的網絡進行了全面評估，這表明通過將深度提升到 16-19 個權重層可以實現對現有技術配置的顯著改進，這些發現是研究者們提交 2014 年 ImageNet 挑戰賽的基礎，該研究團隊分別獲得了定位和分類軌道的第一和第二名。該研究還表明，其表示可以很好地推廣到其他數據集，並在這些數據集上取得了最先進的結果。

Bibliography

```
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
```


42. Mo H, Chen B, Luo W. Fake faces identification via convolutional neural network. In: Proc. of the 6th ACM Workshop on Information Hiding and Multimedia Security. 2018. 43−47.

Mo, H., Chen, B., & Luo, W. (2018, June). Fake faces identification via convolutional neural network. In Proceedings of the 6th ACM workshop on information hiding and multimedia security (pp. 43-47).

Link : https://dl.acm.org/doi/10.1145/3206004.3206009

Note : Mo 等人增加高通滤波和背景作为 CNN 输入,对检测结果有提升.

```
Generative Adversarial Network (GAN) is a prominent generative model that are widely used in various applications. 

Recent studies have indicated that it is possible to obtain fake face images with a high visual quality based on this novel model. 

If those fake faces are abused in image tampering, it would cause some potential moral, ethical and legal problems. 

In this paper, therefore, we first propose a Convolutional Neural Network (CNN) based method to identify fake face images generated by the current best method, and provide experimental evidences to show that the proposed method can achieve satisfactory results with an average accuracy over 99.4%. 

In addition, we provide comparative results evaluated on some variants of the proposed CNN architecture, including the high pass filter, the number of the layer groups and the activation function, to further verify the rationality of our method.
```

生成對抗網絡 (GAN) 是一種突出的生成模型，廣泛用於各種應用，最近的研究表明，基於這種新穎的模型可以獲得具有高視覺質量的假人臉圖像。如果這些假臉被濫用於圖像篡改，將導致一些潛在的道德、倫理和法律問題。因此，在該研究的研究者們首先提出了一種基於卷積神經網絡（CNN）的方法來識別當前最佳方法生成的假人臉圖像，並提供實驗證據表明該方法可以達到令人滿意的結果，平均準確率超過 99.4 %。此外，研究者們提供了對所提出的 CNN 架構的一些變體進行評估的比較結果，包括高通濾波器、層組數和激活函數，以進一步驗證我們方法的合理性。

Bibliography

```
@inproceedings{mo2018fake,
  title={Fake faces identification via convolutional neural network},
  author={Mo, Huaxiao and Chen, Bolin and Luo, Weiqi},
  booktitle={Proceedings of the 6th ACM workshop on information hiding and multimedia security},
  pages={43--47},
  year={2018}
}
```


43. Durall R, Keuper M, Pfreundt FJ, Keuper J. Unmasking DeepFakes with simple features. arXiv preprint arXiv:1911.00686, 2019.

Durall, R., Keuper, M., Pfreundt, F. J., & Keuper, J. (2019). Unmasking deepfakes with simple features. arXiv preprint arXiv:1911.00686.

Link : https://arxiv.org/abs/1911.00686

Note : Durall 等人通过离散傅里叶变换提取特征学习,显示了很好的效果.

```
Deep generative models have recently achieved impressive results for many real-world applications, successfully generating high-resolution and diverse samples from complex datasets. 

Due to this improvement, fake digital contents have proliferated growing concern and spreading distrust in image content, leading to an urgent need for automated ways to detect these AI-generated fake images.

Despite the fact that many face editing algorithms seem to produce realistic human faces, upon closer examination, they do exhibit artifacts in certain domains which are often hidden to the naked eye. 

In this work, we present a simple way to detect such fake face images - so-called DeepFakes. Our method is based on a classical frequency domain analysis followed by basic classifier. 

Compared to previous systems, which need to be fed with large amounts of labeled data, our approach showed very good results using only a few annotated training samples and even achieved good accuracies in fully unsupervised scenarios. 

For the evaluation on high resolution face images, we combined several public datasets of real and fake faces into a new benchmark: Faces-HQ. Given such high-resolution images, our approach reaches a perfect classification accuracy of 100% when it is trained on as little as 20 annotated samples. 

In a second experiment, in the evaluation of the medium-resolution images of the CelebA dataset, our method achieves 100% accuracy supervised and 96% in an unsupervised setting. 

Finally, evaluating a low-resolution video sequences of the FaceForensics++ dataset, our method achieves 91% accuracy detecting manipulated videos.
```

深度生成模型最近在許多實際應用中取得了令人矚目的成果，成功地從復雜的數據集中生成了高分辨率和多樣化的樣本。由於這種改進，虛假數位內容引起了越來越多的關注並傳播了對圖像內容的不信任，從而迫切需要自動化的方法來檢測這些 AI 生成的虛假圖像。儘管許多人臉編輯算法似乎可以生成逼真的人臉，但經過仔細檢查，它們確實在某些領域表現出偽影，而這些偽影通常是肉眼隱藏的。在這項工作中，研究者提出了一種檢測此類假人臉圖像的簡單方法——所謂的 DeepFakes，該研究的方法基於經典的頻域分析，然後是基本分類器。與需要輸入大量標記數據的先前系統相比，我們的方法僅使用少量帶註釋的訓練樣本就顯示出非常好的結果，甚至在完全無監督的情況下也取得了很好的準確性。對於高分辨率人臉圖像的評估，研究者將幾個真實和虛假人臉的公共數據集組合成一個新的基準：Faces-HQ。鑑於如此高分辨率的圖像，當該研究的方法在少至 20 個帶註釋的樣本上進行訓練時，其方法達到了 100% 的完美分類準確率。在第二個實驗中，在 CelebA 數據集的中等分辨率圖像的評估中，其方法在有監督的情況下達到了 100% 的準確率，在無監督的情況下達到了 96%。最後，評估 FaceForensics++ 數據集的低分辨率視頻序列，該研究的方法檢測操縱影片的準確率達到 91%。

Bibliography

```
@article{durall2019unmasking,
  title={Unmasking deepfakes with simple features},
  author={Durall, Ricard and Keuper, Margret and Pfreundt, Franz-Josef and Keuper, Janis},
  journal={arXiv preprint arXiv:1911.00686},
  year={2019}
}
```


44. Ding X, Raziei Z, Larson EC, Olinick EV, Krueger PS, Hahsler M. Swapped face detection using deep learning and subjective assessment. EURASIP Journal on Information Security, 2020(2020):Article No.6.

Ding, X., Raziei, Z., Larson, E. C., Olinick, E. V., Krueger, P., & Hahsler, M. (2020). Swapped face detection using deep learning and subjective assessment. EURASIP Journal on Information Security, 2020(1), 1-12.

Link : https://jis-eurasipjournals.springeropen.com/articles/10.1186/s13635-020-00109-8

Note : Ding 等人利用迁移学习,使用 Resnet18 进行调优;同时对于这些部署的关键系统,对每个预测提供一个不确定水平,如每个神经网层络输出值差异

```
The tremendous success of deep learning for imaging applications has resulted in numerous beneficial advances. 

Unfortunately, this success has also been a catalyst for malicious uses such as photo-realistic face swapping of parties without consent. 

In this study, we use deep transfer learning for face swapping detection, showing true positive rates greater than 96% with very few false alarms. 

Distinguished from existing methods that only provide detection accuracy, we also provide uncertainty for each prediction, which is critical for trust in the deployment of such detection systems. 

Moreover, we provide a comparison to human subjects. 

To capture human recognition performance, we build a website to collect pairwise comparisons of images from human subjects. 

Based on these comparisons, we infer a consensus ranking from the image perceived as most real to the image perceived as most fake. 

Overall, the results show the effectiveness of our method. 

As part of this study, we create a novel dataset that is, to the best of our knowledge, the largest swapped face dataset created using still images. 

This dataset will be available for academic research use per request. 

Our goal of this study is to inspire more research in the field of image forensics through the creation of a dataset and initial analysis.
```

深度學習在成像應用方面的巨大成功帶來了許多有益的進步。不幸的是，這一成功也成為了惡意使用的催化劑，例如未經同意就照片般逼真的各方換臉。在這項研究中，研究者們使用深度遷移學習進行人臉交換檢測，顯示出大於 96% 的真陽性率，並且誤報率非常低。與僅提供檢測準確性的現有方法不同，該研究還為每個預測提供不確定性，這對於信任此類檢測系統的部署至關重要。此外，研究者們提供了與人類受試者的比較。為了捕捉人類識別性能，此研究建立了一個網站來收集人類受試者圖像的成對比較。基於這些比較，研究者們推斷出從被認為最真實的圖像到被認為最假的圖像的共識排名，總體而言，結果顯示了該研究的方法有效性。作為這項研究的一部分，研究者們創建了一個新的數據集。


Bibliography

```
@article{ding2020swapped,
  title={Swapped face detection using deep learning and subjective assessment},
  author={Ding, Xinyi and Raziei, Zohreh and Larson, Eric C and Olinick, Eli V and Krueger, Paul and Hahsler, Michael},
  journal={EURASIP Journal on Information Security},
  volume={2020},
  number={1},
  pages={1--12},
  year={2020},
  publisher={SpringerOpen}
}
```


45. Cozzolino D, Thies J, Rössler A, Riess C, Niebner M, Verdoliva L. Forensictransfer: Weakly-supervised domain adaptation for forgery detection. arXiv preprint arXiv:1812.02510, 2018.

Cozzolino, D., Thies, J., Rössler, A., Riess, C., Nießner, M., & Verdoliva, L. (2018). Forensictransfer: Weakly-supervised domain adaptation for forgery detection. arXiv preprint arXiv:1812.02510.

Link : https://arxiv.org/abs/1812.02510

Note : Cozzolino 等人设计了一个新的基于自动编码器的神经网络结构,能够学习在不同的扰动域下的编码能力,只需要在一个数据集上训练,在另一个数据集上获取小规模进行调优,就能达到很好的效果.

```
Distinguishing manipulated from real images is becoming increasingly difficult as new sophisticated image forgery approaches come out by the day. 

Naive classification approaches based on Convolutional Neural Networks (CNNs) show excellent performance in detecting image manipulations when they are trained on a specific forgery method. 

However, on examples from unseen manipulation approaches, their performance drops significantly. 

To address this limitation in transferability, we introduce Forensic-Transfer (FT). 

We devise a learning-based forensic detector which adapts well to new domains, i.e., novel manipulation methods and can handle scenarios where only a handful of fake examples are available during training. 

To this end, we learn a forensic embedding based on a novel autoencoder-based architecture that can be used to distinguish between real and fake imagery. 

The learned embedding acts as a form of anomaly detector; namely, an image manipulated from an unseen method will be detected as fake provided it maps sufficiently far away from the cluster of real images. 

Comparing to prior works, FT shows significant improvements in transferability, which we demonstrate in a series of experiments on cutting-edge benchmarks. 

For instance, on unseen examples, we achieve up to 85% in terms of accuracy, and with only a handful of seen examples, our performance already reaches around 95%.
```

隨著新的複雜圖像偽造方法的出現，區分偽造圖像和真實圖像變得越來越困難，基於卷積神經網絡 (CNN) 的樸素分類方法在使用特定的偽造方法進行訓練時，在檢測圖像操作方面表現出出色的性能。

然而，在來自看不見的操縱方法的例子中，它們的性能顯著下降，而為了解決可轉移性的這種限制，此研究引入了取證轉移（FT）。研究者們設計了一種基於學習的取證檢測器，它可以很好地適應新領域，即新穎的操作方法，並且可以處理在訓練期間只有少數假樣本可用的場景。為此，研究者學習了一種基於新型自動編碼器架構的取證嵌入，該架構可用於區分真假圖像，其學習嵌入充當異常檢測器的一種形式；即，如果從不可見的方法處理的圖像與真實圖像集群足夠遠，則該圖像將被檢測為假圖像。與之前的工作相比，FT 顯示出可遷移性的顯著改進，該研究在一系列關於尖端基準的實驗中證明了這一點。例如，在未見過的例子上，研究者們的準確率高達 85%，而只有少數可見的例子，該研究性能已經達到了 95% 左右。

Bibliography

```
@article{cozzolino2018forensictransfer,
  title={Forensictransfer: Weakly-supervised domain adaptation for forgery detection},
  author={Cozzolino, Davide and Thies, Justus and R{\"o}ssler, Andreas and Riess, Christian and Nie{\ss}ner, Matthias and Verdoliva, Luisa},
  journal={arXiv preprint arXiv:1812.02510},
  year={2018}
}
```


46. Nguyen HH, Fang F, Yamagishi J, Echizen I. Multi-task learning for detecting and segmenting manipulated facial images and videos. arXiv preprint arXiv:1906.06876, 2019.

Nguyen, H. H., Fang, F., Yamagishi, J., & Echizen, I. (2019). Multi-task learning for detecting and segmenting manipulated facial images and videos. arXiv preprint arXiv:1906.06876.

Link : https://arxiv.org/abs/1906.06876

Note : Nguyen 等人设计了 Y 型解码器,在分类的同时融入分割和重建损失,通过分割辅助分类效果

```
Detecting manipulated images and videos is an important topic in digital media forensics. 

Most detection methods use binary classification to determine the probability of a query being manipulated. 

Another important topic is locating manipulated regions (i.e., performing segmentation), which are mostly created by three commonly used attacks: removal, copy-move, and splicing. 

We have designed a convolutional neural network that uses the multi-task learning approach to simultaneously detect manipulated images and videos and locate the manipulated regions for each query. 

Information gained by performing one task is shared with the other task and thereby enhance the performance of both tasks. 

A semi-supervised learning approach is used to improve the network's generability. 

The network includes an encoder and a Y-shaped decoder. Activation of the encoded features is used for the binary classification. 

The output of one branch of the decoder is used for segmenting the manipulated regions while that of the other branch is used for reconstructing the input, which helps improve overall performance. 

Experiments using the FaceForensics and FaceForensics++ databases demonstrated the network's effectiveness against facial reenactment attacks and face swapping attacks as well as its ability to deal with the mismatch condition for previously seen attacks. 

Moreover, fine-tuning using just a small amount of data enables the network to deal with unseen attacks.

```

檢測被操縱的圖像和視頻是數字媒體取證中的一個重要課題，大多數檢測方法使用二進制分類來確定查詢被操縱的概率，另一個重要主題是定位被操縱區域（即執行分割），這主要是由三種常用攻擊創建的：刪除、複製移動和拼接。研究者們設計了一個卷積神經網絡，它使用多任務學習方法來同時檢測被操縱的圖像和視頻，並為每個查詢定位被操縱的區域，其通過執行一項任務獲得的信息與另一項任務共享，從而提高兩項任務的性能。另外使用半監督學習方法來提高網絡的可生成性，該網絡包括一個編碼器和一個 Y 形解碼器。編碼特徵的激活用於二進制分類，其解碼器一個分支的輸出用於分割操作區域，而另一個分支的輸出用於重構輸入，這有助於提高整體性能。使用 FaceForensics 和 FaceForensics++ 數據庫的實驗證明了該網絡對面部重演攻擊和面部交換攻擊的有效性，以及它處理先前看到的攻擊的不匹配條件的能力。此外，僅使用少量數據進行微調使網絡能夠處理看不見的攻擊。

Bibliography

```
@article{nguyen2019multi,
  title={Multi-task learning for detecting and segmenting manipulated facial images and videos},
  author={Nguyen, Huy H and Fang, Fuming and Yamagishi, Junichi and Echizen, Isao},
  journal={arXiv preprint arXiv:1906.06876},
  year={2019}
}
```


47. Hsu CC, Lee CY, Zhuang YX. Learning to detect fake face images in the wild. In: Proc. of the Int’l Symp. on Computer, Consumer and Control (IS3C). IEEE, 2018. 388−391.

Hsu, C. C., Lee, C. Y., & Zhuang, Y. X. (2018, December). Learning to detect fake face images in the wild. In 2018 International Symposium on Computer, Consumer and Control (IS3C) (pp. 388-391). IEEE.

Link : https://arxiv.org/abs/1809.08754

Note : Hsu 等人采用对比损失寻找不同生成器生成的图像的特征,后面再连接一个分类器进行分类

```
Although Generative Adversarial Network (GAN) can be used to generate the realistic image, improper use of these technologies brings hidden concerns. 

For example, GAN can be used to generate a tampered video for specific people and inappropriate events, creating images that are detrimental to a particular person, and may even affect that personal safety. 

In this paper, we will develop a deep forgery discriminator (DeepFD) to efficiently and effectively detect the computer-generated images. 

Directly learning a binary classifier is relatively tricky since it is hard to find the common discriminative features for judging the fake images generated from different GANs. 

To address this shortcoming, we adopt contrastive loss in seeking the typical features of the synthesized images generated by different GANs and follow by concatenating a classifier to detect such computer-generated images. 

Experimental results demonstrate that the proposed DeepFD successfully detected 94.7% fake images generated by several state-of-the-art GANs.
```

雖然生成對抗網絡 (GAN) 可用於生成逼真的圖像，但這些技術的不當使用會帶來隱藏的問題，例如，GAN 可用於為特定人員和不當事件生成篡改視頻，創建對特定人員有害的圖像，甚至可能影響人身安全，該研究的研究者們將開發一種深度偽造鑑別器（DeepFD）來有效地檢測計算機生成的圖像，其直接學習二元分類器相對比較棘手，因為很難找到共同的判別特徵來判斷不同 GAN 生成的假圖像。為了解決這個缺點，研究者採用對比損失來尋找由不同 GAN 生成的合成圖像的典型特徵，然後連接一個分類器來檢測這些計算機生成的圖像，實驗結果表明，所提出的 DeepFD 成功檢測到由幾個最先進的 GAN 生成的 94.7% 的假圖像。

Bibliography

```
@inproceedings{hsu2018learning,
  title={Learning to detect fake face images in the wild},
  author={Hsu, Chih-Chung and Lee, Chia-Yen and Zhuang, Yi-Xiu},
  booktitle={2018 International Symposium on Computer, Consumer and Control (IS3C)},
  pages={388--391},
  year={2018},
  organization={IEEE}
}
```


48. Hsu CC, Zhuang YX, Lee CY. Deep fake image detection based on pairwise learning. Applied Sciences, 2020,10(1):Article No.370.

Hsu, C. C., Zhuang, Y. X., & Lee, C. Y. (2020). Deep fake image detection based on pairwise learning. Applied Sciences, 10(1), 370.

Link : https://www.researchgate.net/publication/338382561_Deep_Fake_Image_Detection_Based_on_Pairwise_Learning

Note : Hsu 等人采用对比损失寻找不同生成器生成的图像的特征,后面再连接一个分类器进行分类

```
Generative adversarial networks (GANs) can be used to generate a photo-realistic image from a low-dimension random noise. 

Such a synthesized (fake) image with inappropriate content can be used on social media networks, which can cause severe problems. 

With the aim to successfully detect fake images, an effective and efficient image forgery detector is necessary. 

However, conventional image forgery detectors fail to recognize fake images generated by the GAN-based generator since these images are generated and manipulated from the source image. 

Therefore, in this paper, we propose a deep learning-based approach for detecting the fake images by using the contrastive loss. 

First, several state-of-the-art GANs are employed to generate the fake–real image pairs. Next, the reduced DenseNet is developed to a two-streamed network structure to allow pairwise information as the input. 

Then, the proposed common fake feature network is trained using the pairwise learning to distinguish the features between the fake and real images. 

Finally, a classification layer is concatenated to the proposed common fake feature network to detect whether the input image is fake or real. 

The experimental results demonstrated that the proposed method significantly outperformed other state-of-the-art fake image detectors.
```

生成對抗網絡 (GAN) 可用於從低維隨機噪聲生成照片般逼真的圖像，這種帶有不適當內容的合成（假）圖像可以在社交媒體網絡上使用，這可能會導致嚴重的問題，為了成功檢測假圖像，有效且高效的圖像偽造檢測器是必要的。然而，傳統的圖像偽造檢測器無法識別由基於 GAN 的生成器生成的假圖像，因為這些圖像是從源圖像生成和操縱的。因此，在本文中，研究者們提出了一種基於深度學習的方法，通過使用對比損失來檢測假圖像。首先，採用幾種最先進的 GAN 來生成假-真圖像對。接下來，將簡化的 DenseNet 發展為雙流網絡結構，以允許成對信息作為輸入。然後，使用成對學習來訓練所提出的常見假特徵網絡，以區分假圖像和真實圖像之間的特徵。最後，將分類層連接到所提出的常見假特徵網絡，以檢測輸入圖像是假的還是真的，其實驗結果表明，所提出的方法明顯優於其他最先進的假圖像檢測器。

Bibliography

```
@article{hsu2020deep,
  title={Deep fake image detection based on pairwise learning},
  author={Hsu, Chih-Chung and Zhuang, Yi-Xiu and Lee, Chia-Yen},
  journal={Applied Sciences},
  volume={10},
  number={1},
  pages={370},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
```


49. Dang LM, Hassan SI, Im S, Lee J, Lee S, Moon H. Deep learning based computer generated face identification using convolutional neural network. Applied Sciences, 2018,8(12):Article No.2610.

Dang, L. M., Hassan, S. I., Im, S., Lee, J., Lee, S., & Moon, H. (2018). Deep learning based computer generated face identification using convolutional neural network. Applied Sciences, 8(12), 2610.

Link : https://www.researchgate.net/publication/329652579_Deep_Learning_Based_Computer_Generated_Face_Identification_Using_Convolutional_Neural_Network

Note : Dang 等人设计了特定的 CGFace 网路,专门检测计算机生成的人脸;

```
Generative adversarial networks (GANs) describe an emerging generative model which has made impressive progress in the last few years in generating photorealistic facial images. 

As the result, it has become more and more difficult to differentiate between computer-generated and real face images, even with the human’s eyes. 

If the generated images are used with the intent to mislead and deceive readers, it would probably cause severe ethical, moral, and legal issues. 

Moreover, it is challenging to collect a dataset for computer-generated face identification that is large enough for research purposes because the number of realistic computer-generated images is still limited and scattered on the internet. 

Thus, a development of a novel decision support system for analyzing and detecting computer-generated face images generated by the GAN network is crucial. 

In this paper, we propose a customized convolutional neural network, namely CGFace, which is specifically designed for the computer-generated face detection task by customizing the number of convolutional layers, so it performs well in detecting computer-generated face images. 

After that, an imbalanced framework (IF-CGFace) is created by altering CGFace’s layer structure to adjust to the imbalanced data issue by extracting features from CGFace layers and use them to train AdaBoost and eXtreme Gradient Boosting (XGB). 

Next, we explain the process of generating a large computer-generated dataset based on the state-of-the-art PCGAN and BEGAN model. 

Then, various experiments are carried out to show that the proposed model with augmented input yields the highest accuracy at 98%. 

Finally, we provided comparative results by applying the proposed CNN architecture on images generated by other GAN researches.
```

生成對抗網絡 (GAN) 描述了一種新興的生成模型，該模型在過去幾年中在生成逼真的面部圖像方面取得了令人矚目的進展，結果，即使是用人眼，也越來越難以區分計算機生成的人臉圖像和真實的人臉圖像，如果生成的圖像用於誤導和欺騙讀者，可能會導致嚴重的倫理、道德和法律問題。此外，收集足夠大的用於研究目的的計算機生成人臉識別數據集具有挑戰性，因為現實的計算機生成圖像的數量仍然有限且分散在互聯網上，因此，開發一種用於分析和檢測由 GAN 網絡生成的計算機生成的人臉圖像的新型決策支持系統至關重要。在該研究的研究者們提出了一種定制的捲積神經網絡，即 CGFace，它是專門為計算機生成的人臉檢測任務而設計的，通過自定義卷積層的數量，因此在檢測計算機生成的人臉圖像方面表現良好。之後，通過從 CGFace 層中提取特徵並使用它們來訓練 AdaBoost 和 eXtreme Gradient Boosting (XGB) 來改變 CGFace 的層結構以適應不平衡數據問題，從而創建了一個不平衡框架 (IF-CGFace)。接下來，研究者們將解釋基於最先進的 PCGAN 和 BEGAN 模型生成大型計算機生成數據集的過程。而隨後的進行了各種實驗也表明所提出的具有增強輸入的模型產生了 98% 的最高精度。最後，研究者們通過將所提出的 CNN 架構應用於其他 GAN 研究生成的圖像來提供比較結果。

Bibliography

```
@article{dang2018deep,
  title={Deep learning based computer generated face identification using convolutional neural network},
  author={Dang, L Minh and Hassan, Syed Ibrahim and Im, Suhyeon and Lee, Jaecheol and Lee, Sujin and Moon, Hyeonjoon},
  journal={Applied Sciences},
  volume={8},
  number={12},
  pages={2610},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
```


50. Bayar B, Stamm MC. A deep learning approach to universal image manipulation detection using a new convolutional layer. In: Proc. of the 4th ACM Workshop on Information Hiding and Multimedia Security. 2016. 5−10.

Bayar, B., & Stamm, M. C. (2016, June). A deep learning approach to universal image manipulation detection using a new convolutional layer. In Proceedings of the 4th ACM workshop on information hiding and multimedia security (pp. 5-10).

Link : https://misl.ece.drexel.edu/wp-content/uploads/2017/07/Bayar_IHMMSec_2016.pdf

Note : Bayar 等人设计了受限制的卷积层学习特定的篡改特征

```
When creating a forgery, a forger can modify an image using many different image editing operations. 

Since a forensic examiner must test for each of these, significant interest has arisen in the development of universal forensic algorithms capable of detecting many different image editing operations and manipulations. 

In this paper, we propose a universal forensic approach to performing manipulation detection using deep learning. 

Specifically, we propose a new convolutional network architecture capable of automatically learning manipulation detection features directly from training data. 

In their current form, convolutional neural networks will learn features that capture an image’s content as opposed to manipulation detection features. 

To overcome this issue, we develop a new form of convolutional layer that is specifically designed to suppress an image’s content and adaptively learn manipulation detection features. 

Through a series of experiments, we demonstrate that our proposed approach can automatically learn how to detect multiple image manipulations without relying on pre-selected features or any preprocessing. 

The results of these experiments show that our proposed approach can automatically detect several different manipulations with an average accuracy of 99.10%.
```

創建偽造品時，偽造者可以使用許多不同的圖像編輯操作來修改圖像，由於法醫檢查員必須對其中的每一項進行測試，因此人們對開發能夠檢測許多不同圖像編輯操作和操作的通用法醫算法產生了極大的興趣，該研究的研究者們提出了一種通用的取證方法來使用深度學習執行操作檢測。具體來說，研究者們提出了一種新的捲積網絡架構，能夠直接從訓練數據中自動學習操作檢測特徵，而在目前的形式中，卷積神經網絡將學習捕捉圖像內容的特徵，而不是操作檢測特徵。為了克服這個問題，該研究開發了一種新形式的捲積層，專門用於抑製圖像的內容並自適應地學習操作檢測特徵，通過一系列實驗，其研究證明了研究者們提出的方法可以自動學習如何檢測多個圖像操作，而不依賴於預先選擇的特徵或任何預處理。這些實驗的結果表明，該研究提出的方法可以自動檢測幾種不同的操作，平均準確率為 99.10%。

Bibliography

```
@inproceedings{bayar2016deep,
  title={A deep learning approach to universal image manipulation detection using a new convolutional layer},
  author={Bayar, Belhassen and Stamm, Matthew C},
  booktitle={Proceedings of the 4th ACM workshop on information hiding and multimedia security},
  pages={5--10},
  year={2016}
}
```


51. Dang H, Liu F, Stehouwer J, Liu X, Jain A. On the detection of digital face manipulation. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020. 5780−5789.

Dang, H., Liu, F., Stehouwer, J., Liu, X., & Jain, A. K. (2020). On the detection of digital face manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition (pp. 5781-5790).

Link : https://arxiv.org/abs/1910.01717

Note : CNN+Attention, 增加注意力机制, DFFD,;Stehouwer 等人通过在主干网络增加注意力机制来聚焦篡改区域;

```
Detecting manipulated facial images and videos is an increasingly important topic in digital media forensics. 

As advanced face synthesis and manipulation methods are made available, new types of fake face representations are being created which have raised significant concerns for their use in social media. 

Hence, it is crucial to detect manipulated face images and localize manipulated regions. 

Instead of simply using multi-task learning to simultaneously detect manipulated images and predict the manipulated mask (regions), we propose to utilize an attention mechanism to process and improve the feature maps for the classification task. 

The learned attention maps highlight the informative regions to further improve the binary classification (genuine face v. fake face), and also visualize the manipulated regions. 

To enable our study of manipulated face detection and localization, we collect a large-scale database that contains numerous types of facial forgeries. 

With this dataset, we perform a thorough analysis of data-driven fake face detection. 

We show that the use of an attention mechanism improves facial forgery detection and manipulated region localization.
```

檢測被操縱的面部圖像和視頻是數字媒體取證中越來越重要的主題。隨著先進的人臉合成和操作方法的出現，新類型的假人臉表示正在被創造出來，這引起了人們對它們在社交媒體中的使用的極大關注。因此，檢測操縱的人臉圖像和定位操縱區域至關重要。該研究建議利用注意力機制來處理和改進分類任務的特徵圖，而不是簡單地使用多任務學習來同時檢測操縱圖像和預測操縱掩碼（區域），其學習到的注意力圖突出顯示信息區域以進一步改進二元分類（真人臉與假人臉），並可視化操作區域，為了使研究者能夠研究操縱的面部檢測和定位，該研究收集了一個包含多種類型的面部偽造的大型數據庫。同時使用這個數據集的過程中，研究者對數據驅動的假人臉檢測進行了徹底的分析，成果展示了注意力機制的使用改進了面部偽造檢測和操縱區域定位。

Bibliography

```
@inproceedings{dang2020detection,
  title={On the detection of digital face manipulation},
  author={Dang, Hao and Liu, Feng and Stehouwer, Joel and Liu, Xiaoming and Jain, Anil K},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition},
  pages={5781--5790},
  year={2020}
}
```


52. Rahmouni N, Nozick V, Yamagishi J, Echizen I. Distinguishing computer graphics from natural images using convolution neural networks. In: Proc. of the IEEE Workshop on Information Forensics and Security (WIFS). IEEE, 2017. 1−6.

Rahmouni, N., Nozick, V., Yamagishi, J., & Echizen, I. (2017, December). Distinguishing computer graphics from natural images using convolution neural networks. In 2017 IEEE Workshop on Information Forensics and Security (WIFS) (pp. 1-6). IEEE.

Link : http://www-igm.univ-mlv.fr/~vnozick/publications/Rahmouni_WIFS_2017/Rahmouni_WIFS_2017.pdf

Note : Rahmouni 等人加入了计算统计数据的全局池化层.

```
This paper presents a deep-learning method for distinguishing computer generated graphics from real photographic images. 

The proposed method uses a Convolutional Neural Network (CNN) with a custom pooling layer to optimize current best-performing algorithms feature extraction scheme. 

Local estimates of class probabilities are computed and aggregated to predict the label of the whole picture. 

We evaluate our work on recent photo-realistic computer graphics and show that it outperforms state of the art methods for both local and full image classification.
```

該研究提出了一種深度學習方法，用於將計算機生成的圖形與真實的攝影圖像區分開來。所提出的方法使用帶有自定義池化層的捲積神經網絡 (CNN) 來優化當前性能最佳的算法特徵提取方案，來計算和聚合類概率的局部估計以預測整個圖片的標籤，研究者評估了我們在最近的照片般逼真的計算機圖形方面的工作，並表明它在局部和完整圖像分類方面都優於最先進的方法。

Bibliography

```
@inproceedings{rahmouni2017distinguishing,
  title={Distinguishing computer graphics from natural images using convolution neural networks},
  author={Rahmouni, Nicolas and Nozick, Vincent and Yamagishi, Junichi and Echizen, Isao},
  booktitle={2017 IEEE Workshop on Information Forensics and Security (WIFS)},
  pages={1--6},
  year={2017},
  organization={IEEE}
}
```


53. Li X, Yu K, Ji S, Wang Y, Wu C, Xue H. Fighting against Deepfake: Patch&Pair convolutional neural networks (PPCNN). In: Proc. of the Companion Web Conf. 2020. 2020. 88−89.

Li, X., Yu, K., Ji, S., Wang, Y., Wu, C., & Xue, H. (2020, April). Fighting against deepfake: Patch&pair convolutional neural networks (PPCNN). In Companion Proceedings of the Web Conference 2020 (pp. 88-89).

Link : https://dl.acm.org/doi/fullHtml/10.1145/3366424.3382711

Note : 设计了基于图片块的双流网路框架,一条流学习人脸块的微观特征,另一条流学习人脸和背景区域的差异性.通过多任务学习,能够较好地提升模型的泛化能力. 

```
In this paper, we propose a novel Patch&Pair Convolutional Neural Networks (PPCNN) to distinguish Deepfake videos or images from real ones. 

Through the comprehensive evaluations on public datasets, we demonstrate that our model performs better than existing detection methods and show better generalization.
```

該研究提出了一種新穎的 Patch&Pair 卷積神經網絡 (PPCNN) 來區分 Deepfake 視頻或圖像與真實視頻或圖像，其通過對公共數據集的綜合評估，研究者證明其研究的模型比現有的檢測方法表現更好，並表現出更好的泛化性。

Bibliography

```
@inproceedings{li2020fighting,
  title={Fighting against deepfake: Patch\&pair convolutional neural networks (PPCNN)},
  author={Li, Xurong and Yu, Kun and Ji, Shouling and Wang, Yan and Wu, Chunming and Xue, Hui},
  booktitle={Companion Proceedings of the Web Conference 2020},
  pages={88--89},
  year={2020}
}
```


54. Brockschmidt J, Shang J, Wu J. On the generality of facial forgery detection. In: Proc. of the IEEE 16th Int’l Conf. on Mobile Ad Hoc and Sensor Systems Workshops (MASSW). IEEE, 2019. 43−47.

Brockschmidt, J., Shang, J., & Wu, J. (2019, November). On the generality of facial forgery detection. In 2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems Workshops (MASSW) (pp. 43-47). IEEE.

Link : https://ieeexplore.ieee.org/document/9059392

Note : 学习篡改图片的特点可行且高效.此类方法不仅可以判断单帧图像的真伪,还可以利用组合策略检测视频帧,应用范围较广,但是也存在很多局限性,学习到的模型大多数依赖相同的数据分布,在面对未知篡改类型时
很乏力

```
A variety of architectures have been designed or repurposed for the task of facial forgery detection. 

While many of these designs have seen great success, they largely fail to address challenges these models may face in practice. 

A major challenge is posed by generality, wherein models must be prepared to perform in a variety of domains. 

In this paper, we investigate the ability of state-of-the-art facial forgery detection architectures to generalize. 

We first propose two criteria for generality: reliably detecting multiple spoofing techniques and reliably detecting unseen spoofing techniques. 

We then devise experiments which measure how a given architecture performs against these criteria. 

Our analysis focuses on two state-of-the-art facial forgery detection architectures, MesoNet and XceptionNet, both being convolutional neural networks (CNNs). 

Our experiments use samples from six state-of-the-art facial forgery techniques: Deepfakes, Face2Face, FaceSwap, GANnotation, ICface, and X2Face. 

We find MesoNet and XceptionNet show potential to generalize to multiple spoofing techniques but with a slight trade-off in accuracy, and largely fail against unseen techniques. 

We loosely extrapolate these results to similar CNN architectures and emphasize the need for better architectures to meet the challenges of generality.
```

已經為面部偽造檢測任務設計或重新利用了各種架構，儘管其中許多設計取得了巨大成功，但它們在很大程度上未能解決這些模型在實踐中可能面臨的挑戰，這個普遍性帶來了一個重大挑戰，其中模型必須準備好在各種領域中執行。該研究研究了最先進的面部偽造檢測架構的泛化能力，其研究者首先提出兩個通用性標準：可靠地檢測多種欺騙技術和可靠地檢測看不見的欺騙技術，隨後設計實驗來衡量給定架構如何根據這些標準執行。研究者的分析側重於兩種最先進的面部偽造檢測架構，MesoNet 和 XceptionNet，它們都是卷積神經網絡 (CNN)，而實驗使用來自六種最先進的面部偽造技術的樣本：Deepfakes、Face2Face、FaceSwap、GANnotation、ICface 和 X2Face。研究者發現 MesoNet 和 XceptionNet 顯示出泛化到多種欺騙技術的潛力，但在準確性上略有權衡，並且在很大程度上無法對抗看不見的技術。最後將這些結果鬆散地推斷為類似的 CNN 架構，並強調需要更好的架構來應對普遍性的挑戰。

Bibliography

```
@inproceedings{brockschmidt2019generality,
  title={On the generality of facial forgery detection},
  author={Brockschmidt, Joshua and Shang, Jiacheng and Wu, Jie},
  booktitle={2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems Workshops (MASSW)},
  pages={43--47},
  year={2019},
  organization={IEEE}
}
```


55. Sohrawardi SJ, Chintha A, Thai B, Seng S, Hickerson A, Ptucha R, Wright M. Poster: Towards robust open-world detection of Deepfakes. In: Proc. of the ACM SIGSAC Conf. on Computer and Communications Security. 2019. 2613−2615.

Sohrawardi, S. J., Chintha, A., Thai, B., Seng, S., Hickerson, A., Ptucha, R., & Wright, M. (2019, November). Poster: Towards robust open-world detection of deepfakes. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (pp. 2613-2615).

Link : https://www.researchgate.net/publication/337092099_Poster_Towards_Robust_Open-World_Detection_of_Deepfakes

Note : 学习篡改图片的特点可行且高效.此类方法不仅可以判断单帧图像的真伪,还可以利用组合策略检测视频帧,应用范围较广,但是也存在很多局限性,学习到的模型大多数依赖相同的数据分布,在面对未知篡改类型时很乏力

```
There is heightened concern over deliberately inaccurate news. 

Recently, so-called deepfake videos and images that are modified by or generated by artificial intelligence techniques have become more realistic and easier to create. 

These techniques could be used to create fake announcements from public figures or videos of events that did not happen, misleading mass audiences in dangerous ways. 

Although some recent research has examined accurate detection of deepfakes, those methodologies do not generalize well to real-world scenarios and are not available to the public in a usable form. 

In this project, we propose a system that will robustly and efficiently enable users to determine whether or not a video posted online is a deepfake. 

We approach the problem from the journalists' perspective and work towards developing a tool to fit seamlessly into their workflow. 

Results demonstrate accurate detection on both within and mismatched datasets.
```

人們對故意不准確的新聞表示高度關注，最近，由人工智能技術修改或生成的所謂 deepfake 視頻和圖像變得更加逼真，更容易創建。這些技術可用於從公眾人物或未發生事件的視頻中製作虛假公告，以危險的方式誤導大眾觀眾，儘管最近的一些研究已經檢查了深度偽造的準確檢測，但這些方法並不能很好地推廣到現實世界的場景，並且不能以可用的形式向公眾提供。在這個研究工作中，研究者們提出了一個系統，該系統將強大而有效地使用戶能夠確定在線發布的影片是否是 deepfake，該研究從記者的角度處理問題，並努力開發一種工具以無縫融入他們的工作流程，結果表明對內部數據集和不匹配數據集的準確檢測。

Bibliography

```
@inproceedings{sohrawardi2019poster,
  title={Poster: Towards robust open-world detection of deepfakes},
  author={Sohrawardi, Saniat Javid and Chintha, Akash and Thai, Bao and Seng, Sovantharith and Hickerson, Andrea and Ptucha, Raymond and Wright, Matthew},
  booktitle={Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
  pages={2613--2615},
  year={2019}
}
```


56. Agarwal S, Farid H, Gu Y, He M, Nagano K, Li H. Protecting world leaders against deep fakes. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops. 2019. 38−45.

Agarwal, S., Farid, H., Gu, Y., He, M., Nagano, K., & Li, H. (2019, June). Protecting World Leaders Against Deep Fakes. In CVPR workshops (Vol. 1).

Link : https://openaccess.thecvf.com/content_CVPRW_2019/papers/Media%20Forensics/Agarwal_Protecting_World_Leaders_Against_Deep_Fakes_CVPRW_2019_paper.pdf

Note : 5]发现:作为个体,他们有不一致的面部表情和移动,通过追踪面部和头部移动然后抽取特定动作集合的存在和强度,脸部肌肉的移动可以编码成动作单元,再利用皮尔森系数对特征之间的相关性进行扩充,最后在扩充后的特征集合上建立一个新的单分类 SVM 来区分各类造假视频.然而实验结果显示:虽然 AUC 达到 0.9 以上,但是召回普遍不高,实用性较差

```
The creation of sophisticated fake videos has been largely relegated to Hollywood studios or state actors. 

Recent advances in deep learning, however, have made it significantly easier to create sophisticated and compelling fake videos. 

With relatively modest amounts of data and computing power, the average person can, for example, create a video of a world leader confessing to illegal activity leading
to a constitutional crisis, a military leader saying something racially insensitive leading to civil unrest in an area of military activity, or a corporate titan claiming that their profits are weak leading to global stock manipulation. 

These so called deep fakes pose a significant threat to our democracy, national security, and society. 

To contend with this growing threat, we describe a forensic technique that models facial expressions and movements that typify an individual’s speaking pattern.

Although not visually apparent, these correlations are often violated by the nature of how deep-fake videos are created and can, therefore, be used for authentication
```

製作複雜的假視頻在很大程度上已歸咎於好萊塢製片廠或國家演員，然而，深度學習的最新進展使得創建複雜且引人注目的假視頻變得更加容易。例如，普通人可以利用相對適中的數據量和計算能力創建一個世界領導人承認領導非法活動的視頻，造成憲法危機，軍事領導人說出種族不敏感的話導致軍事活動領域的內亂，或者企業巨頭聲稱他們的利潤微弱導致全球股票操縱，綜上所述這些所謂的深度造假對我們的民主、國家安全和社會構成了重大威脅。為了應對這種日益增長的威脅，研究者描述了一種法醫技術，該技術可以對代表個人說話模式的面部表情和動作進行建模。儘管在視覺上並不明顯，但這些相關性經常被深度偽造視頻的創建方式所破壞，因此可以用於身份驗證

Bibliography

```
@inproceedings{agarwal2019protecting,
  title={Protecting World Leaders Against Deep Fakes.},
  author={Agarwal, Shruti and Farid, Hany and Gu, Yuming and He, Mingming and Nagano, Koki and Li, Hao},
  booktitle={CVPR workshops},
  volume={1},
  year={2019}
}
```


57. Amerini I, Galteri L, Caldelli R, Bimbo AD. Deepfake video detection through optical flow based CNN. In: Proc. of the IEEE Int’l Conf. on Computer Vision Workshops. 2019. 1205−1207.

Amerini, I., Galteri, L., Caldelli, R., & Del Bimbo, A. (2019). Deepfake video detection through optical flow based cnn. In Proceedings of the IEEE/CVF international conference on computer vision workshops (pp. 0-0).

Link : https://openaccess.thecvf.com/content_ICCVW_2019/papers/HBU/Amerini_Deepfake_Video_Detection_through_Optical_Flow_Based_CNN_ICCVW_2019_paper.pdf

Note : 探索帧间光流的不同,采用 VGG16 学习光流的差异并进行分类,因为光流是连续帧间的运动差异计算的,自然拍摄和伪造的视频之间的运动差异很大. 

```
Recent advances in visual media technology have led to new tools for processing and, above all, generating multimedia contents. 

In particular, modern AI-based technologies have provided easy-to-use tools to create extremely realistic manipulated videos. 

Such synthetic videos, named Deep Fakes, may constitute a serious threat to attack the reputation of public subjects or to address the general opinion on a certain event. According to this, being able to individuate this kind of fake information becomes fundamental.

In this work, a new forensic technique able to discern between fake and original video sequences is given; unlike other state-of-the-art methods which resorts at single video frames, we propose the adoption of optical flow fields to exploit possible inter-frame dissimilarities. 

Such a clue is then used as feature to be learned by CNN classifiers. Preliminary results obtained on FaceForensics++ dataset highlight very promising performances.
```

視覺媒體技術的最新進展帶來了用於處理，尤其是生成多媒體內容的新工具，特別在於現代基於人工智能的技術提供了易於使用的工具來創建極其逼真的影響，這種名為 Deep Fakes 的合成視頻可能對攻擊公共主體的聲譽或解決對某一事件的普遍看法構成嚴重威脅。 據此，能夠對這種虛假信息進行個體化就變得至關重要。在這項工作中，給出了一種能夠區分假視頻序列和原始視頻序列的新取證技術； 與採用單個視頻幀的其他最先進的方法不同，研究者們建議採用光流場來利用可能的幀間差異。然後將這樣的線索用作 CNN 分類器要學習的特徵。 在 FaceForensics++ 數據集上獲得的初步結果突出了非常有前景的性能。

Bibliography

```
@inproceedings{amerini2019deepfake,
  title={Deepfake video detection through optical flow based cnn},
  author={Amerini, Irene and Galteri, Leonardo and Caldelli, Roberto and Del Bimbo, Alberto},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision workshops},
  pages={0--0},
  year={2019}
}
```


58. Güera D, Delp EJ. Deepfake video detection using recurrent neural networks. In: Proc. of the 15th IEEE Int’l Conf. on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2018. 1−6.

Güera, D., & Delp, E. J. (2018, November). Deepfake video detection using recurrent neural networks. In 2018 15th IEEE international conference on advanced video and signal based surveillance (AVSS) (pp. 1-6). IEEE.

Link : https://ieeexplore.ieee.org/document/8639163

Note : 考虑用循环神经网络处理深度伪造的序列数据,因为多个相机视角,光照条件的不同,不同的视频压缩率使得生成器很难产生实际真实的在不同条件下的脸,这个会导致交换的脸在剩下的场景下看起来不一致.
此外,因为生成器没办法意识到皮肤或者其他场景信息,所以新脸和剩下帧之间的融合性差,不同帧场景间的光源会引起大多数脸部闪烁现象,这个可以被时序网络较好地捕捉到.

```
In recent months a machine learning based free software tool has made it easy to create believable face swaps in videos that leaves few traces of manipulation, in what are known as "deepfake" videos. 

Scenarios where these realistic fake videos are used to create political distress, blackmail someone or fake terrorism events are easily envisioned. 

This paper proposes a temporal-aware pipeline to automatically detect deepfake videos. Our system uses a convolutional neural network (CNN) to extract frame-level features. 

These features are then used to train a recurrent neural network (RNN) that learns to classify if a video has been subject to manipulation or not. 

We evaluate our method against a large set of deepfake videos collected from multiple video websites. 

We show how our system can achieve competitive results in this task while using a simple architecture.
```

最近幾個月，基於機器學習的免費軟件工具可以輕鬆地在視頻中創建可信的人臉交換，幾乎沒有人為操作的痕跡，即所謂的“深度偽造”視頻，這些逼真的假視頻被用來製造政治困境、敲詐某人或假恐怖主義事件的場景很容易想像。該研究提出了一種時間感知管道來自動檢測 deepfake 視頻。 研究者的系統使用卷積神經網絡 (CNN) 來提取幀級特徵，然後使用這些特徵來訓練循環神經網絡 (RNN)，該網絡學習對視頻是否受到操縱進行分類。同時研究者針對從多個視頻網站收集的大量 deepfake 視頻評估該研究的方法，最後展示了該研究的系統如何在使用簡單架構的同時在這項任務中取得有競爭力的結果。

Bibliography

```
@inproceedings{guera2018deepfake,
  title={Deepfake video detection using recurrent neural networks},
  author={G{\"u}era, David and Delp, Edward J},
  booktitle={2018 15th IEEE international conference on advanced video and signal based surveillance (AVSS)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}
```


59. Sabir E, Cheng J, Jaiswal A, AbdAlmageed W, Masi I, Natarajan P. Recurrent convolutional strategies for face manipulation detection in videos. arXiv preprint arXiv:1905.00582, 2019.

Sabir, E., Cheng, J., Jaiswal, A., AbdAlmageed, W., Masi, I., & Natarajan, P. (2019). Recurrent convolutional strategies for face manipulation detection in videos. Interfaces (GUI), 3(1), 80-87.

Link : https://arxiv.org/abs/1905.00582

Note : CNN+Bi-LSTM, 图片的时序信息, FF++/LQ/DF/F2F/FS,采用双向时序网络和人脸对齐结合的方法学习伪造序列,结果显示,基于关键点的人脸对齐与 Bidrectional-recurrent-denset 对视频的篡改检测最佳

```
The spread of misinformation through synthetically generated yet realistic images and videos has become a significant problem, calling for robust manipulation detection methods.

Despite the predominant effort of detecting face manipulation in still images, less attention has been paid to the identification of tampered faces in videos by taking advantage of the temporal information present in the stream.

Recurrent convolutional models are a class of deep learning models which have proven effective at exploiting the temporal information from image streams across domains. 

We thereby distill the best strategy for combining variations in these models along with domain specific face preprocessing techniques through extensive experimentation to obtain state-of-the-art performance on publicly available video-based facial manipulation benchmarks. 

Specifically, we attempt to detect Deepfake, Face2Face and FaceSwap tampered faces in video streams. 

Evaluation is performed on the recently introduced FaceForensics++ dataset, improving the previous state-of-the-art by up to 4.55% in accuracy.
```

通過合成生成但逼真的圖像和視頻傳播錯誤信息已成為一個重大問題，需要強大的操縱檢測方法，儘管在靜止圖像中檢測人臉操作的主要努力，通過利用流中存在的時間信息來識別視頻中被篡改的人臉卻很少受到關注。循環卷積模型是一類深度學習模型，已被證明可以有效地利用跨域圖像流中的時間信息。因此，研究者通過廣泛的實驗提取了將這些模型的變化與特定領域的面部預處理技術相結合的最佳策略，以在公開的基於視頻的面部操作基准上獲得最先進的性能。具體來說，該研究嘗試檢測視頻流中的 Deepfake、Face2Face 和 FaceSwap 篡改人臉，其評估是在最近推出的 FaceForensics++ 數據集上進行的，將之前最先進的準確率提高了 4.55%。

Bibliography

```
@article{sabir2019recurrent,
  title={Recurrent convolutional strategies for face manipulation detection in videos},
  author={Sabir, Ekraam and Cheng, Jiaxin and Jaiswal, Ayush and AbdAlmageed, Wael and Masi, Iacopo and Natarajan, Prem},
  journal={Interfaces (GUI)},
  volume={3},
  number={1},
  pages={80--87},
  year={2019},
  publisher={CVPR Long Beach, CA, USA}
}
```


60. Todisco M, Delgado H, Evans NWD. A new feature for automatic speaker verification anti-spoofing: Constant Q cepstral coefficients. In: Proc. of the Odyssey. 2016. 283−290.

Todisco, M., Delgado, H., & Evans, N. W. (2016, June). A New Feature for Automatic Speaker Verification Anti-Spoofing: Constant Q Cepstral Coefficients. In Odyssey (Vol. 2016, pp. 283-290).

Link : https://www.eurecom.fr/publication/4855

Note : Todisco 等人提出的常量 Q 倒谱系数(constant-Q cepstral coefficients,简称 CQCC)

```
Efforts to develop new countermeasures in order to protect automatic speaker verification from spoofing have intensified over recent years. 

The ASVspoof 2015 initiative showed that there is great potential to detect spoofing attacks, but also that the detection of previously unforeseen spoofing attacks remains challenging. 

This paper argues that there is more to be gained from the study of features rather than classifiers and introduces a new feature for spoofing detection based on the constant Q transform, a perceptually-inspired time-frequency analysis tool popular in the study of music. 

Experimental results obtained using the standard ASVspoof 2015 database show that, when coupled with a standard Gaussian mixture model-based classifier, the proposed constant Q cepstral coefficients (CQCCs) outperform all previously reported results by a significant margin. 

In particular, those for a subset of unknown spoofing attacks (for which no matched training data was used) is 0.46%, a relative improvement of 72% over the best, previously reported results. 
```

近年來，開發新的對策以保護自動說話人驗證免受欺騙的努力已經加強，其 ASVspoof 2015 計劃表明，檢測欺騙攻擊的潛力很大，但檢測以前無法預見的欺騙攻擊仍然具有挑戰性，該研究認為，從研究特徵而不是分類器中可以獲得更多收益，並介紹了一種基於恆定 Q 變換的欺騙檢測新特徵，這是一種在音樂研究中流行的受感知啟發的時頻分析工具。使用標準 ASVspoof 2015 數據庫獲得的實驗結果表明，當與基於標準高斯混合模型的分類器結合使用時，所提出的恆定 Q 倒譜係數 (CQCC) 的性能明顯優於所有先前報告的結果，特別是，對於未知欺騙攻擊子集（未使用匹配的訓練數據）的攻擊為 0.46%，相對於先前報告的最佳結果提高了 72%。

Bibliography

```
@inproceedings{todisco2016new,
  title={A New Feature for Automatic Speaker Verification Anti-Spoofing: Constant Q Cepstral Coefficients.},
  author={Todisco, Massimiliano and Delgado, H{\'e}ctor and Evans, Nicholas WD},
  booktitle={Odyssey},
  volume={2016},
  pages={283--290},
  year={2016}
}
```


61. Rössler A, Cozzolino D, Verdoliva L, Christian R, Justus T, Matthias N. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179, 2018.

Rössler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., & Nießner, M. (2018). Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179.

Link : https://arxiv.org/abs/1803.09179

Note : Open source dataset of the Deepfake - 深度伪造开源数据集;FaceForensics(FF) Face2Face FaceForensics++的前身,只有一种篡改类型

```
With recent advances in computer vision and graphics, it is now possible to generate videos with extremely realistic synthetic faces, even in real time. 

Countless applications are possible, some of which raise a legitimate alarm, calling for reliable detectors of fake videos. 

In fact, distinguishing between original and manipulated video can be a challenge for humans and computers alike, especially when the videos are compressed or have low resolution, as it often happens on social networks. 

Research on the detection of face manipulations has been seriously hampered by the lack of adequate datasets. 

To this end, we introduce a novel face manipulation dataset of about half a million edited images (from over 1000 videos). 

The manipulations have been generated with a state-of-the-art face editing approach. 

It exceeds all existing video manipulation datasets by at least an order of magnitude. 

Using our new dataset, we introduce benchmarks for classical image forensic tasks, including classification and segmentation, considering videos compressed at various quality levels. 

In addition, we introduce a benchmark evaluation for creating indistinguishable forgeries with known ground truth; for instance with generative refinement models.
```

隨著計算機視覺和圖形學的最新進展，現在甚至可以實時生成具有極其逼真的合成人臉的視頻，無數的應用是可能的，其中一些會發出合理的警報，需要可靠的假視頻檢測器。事實上，區分原始視頻和經過處理的視頻對於人類和計算機來說都是一個挑戰，尤其是當視頻被壓縮或分辨率低時，因為它經常發生在社交網絡上。同時由於缺乏足夠的數據集，人臉操作檢測的研究受到了嚴重阻礙。為此，研究者引入了一個新穎的人臉處理數據集，其中包含大約 50 萬張編輯圖像（來自 1000 多個視頻），這些操作是使用最先進的面部編輯方法生成的。它比所有現有的視頻操作數據集至少高出一個數量級。使用研究者的新數據集，我們介紹了經典圖像取證任務的基準，包括分類和分割，考慮到以各種質量級別壓縮的視頻。此外，該研究引入了一個基準評估，用於創建具有已知基本事實的難以區分的偽造品；例如生成細化模型。

Bibliography

```
@article{rossler2018faceforensics,
  title={Faceforensics: A large-scale video dataset for forgery detection in human faces},
  author={R{\"o}ssler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Nie{\ss}ner, Matthias},
  journal={arXiv preprint arXiv:1803.09179},
  year={2018}
}
```


62. Rossler A, Cozzolino D, Verdoliva L, Riess C, Thies J, Niessner M. Faceforensics++: Learning to detect manipulated facial images. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 1−11.

Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., & Nießner, M. (2019). Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1-11).

Link : https://arxiv.org/abs/1901.08971

Note : FaceForensics++(FF++),Deepfakes,FaceSwap,Face2face,Neuraltexture; 每一类篡改视频均被 C0,C23,C40 这 3 种参数压缩

```
The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. 

At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. 

This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. 

To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. 

In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. 

The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. 

This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. 

Based on this data, we performed a thorough analysis of data-driven forgery detectors. 

We show that the use of additional domainspecific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.
```

合成圖像生成和處理的快速進展現在已經到了引發對社會影響的重大擔憂的地步。充其量，這會導致對數字內容失去信任，但可能會通過傳播虛假信息或虛假新聞而造成進一步的傷害，該研究研究了最先進的圖像處理的真實性，以及自動或人工檢測它們的難度，而為了標準化檢測方法的評估，研究者提出了面部操作檢測的自動化基準。特別是，該基準基於 DeepFakes、Face2Face、FaceSwap 和 NeuralTextures 作為隨機壓縮級別和大小的面部操作的突出代表。該基準是公開的，包含一個隱藏的測試集以及一個包含超過 180 萬張操縱圖像的數據庫，而且該數據集相對於可比較的、公開可用的偽造數據集大一個數量級。基於這些數據，研究者們對數據驅動的偽造檢測器進行了徹底的分析。其研究表明，即使在存在強壓縮的情況下，使用額外的特定領域知識也可以將偽造檢測提高到前所未有的準確性，並且明顯優於人類觀察者。

Bibliography

```
@inproceedings{rossler2019faceforensics++,
  title={Faceforensics++: Learning to detect manipulated facial images},
  author={Rossler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1--11},
  year={2019}
}
```


63. Korshunov P, Marcel S. Deepfakes: A new threat to face recognition? Assessment and detection. arXiv preprint arXiv:1812.08685, 2018.

Korshunov, P., & Marcel, S. (2018). Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685.

Link : https://arxiv.org/abs/1812.08685

Note : DeepfakeTIMIT,faceswapGAN,GAN 版本 Deepfakes 换脸.有高清和低清两个版本

```
It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). 

Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. 

To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. 

We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. 

To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. 

We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. 

By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. 

The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97% equal error rate on high quality Deepfakes. 

Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so.
```

通過使用預先訓練的生成對抗網絡 (GAN)，將視頻中的一個人的臉自動替換為另一個人的臉變得越來越容易。最近的公共醜聞，例如，名人的面孔被交換到色情視頻上，需要自動檢測這些 Deepfake 視頻的方法，為了幫助開發此類方法，在本文中，我們展示了第一組從 VidTIMIT 數據庫的視頻中生成的公開可用的 Deepfake 視頻。研究者使用基於 GAN 的開源軟件來創建 Deepfakes，該研究強調訓練和混合參數可以顯著影響結果視頻的質量。為了證明這種影響，研究者使用不同調整的參數集生成了具有低和高視覺質量的視頻（每個 320 個視頻），其研究展示了基於 VGG 和 Facenet 神經網絡的最先進的人臉識別系統容易受到 Deepfake 視頻的攻擊，錯誤接受率分別為 85.62% 和 95.00%，這意味著檢測 Deepfake 視頻的方法是必要的。通過考慮幾種基線方法，我們發現基於口型同步不一致檢測的視聽方法無法區分 Deepfake 視頻。性能最佳的方法，基於視覺質量指標，常用於演示攻擊檢測領域，在高質量 Deepfakes 上的錯誤率為 8.97%，最後的實驗表明，GAN 生成的 Deepfake 視頻對人臉識別系統和現有檢測方法都具有挑戰性，而人臉交換技術的進一步發展將使其變得更加困難。

Bibliography

```
@article{korshunov2018deepfakes,
  title={Deepfakes: a new threat to face recognition? assessment and detection},
  author={Korshunov, Pavel and Marcel, S{\'e}bastien},
  journal={arXiv preprint arXiv:1812.08685},
  year={2018}
}
```


64. VidTIMIT. 2019. http://conradsanderson.id.au/vidtimit/

Link : http://conradsanderson.id.au/vidtimit/

Note : 深度伪造开源数据集 VidTIMIT 获取源


Bibliography

```
@online{list1064,
    title     = "VidTIMIT.",
    url       = "http://conradsanderson.id.au/vidtimit/"
}
```


65. Afchar D, Nozick V, Yamagishi J, Echizen I. Mesonet: A compact facial video forgery detection network. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). IEEE, 2018. 1−7.

Afchar, D., Nozick, V., Yamagishi, J., & Echizen, I. (2018, December). Mesonet: a compact facial video forgery detection network. In 2018 IEEE international workshop on information forensics and security (WIFS) (pp. 1-7). IEEE.

Link : https://arxiv.org/abs/1809.00888

Note : 深度伪造开源数据集, Mesonet data

```
This paper presents a method to automatically and efficiently detect face tampering in videos, and particularly focuses on two recent techniques used to generate hyper-realistic forged videos: Deepfake and Face2Face. 

Traditional image forensics techniques are usually not well suited to videos due to the compression that strongly degrades the data. 

Thus, this paper follows a deep learning approach and presents two networks, both with a low number of layers to focus on the mesoscopic properties of images. 

We evaluate those fast networks on both an existing dataset and a dataset we have constituted from online videos. 

The tests demonstrate a very successful detection rate with more than 98% for Deepfake and 95% for Face2Face.
```

該研究提出了一種自動有效地檢測視頻中的人臉篡改的方法，特別關注最近用於生成超逼真偽造視頻的兩種技術：Deepfake 和 Face2Face，由於壓縮會嚴重降低數據質量，傳統的圖像取證技術通常不太適合視頻。因此，該研究采用深度學習方法並提出了兩個網絡，兩者都具有較少的層數，以專注於圖像的細觀特性。該研究在現有數據集和研究者從在線視頻構成的數據集上評估這些快速網絡。最後測試證明了非常成功的檢測率，Deepfake 的檢測率超過 98%，Face2Face 的檢測率超過 95%。

Bibliography

```
@inproceedings{afchar2018mesonet,
  title={Mesonet: a compact facial video forgery detection network},
  author={Afchar, Darius and Nozick, Vincent and Yamagishi, Junichi and Echizen, Isao},
  booktitle={2018 IEEE international workshop on information forensics and security (WIFS)},
  pages={1--7},
  year={2018},
  organization={IEEE}
}
```


66. Li Y, Yang X, Sun P, Qi H, Lyu S. Celeb-DF: A new dataset for Deepfake forensics. arXiv preprint arXiv:1909.12962, 2019.

Li, Y., Yang, X., Sun, P., Qi, H., & Lyu, S. (2019). Celeb-df: A new dataset for deepfake forensics.

Link : https://arxiv.org/abs/1909.12962

Note : 深度伪造开源数据集,Celeb-DF, Deepfakes 针对过去伪造视频的质量差、不稳定等缺点进行改进,效果更好

```
AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. 

The need to develop and evaluate DeepFake detection algorithms calls for large-scale datasets. 

However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. 

We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. 

We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF.
```

AI 合成的換臉視頻，俗稱 DeepFakes，是一個威脅在線信息可信度的新興問題，其開發和評估 DeepFake 檢測算法的需求需要大規模數據集。然而，當前的 DeepFake 數據集視覺質量較低，與互聯網上流傳的 DeepFake 視頻不一樣。該研究提出了一個新的具有挑戰性的大規模 DeepFake 視頻數據集 Celeb-DF，其中包含使用改進的合成過程生成的 5,639 個名人的高質量 DeepFake 視頻。其研究者對 DeepFake 檢測方法和數據集進行了全面評估，以證明 Celeb-DF 帶來的挑戰不斷升級。

Bibliography

```
@article{li2019celeb,
  title={Celeb-df: A new dataset for deepfake forensics},
  author={Li, Yuezun and Yang, Xin and Sun, Pu and Qi, Honggang and Lyu, Siwei},
  year={2019}
}
```


67. DeepfakeDetection. 2019. https://github.com/ondyari/FaceForensics

Link : https://github.com/ondyari/FaceForensics

Note : 深度伪造开源数据集,DeepfakeDetection(DFD),363 个不同场景下的原视频,然后进行换脸.篡改视频均 C0,C23,C40 这 3 种参数压缩


Bibliography

```
@online{list1067,
    title     = "DeepfakeDetection",
    url       = "https://github.com/ondyari/FaceForensics"
}
```


68. Dolhansky B, Howes R, Pflaum B, Baram N, Ferrer C. The Deepfake detection challenge (DFDC) preview dataset. arXiv preprint arXiv:1910.08854, 2019.

Dolhansky, B., Howes, R., Pflaum, B., Baram, N., & Ferrer, C. C. (2019). The deepfake detection challenge (dfdc) preview dataset. arXiv preprint arXiv:1910.08854.

Link : https://arxiv.org/abs/1910.08854

Note : 深度伪造开源数据集,DFDC preview dataset Unknown Deepfakes 竞赛的预赛数据

```

In this paper, we introduce a preview of the Deepfakes Detection Challenge (DFDC) dataset consisting of 5K videos featuring two facial modification algorithms. 

A data collection campaign has been carried out where participating actors have entered into an agreement to the use and manipulation of their likenesses in our creation of the dataset. 

Diversity in several axes (gender, skin-tone, age, etc.) has been considered and actors recorded videos with arbitrary backgrounds thus bringing visual variability. 

Finally, a set of specific metrics to evaluate the performance have been defined and two existing models for detecting deepfakes have been tested to provide a reference performance baseline. 

The DFDC dataset preview can be downloaded at: this http URL(http://deepfakedetectionchallenge.ai/)
```

在該研究中的研究者們介紹了 Deepfakes Detection Challenge (DFDC) 數據集的預覽，該數據集由 5K 視頻組成，具有兩種面部修改演算法。已經開展了一項數據收集活動，參與的參與者已就在我們創建數據集時使用和操縱他們的肖像達成協議，同時考慮了多個軸（性別、膚色、年齡等）的多樣性，並且演員錄製了具有任意背景的視頻，從而帶來了視覺上的可變性。最後，定義了一組評估性能的特定指標，並測試了兩個現有的檢測深度偽造模型，以提供參考性能基線。

Bibliography

```
@article{dolhansky2019deepfake,
  title={The deepfake detection challenge (dfdc) preview dataset},
  author={Dolhansky, Brian and Howes, Russ and Pflaum, Ben and Baram, Nicole and Ferrer, Cristian Canton},
  journal={arXiv preprint arXiv:1910.08854},
  year={2019}
}
```


69. DFDC. 2020. https://www.kaggle.com/c/deepfake-detection-challenge/data

Link : https://www.kaggle.com/c/deepfake-detection-challenge/data

Note : DFDC Unknown Deepfakes 竞赛的正式全部数据

Bibliography

```
@online{list1069,
    title     = "DFDC.",
    url       = "https://www.kaggle.com/c/deepfake-detection-challenge/data"
}
```


70. Jiang L, Li R, Wu W, Qian C, Loy C. DeeperForensics-1.0: A large-scale dataset for real-world face forgery detection. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020. 2886−2895.

Jiang, L., Li, R., Wu, W., Qian, C., & Loy, C. C. (2020). Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 2889-2898).

Link : https://openaccess.thecvf.com/content_CVPR_2020/html/Jiang_DeeperForensics-1.0_A_Large-Scale_Dataset_for_Real-World_Face_Forgery_Detection_CVPR_2020_paper.html

https://github.com/EndlessSora/DeeperForensics-1.0

Note : 深度伪造开源数据集,DeeperForensics1.0,{DeepFake,Variational,Auto-Encoder}改进的生成

```
We present our on-going effort of constructing a large- scale benchmark for face forgery detection. 

The first version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far, with 60, 000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. 

Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. 

All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. 

The quality of generated videos outperforms those in existing datasets, validated by user studies. 

The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. 

We further contribute a comprehensive study that evaluates five representative detection baselines and make a thorough analysis of different settings.
```

研究者展示了其研究為構建面部偽造檢測的大規模基準所做的持續努力，該基準的第一個版本 DeeperForensics-1.0 代表了迄今為止最大的人臉偽造檢測數據集，由 60,000 個視頻組成，總共 1760 萬幀，是現有同類數據集的 10 倍，應用廣泛的現實世界擾動以獲得更大規模和更高多樣性的更具挑戰性的基準，DeeperForensics-1.0 中的所有源視頻都經過精心收集，假視頻由新提出的端到端人臉交換框架生成。經用戶研究驗證，生成視頻的質量優於現有數據集中的質量，該基準具有一個隱藏的測試集，其中包含在人工評估中獲得高欺騙性分數的操縱視頻，研究者進一步貢獻了一項綜合研究，評估五個具有代表性的檢測基線，並對不同的設置進行全面分析。

Bibliography

```
@inproceedings{jiang2020deeperforensics,
  title={Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection},
  author={Jiang, Liming and Li, Ren and Wu, Wayne and Qian, Chen and Loy, Chen Change},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2889--2898},
  year={2020}
}
```


71. ASVspoof 2015 database. 2020. https://datashare.is.ed.ac.uk/handle/10283/853

Link : https://datashare.is.ed.ac.uk/handle/10283/853

Note : 深度伪造开源数据集, synthetic and converted speech, 106 speakers

Bibliography

```
@online{list1071,
    title     = "ASVspoof 2015 database.",
    url       = "https://datashare.is.ed.ac.uk/handle/10283/853"
}
```


72. ASVspoof 2019 database. 2020. https://datashare.is.ed.ac.uk/handle/10283/3336

Link : https://datashare.is.ed.ac.uk/handle/10283/3336

Note : 深度伪造开源数据集, synthetic and converted speech, 107 speakers

Bibliography

```
@online{list1072,
    title     = "ASVspoof 2015 database.",
    url       = "https://datashare.is.ed.ac.uk/handle/10283/853"
}
```


73. Abu-El-Haija S, Kothari N, Lee J, Natsev P, Toderici G, Varadarajan B, Vijayanarasimhan S. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.

Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., & Vijayanarasimhan, S. (2016). Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675.

Link : https://arxiv.org/abs/1609.08675

Note : Youtube8M,选取该数据集中标签为人脸、新闻播报员、新闻联播的视频以及 YouTube 上有类似标签的视频共 1 004 个,所有选取的视频分辨率大于 480p.

除此之外,作者用人脸检测器抽取视频中的人脸序列,确保所选视频连续 300 帧中含有人脸,并手动过滤掉人脸遮挡过多的视频以确保视频质量.

最后,采用 Face2Face 的换表情的方法构造 1 004 个假视频.此数据集视频规模大、源视频人脸质量高,但是篡改痕迹明显,篡改方式单一. 

```
Many recent advancements in Computer Vision are attributed to large datasets. 

Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. 

It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets.

In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. 

To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. 

While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. 

We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. 

Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. 

Finally, we compressed the frame features and make both the features and video-level labels available for download.

We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. 

Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. 

We plan to release code for training a TensorFlow model and for computing metrics.
```

計算機視覺的許多最新進展都歸功於大型數據集，用於機器學習的開源軟件包和廉價的商品硬件降低了大規模探索新方法的門檻。可以在幾天內訓練超過數百萬個示例的模型。儘管存在用於圖像理解的大規模數據集，例如 ImageNet，但沒有可比大小的視頻分類數據集。該研究介紹了最大的多標籤視頻分類數據集 YouTube-8M，由約 800 萬個視頻（50 萬小時的視頻）組成，並用 4800 個視覺實體的詞彙表進行註釋，為了獲取視頻及其標籤，研究者使用了 YouTube 視頻註釋系統，該系統使用其主要主題標記視頻。雖然標籤是機器生成的，但它們具有高精度，並且源自各種基於人類的信號，包括元數據和查詢點擊信號，其使用自動和手動策展策略過濾了視頻標籤（知識圖實體），包括詢問人工評估者標籤是否在視覺上可識別。然後，研究者以每秒一幀的速度解碼每個視頻，並使用在 ImageNet 上預訓練的 Deep CNN 在分類層之前立即提取隱藏表示。最後，研究者壓縮了幀特徵並使特徵和視頻級標籤都可供下載。而研究在數據集上訓練了各種（適度的）分類模型，使用流行的評估指標對其進行評估，並將它們作為基線報告。儘管數據集很大，但該研究的一些模型在使用 TensorFlow 的單台機器上訓練不到一天就可以收斂。

Bibliography

```
@article{abu2016youtube,
  title={Youtube-8m: A large-scale video classification benchmark},
  author={Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  journal={arXiv preprint arXiv:1609.08675},
  year={2016}
}
```


74. Amerini I, Ballan L, Caldelli R, Bimbo AD, Serra G. A sift-based forensic method for copy-move attack detection and transformation recovery. IEEE Trans. on Information Forensics and Security, 2011,6(3):1099−1110.

Amerini, I., Ballan, L., Caldelli, R., Del Bimbo, A., & Serra, G. (2011). A sift-based forensic method for copy–move attack detection and transformation recovery. IEEE transactions on information forensics and security, 6(3), 1099-1110.

Link : https://www.researchgate.net/publication/224225329_A_SIFT-Based_Forensic_Method_for_Copy-Move_Attack_Detection_and_Transformation_Recovery

Note : 传统的图像取证初始主要是基于传统的信号处理方法,大多数依赖于特定篡改的证据,利用图像的频域特征和统计特征进行区分,如局部噪音分析、图像质量评估、设备指纹、光照等,該篇解决复制-移动

```
One of the principal problems in image forensics is determining if a particular image is authentic or not. 

This can be a crucial task when images are used as basic evidence to influence judgment like, for example, in a court of law. 

To carry out such forensic analysis, various technological instruments have been developed in the literature. 

In this paper, the problem of detecting if an image has been forged is investigated; in particular, attention has been paid to the case in which an area of an image is copied and then pasted onto another zone to create a duplication or to cancel something that was awkward. 

Generally, to adapt the image patch to the new context a geometric transformation is needed. 

To detect such modifications, a novel methodology based on scale invariant features transform (SIFT) is proposed. Such a method allows us to both understand if a copy-move attack has occurred and, furthermore, to recover the geometric transformation used to perform cloning. 

Extensive experimental results are presented to confirm that the technique is able to precisely individuate the altered area and, in addition, to estimate the geometric transformation parameters with high reliability. 

The method also deals with multiple cloning.
```

圖像取證的主要問題之一是確定特定圖像是否真實。當圖像被用作影響判斷的基本證據時，例如在法庭上，這可能是一項至關重要的任務。為了進行這種取證分析，文獻中已經開發了各種技術儀器。此研究探究圖像是否被偽造的檢測問題；特別是，已經註意到圖像的一個區域被複製然後粘貼到另一個區域以創建複製或取消一些尷尬的東西的情況。通常，為了使圖像補丁適應新的上下文，需要進行幾何變換。為了檢測這種修改，提出了一種基於尺度不變特徵變換（SIFT）的新方法。這種方法使研究者既可以了解是否發生了複製移動攻擊，還可以恢復用於執行克隆的幾何變換。廣泛的實驗結果證實了該技術能夠精確地個體化改變區域，此外，還能夠以高可靠性估計幾何變換參數。該方法還處理多重克隆。

Bibliography

```
@article{amerini2011sift,
  title={A sift-based forensic method for copy--move attack detection and transformation recovery},
  author={Amerini, Irene and Ballan, Lamberto and Caldelli, Roberto and Del Bimbo, Alberto and Serra, Giuseppe},
  journal={IEEE transactions on information forensics and security},
  volume={6},
  number={3},
  pages={1099--1110},
  year={2011},
  publisher={IEEE}
}
```


75. De Carvalho TJ, Riess C, Angelopoulou E, Pedrini H, Rocha A. Exposing digital image forgeries by illumination color classification. IEEE Trans. on Information Forensics and Security, 2013,8(7):1182−1194.

De Carvalho, T. J., Riess, C., Angelopoulou, E., Pedrini, H., & de Rezende Rocha, A. (2013). Exposing digital image forgeries by illumination color classification. IEEE Transactions on Information Forensics and Security, 8(7), 1182-1194.

Link : https://ieeexplore.ieee.org/document/6522874

Note : 传统的图像取证初始主要是基于传统的信号处理方法,大多数依赖于特定篡改的证据,利用图像的频域特征和统计特征进行区分,如局部噪音分析、图像质量评估、设备指纹、光照等,該篇解决拼接

```
For decades, photographs have been used to document space-time events and they have often served as evidence in courts. 

Although photographers are able to create composites of analog pictures, this process is very time consuming and requires expert knowledge. 

Today, however, powerful digital image editing software makes image modifications straightforward. This undermines our trust in photographs and, in particular, questions pictures as evidence for real-world events. 

In this paper, we analyze one of the most common forms of photographic manipulation, known as image composition or splicing. 

We propose a forgery detection method that exploits subtle inconsistencies in the color of the illumination of images. Our approach is machine-learning-based and requires minimal user interaction. 

The technique is applicable to images containing two or more people and requires no expert interaction for the tampering decision. 

To achieve this, we incorporate information from physics- and statistical-based illuminant estimators on image regions of similar material. 

From these illuminant estimates, we extract texture- and edge-based features which are then provided to a machine-learning approach for automatic decision-making. 

The classification performance using an SVM meta-fusion classifier is promising. It yields detection rates of 86% on a new benchmark dataset consisting of 200 images, and 83% on 50 images that were collected from the Internet.
```

幾十年來，照片一直被用來記錄時空事件，並且經常在法庭上作為證據。雖然攝影師能夠創建模擬圖片的合成，但這個過程非常耗時並且需要專業知識。然而，今天，強大的數字圖像編輯軟件使圖像修改變得簡單。這破壞了大家對照片的信任，尤其是質疑將照片作為真實世界事件證據的問題。該研究分析了一種最常見的攝影操作形式，即圖像合成或拼接。研究者們提出了一種偽造檢測方法，該方法利用圖像照明顏色的細微不一致，其方法是基於機器學習的，並且需要最少的用戶交互。該技術適用於包含兩個或更多人的圖像，並且不需要專家交互來做出篡改決定，為了實現這一點，研究者將來自基於物理和統計的光源估計器的信息整合到類似材料的圖像區域上。從這些光源估計中，該研究者提取基於紋理和邊緣的特徵，然後將其提供給機器學習方法以進行自動決策。使用 SVM 元融合分類器的分類性能是有希望的。它在由 200 張圖像組成的新基準數據集上產生 86% 的檢測率，在從 Internet 收集的 50 張圖像上產生 83% 的檢測率。

Bibliography

```
@article{de2013exposing,
  title={Exposing digital image forgeries by illumination color classification},
  author={De Carvalho, Tiago Jos{\'e} and Riess, Christian and Angelopoulou, Elli and Pedrini, Helio and de Rezende Rocha, Anderson},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={8},
  number={7},
  pages={1182--1194},
  year={2013},
  publisher={IEEE}
}
```


76. Lukáš J, Fridrich J, Goljan M. Detecting digital image forgeries using sensor pattern noise. In: Proc. of the Security, Steganography, and Watermarking of Multimedia Contents VIII, Vol.6072. Int’l Society for Optics and Photonics, 2006.

Lukáš, J., Fridrich, J., & Goljan, M. (2006, February). Detecting digital image forgeries using sensor pattern noise. In Security, Steganography, and Watermarking of Multimedia Contents VIII (Vol. 6072, pp. 362-372). SPIE.

Link : https://www.spiedigitallibrary.org/conference-proceedings-of-spie/6072/60720Y/Detecting-digital-image-forgeries-using-sensor-pattern-noise/10.1117/12.640109.short?SSO=1

Note : Lukas 等人提出了数字图像的相机设备指纹光响应不均匀性(PRNU)

```
We present a new approach to detection of forgeries in digital images under the assumption that either the camera that took the image is available or other images taken by that camera are available. 

Our method is based on detecting the presence of the camera pattern noise, which is a unique stochastic characteristic of imaging sensors, in individual regions in the image. 

The forged region is determined as the one that lacks the pattern noise. 

The presence of the noise is established using correlation as in detection of spread spectrum watermarks. 

We proposed two approaches. 

In the first one, the user selects an area for integrity verification. 

The second method attempts to automatically determine the forged area without assuming any a priori knowledge. 

The methods are tested both on examples of real forgeries and on non-forged images. 

We also investigate how further image processing applied to the forged image, such as lossy compression or filtering, influences our ability to verify image integrity.
```

在假設拍攝圖像的相機可用或該相機拍攝的其他圖像可用的假設下，研究者們提出了一種檢測數字圖像中的偽造品的新方法，其方法基於檢測圖像中各個區域中相機模式噪聲的存在，這是成像傳感器的獨特隨機特性。而偽造區域被確定為缺少圖案噪聲的區域。噪聲的存在是使用相關性確定的，如在擴頻水印的檢測中。研究者們提出了兩種方法在第一個中，用戶選擇一個區域進行完整性驗證。第二種方法試圖在不假設任何先驗知識的情況下自動確定偽造區域。這些方法在真實偽造的例子和非偽造圖像上都進行了測試，其研究者還研究了應用於偽造圖像的進一步圖像處理，例如有損壓縮或過濾，如何影響驗證圖像完整性的能力。

Bibliography

```
@inproceedings{lukavs2006detecting,
  title={Detecting digital image forgeries using sensor pattern noise},
  author={Luk{\'a}{\v{s}}, Jan and Fridrich, Jessica and Goljan, Miroslav},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VIII},
  volume={6072},
  pages={362--372},
  year={2006},
  organization={SPIE}
}
```


77. Chierchia G, Parrilli S, Poggi G, Verdoliva L, Sansone C. PRNU-based detection of small-size image forgeries. In: Proc. of the 17th Int’l Conf. on Digital Signal Processing (DSP). IEEE, 2011. 1−6. 

Chierchia, G., Parrilli, S., Poggi, G., Verdoliva, L., & Sansone, C. (2011, July). PRNU-based detection of small-size image forgeries. In 2011 17th International Conference on Digital Signal Processing (DSP) (pp. 1-6). IEEE.

Link : https://ieeexplore.ieee.org/document/6004957

Note : Chierchia 等人进一步利用光响应不均匀性检测小的篡改图像

```
The Photo-Response Non-Uniformity (PRNU) has been recently introduced as a powerful tool to detect image forgeries. 

In spite of its effectiveness in many scenarios, the proposed method fails to detect small manipulations. 

In this work we propose a modified version of the detection algorithm described in, based on a preliminary segmentation of the image, which guarantees a better detection performance for small size additive forgeries.
```

最近引入了光響應非均勻性 (PRNU) 作為檢測圖像偽造的強大工具，儘管它在許多情況下都有效，但所提出的方法無法檢測到小的操作。在這項工作中，研究者基於圖像的初步分割提出了中描述的檢測算法的修改版本，這保證了對小尺寸加法偽造品的更好檢測性能。

Bibliography

```
@inproceedings{chierchia2011prnu,
  title={PRNU-based detection of small-size image forgeries},
  author={Chierchia, Giovanni and Parrilli, Sara and Poggi, Giovanni and Verdoliva, Luisa and Sansone, Carlo},
  booktitle={2011 17th International Conference on Digital Signal Processing (DSP)},
  pages={1--6},
  year={2011},
  organization={IEEE}
}
```


78. Fridrich J, Kodovsky J. Rich models for steganalysis of digital images. IEEE Trans. on Information Forensics and Security, 2012, 7(3):868−882.

Fridrich, J., & Kodovsky, J. (2012). Rich models for steganalysis of digital images. IEEE Transactions on information Forensics and Security, 7(3), 868-882.

Link : https://ieeexplore.ieee.org/document/6197267

Note : Jessica 等人通过组装噪声分量模型提出了数字图像的隐写特征,随后,噪声特征被广泛运用在图像取证领域

```
We describe a novel general strategy for building steganography detectors for digital images. 

The process starts with assembling a rich model of the noise component as a union of many diverse submodels formed by joint distributions of neighboring samples from quantized image noise residuals obtained using linear and nonlinear high-pass filters. 

In contrast to previous approaches, we make the model assembly a part of the training process driven by samples drawn from the corresponding cover- and stego-sources. 

Ensemble classifiers are used to assemble the model as well as the final steganalyzer due to their low computational complexity and ability to efficiently work with high-dimensional feature spaces and large training sets. 

We demonstrate the proposed framework on three steganographic algorithms designed to hide messages in images represented in the spatial domain: HUGO, edge-adaptive algorithm by Luo , and optimally coded ternary ±1 embedding. 

For each algorithm, we apply a simple submodel-selection technique to increase the detection accuracy per model dimensionality and show how the detection saturates with increasing complexity of the rich model. 

By observing the differences between how different submodels engage in detection, an interesting interplay between the embedding and detection is revealed. 

Steganalysis built around rich image models combined with ensemble classifiers is a promising direction towards automatizing steganalysis for a wide spectrum of steganographic schemes.

```

該研究描述了一種新的通用策略，用於為數字圖像構建隱寫檢測器，該過程首先組裝一個豐富的噪聲分量模型，作為由使用線性和非線性高通濾波器獲得的量化圖像噪聲殘差的相鄰樣本的聯合分佈形成的許多不同子模型的聯合。與以前的方法相比，研究者使模型組裝成為訓練過程的一部分，該過程由從相應的覆蓋和隱秘源中抽取的樣本驅動。集成分類器用於組裝模型以及最終的隱寫分析器，因為它們的計算複雜度低並且能夠有效地處理高維特徵空間和大型訓練集。該研究在三種隱寫算法上演示了所提出的框架，這些演算法旨在隱藏空間域中表示的圖像中的消息：HUGO、Luo 的邊緣自適應算法和優化編碼的三元 ±1 嵌入。對於每種算法，研究者應用一種簡單的子模型選擇技術來提高每個模型維度的檢測精度，並展示檢測如何隨著豐富模型的複雜性增加而飽和。通過觀察不同子模型如何參與檢測之間的差異，揭示了嵌入和檢測之間有趣的相互作用。圍繞豐富的圖像模型構建的隱寫分析與集成分類器相結合，是為廣泛的隱寫方案自動化隱寫分析的一個有前途的方向。

Bibliography

```
@article{fridrich2012rich,
  title={Rich models for steganalysis of digital images},
  author={Fridrich, Jessica and Kodovsky, Jan},
  journal={IEEE Transactions on information Forensics and Security},
  volume={7},
  number={3},
  pages={868--882},
  year={2012},
  publisher={IEEE}
}
```


79. Wang W, Dong J, Tan T. Exploring DCT coefficient quantization effects for local tampering detection. IEEE Trans. on Information Forensics and Security, 2014,9(10):1653−1666.

Wang, W., Dong, J., & Tan, T. (2014). Exploring DCT coefficient quantization effects for local tampering detection. IEEE Transactions on Information Forensics and Security, 9(10), 1653-1666.

Link : https://ieeexplore.ieee.org/document/6871380

Note : 利用 JPEG 压缩分析篡改痕迹

```
In this paper, we focus on local image tampering detection. 

For a JPEG image, the probability distributions of its DCT coefficients will be disturbed by tampering operation. 

The tampered region and the unchanged region have different distributions, which is an important clue for locating tampering. 

Based on the assumption of Laplacian distribution of unquantized ac DCT coefficients, these two distributions as well as the size of tampered region can be estimated so that the probability of each DCT block being tampered is obtained. 

More accurate localization results could be got when we consider the prior knowledge of common tampered regions. 

We also design three kinds of features that can distinguish truly tampered regions from the false ones to reduce false alarm. 

For a tampered image which is saved in lossless compressed format, we also propose the specialized approach, which employs the quantization noise of high-frequency DCT coefficient, to improve the tampering localization performance. 

Extensive experiments on large scale databases prove the effectiveness of our proposed method and demonstrate that our method is suitable for locating tampered regions with different scales.
```

本研究專注於局部圖像篡改檢測。對於JPEG圖像，其DCT係數的概率分佈會受到篡改操作的干擾，篡改區域和未改動區域分佈不同，是定位篡改的重要線索。基於未量化的ac DCT係數的拉普拉斯分佈假設，可以估計這兩個分佈以及被篡改區域的大小，從而得到每個DCT塊被篡改的概率，當研究者考慮常見篡改區域的先驗知識時，可以獲得更準確的定位結果，其研究者還設計了三種可以區分真實篡改區域和虛假區域的特徵，以減少誤報。對於以無損壓縮格式保存的篡改圖像，而研究者還提出了一種專門的方法，該方法利用高頻DCT係數的量化噪聲來提高篡改定位性能。在大規模數據庫上的大量實驗證明了該研究提出的方法的有效性，並證明其方法適用於定位不同尺度的篡改區域。

Bibliography

```
@article{wang2014exploring,
  title={Exploring DCT coefficient quantization effects for local tampering detection},
  author={Wang, Wei and Dong, Jing and Tan, Tieniu},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={9},
  number={10},
  pages={1653--1666},
  year={2014},
  publisher={IEEE}
}
```


80. Nataraj L, Sarkar A, Manjunath BS. Adding gaussian noise to “denoise” JPEG for detecting image resizing. In: Proc. of the 16th IEEE Int’l Conf. on Image Processing (ICIP). IEEE, 2009. 1493−1496.

Nataraj, L., Sarkar, A., & Manjunath, B. S. (2009, November). Adding gaussian noise to “denoise” JPEG for detecting image resizing. In 2009 16th IEEE International Conference on Image Processing (ICIP) (pp. 1493-1496). IEEE.

Link : https://ieeexplore.ieee.org/document/5414609

Note : 向 JPEG 压缩的图像中添加噪声提升检测性能

```
A common problem affecting most image resizing detection algorithms is that they are susceptible to JPEG compression. 

This is because JPEG introduces periodic artifacts, as it works on 8×8 blocks. 

We propose a novel yet counter intuitive technique to "denoise" JPEG images by adding Gaussian noise. 

We add a suitable amount of Gaussian noise to a resized and JPEG compressed image so that the periodicity due to JPEG compression is suppressed while that due to resizing is retained. 

The controlled Gaussian noise addition works better than median filtering and weighted averaging based filtering for suppressing the JPEG induced periodicity.
```

影響大多數圖像大小調整檢測算法的一個常見問題是它們容易受到 JPEG 壓縮的影響，這是因為 JPEG 引入了周期性偽影，因為它適用於 8×8 塊。研究者提出了一種新穎但反直覺的技術，通過添加高斯噪聲來“去噪”JPEG 圖像。其研究者將適量的高斯噪聲添加到調整大小和 JPEG 壓縮的圖像中，以便抑制由於 JPEG 壓縮而導致的周期性，而由於調整大小而導致的周期性得以保留，受控的高斯噪聲添加比中值濾波和基於加權平均的濾波更有效地抑制 JPEG 引起的周期性。

Bibliography

```
@inproceedings{nataraj2009adding,
  title={Adding gaussian noise to “denoise” JPEG for detecting image resizing},
  author={Nataraj, Lakshmanan and Sarkar, Anindya and Manjunath, Bangalore S},
  booktitle={2009 16th IEEE International Conference on Image Processing (ICIP)},
  pages={1493--1496},
  year={2009},
  organization={IEEE}
}
```


81. Bianchi T, De Rosa A, Piva A. Improved DCT coefficient analysis for forgery localization in JPEG images. In: Proc. of the IEEE Int’l Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011. 2444−2447.

Bianchi, T., De Rosa, A., & Piva, A. (2011, May). Improved DCT coefficient analysis for forgery localization in JPEG images. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2444-2447). IEEE.

Link : https://ieeexplore.ieee.org/document/5946978

Note : 向 JPEG 压缩的图像中添加噪声提升检测性能

```
In this paper, we propose a statistical test to discriminate between original and forged regions in JPEG images, under the hypothesis that the former are doubly compressed while the latter are singly compressed. 

New probability models for the DCT coefficients of singly and doubly compressed regions are proposed, together with a reliable method for estimating the primary quantization factor in the case of double compression. 

Based on such models, the probability for each DCT block to be forged is derived. Experimental results demonstrate a better discriminating behavior with respect to previously proposed methods.
```

在本文中，研究者提出了一種統計測試來區分 JPEG 圖像中的原始區域和偽造區域，假設前者是雙重壓縮的，而後者是單次壓縮的。提出了單壓縮和雙壓縮區域DCT係數的新概率模型，以及雙壓縮情況下估計主量化因子的可靠方法，基於這樣的模型，推導出每個 DCT 塊被偽造的概率。 實驗結果證明了相對於先前提出的方法更好的區分行為。

Bibliography

```
@inproceedings{bianchi2011improved,
  title={Improved DCT coefficient analysis for forgery localization in JPEG images},
  author={Bianchi, Tiziano and De Rosa, Alessia and Piva, Alessandro},
  booktitle={2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2444--2447},
  year={2011},
  organization={IEEE}
}
```


82. Pan X, Zhang X, Lyu S. Exposing image splicing with inconsistent local noise variances. In: Proc. of the IEEE Int’l Conf. on Computational Photography (ICCP). IEEE, 2012. 1−10.

Pan, X., Zhang, X., & Lyu, S. (2012, April). Exposing image splicing with inconsistent local noise variances. In 2012 IEEE International Conference on Computational Photography (ICCP) (pp. 1-10). IEEE.

Link : https://ieeexplore.ieee.org/document/6215223

Note : 利用局部噪音方差分析拼接痕迹

```
Image splicing is a simple and common image tampering operation, where a selected region from an image is pasted into another image with the aim to change its content. 

In this paper, based on the fact that images from different origins tend to have different amount of noise introduced by the sensors or post-processing steps, we describe an effective method to expose image splicing by detecting inconsistencies in local noise variances. 

Our method estimates local noise variances based on an observation that kurtosis values of natural images in band-pass filtered domains tend to concentrate around a constant value, and is accelerated by the use of integral image. 

We demonstrate the efficacy and robustness of our method based on several sets of forged images generated with image splicing.
```

圖像拼接是一種簡單而常見的圖像篡改操作，將圖像中的選定區域粘貼到另一張圖像中，目的是改變其內容。該研究基於來自不同來源的圖像往往具有由傳感器或後處理步驟引入的不同數量的噪聲，研究者描述了一種通過檢測局部噪聲方差的不一致性來暴露圖像拼接的有效方法，其方法基於以下觀察估計局部噪聲方差：帶通濾波域中自然圖像的峰度值傾向於集中在一個恆定值附近，並通過使用積分圖像來加速。最後基於通過圖像拼接生成的幾組偽造圖像證明了我們方法的有效性和魯棒性。

Bibliography

```
@inproceedings{pan2012exposing,
  title={Exposing image splicing with inconsistent local noise variances},
  author={Pan, Xunyu and Zhang, Xing and Lyu, Siwei},
  booktitle={2012 IEEE International Conference on Computational Photography (ICCP)},
  pages={1--10},
  year={2012},
  organization={IEEE}
}
```


83. Ferrara P, Bianchi T, De Rosa A, Piva A. Image forgery localization via fine-grained analysis of CFA artifacts. IEEE Trans. on Information Forensics and Security, 2012,7(5):1566−1577.

Ferrara, P., Bianchi, T., De Rosa, A., & Piva, A. (2012). Image forgery localization via fine-grained analysis of CFA artifacts. IEEE Transactions on Information Forensics and Security, 7(5), 1566-1577.

Link : https://ieeexplore.ieee.org/document/6210378

Note : 利用色彩过滤矩阵(color filter array,简称 CFA)模型进行篡改定位

```

In this paper, a forensic tool able to discriminate between original and forged regions in an image captured by a digital camera is presented. 

We make the assumption that the image is acquired using a Color Filter Array, and that tampering removes the artifacts due to the demosaicking algorithm. 

The proposed method is based on a new feature measuring the presence of demosaicking artifacts at a local level, and on a new statistical model allowing to derive the tampering probability of each 2 × 2 image block without requiring to know a priori the position of the forged region. 

Experimental results on different cameras equipped with different demosaicking algorithms demonstrate both the validity of the theoretical model and the effectiveness of our scheme.
```

該研究提出了一種能夠區分數碼相機捕獲的圖像中的原始區域和偽造區域的取證工具，研究者假設圖像是使用濾色器陣列獲取的，並且由於去馬賽克算法，篡改會消除偽影，其所提出的方法是基於一個新的特徵來測量局部水平的去馬賽克偽影的存在，以及一個新的統計模型，允許推導出每個 2 × 2 圖像塊的篡改概率，而無需先驗地知道偽造的位置。最後在配備不同去馬賽克算法的不同相機上的實驗結果證明了理論模型的有效性和方案的有效性。

Bibliography

```
@ARTICLE{6210378,
  author={Ferrara, Pasquale and Bianchi, Tiziano and De Rosa, Alessia and Piva, Alessandro},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Image Forgery Localization via Fine-Grained Analysis of CFA Artifacts}, 
  year={2012},
  volume={7},
  number={5},
  pages={1566-1577},
  doi={10.1109/TIFS.2012.2202227}}
```


84. Cozzolino D, Verdoliva L. Noiseprint: A CNN-based camera model fingerprint. IEEE Trans. on Information Forensics and Security, 2019,15:144−159.

Cozzolino, D., & Verdoliva, L. (2019). Noiseprint: a CNN-based camera model fingerprint. IEEE Transactions on Information Forensics and Security, 15, 144-159.

Link : https://arxiv.org/abs/1808.08396

Note : Cozzolino 等人设计了一个孪生网络,在来自不同相机的图像块上训练来提取图片的噪音指纹,从而实现检测.

```
Forensic analyses of digital images rely heavily on the traces of in-camera and out-camera processes left on the acquired images. 

Such traces represent a sort of camera fingerprint. 

If one is able to recover them, by suppressing the high-level scene content and other disturbances, a number of forensic tasks can be easily accomplished. 

A notable example is the PRNU pattern, which can be regarded as a device fingerprint, and has received great attention in multimedia forensics. 

In this paper we propose a method to extract a camera model fingerprint, called noiseprint, where the scene content is largely suppressed and model-related artifacts are enhanced. 

This is obtained by means of a Siamese network, which is trained with pairs of image patches coming from the same (label +1) or different (label -1) cameras.

Although noiseprints can be used for a large variety of forensic tasks, here we focus on image forgery localization. 

Experiments on several datasets widespread in the forensic community show noiseprint-based methods to provide state-of-the-art performance.
```

數字圖像的取證分析在很大程度上依賴於在獲取的圖像上留下的相機內和相機外過程的痕跡，這樣的痕跡代表了一種相機指紋，若能夠恢復它們，通過抑制高級場景內容和其他干擾，可以輕鬆完成多項取證任務。一個值得注意的例子是 PRNU 模式，它可以被視為設備指紋，在多媒體取證中受到了極大的關注。該研究提出了一種提取相機模型指紋的方法，稱為噪聲指紋，其中場景內容在很大程度上被抑制，與模型相關的偽影得到增強。這是通過連體網絡獲得的，該網絡使用來自相同（標籤 +1）或不同（標籤 -1）相機的成對圖像塊進行訓練。儘管噪聲印記可用於多種取證任務，但這裡我們專注於圖像偽造定位，而在廣泛使用的幾個數據集上的實驗表明，基於噪聲印記的方法可以提供最先進的性能。

Bibliography

```
@article{cozzolino2019noiseprint,
  title={Noiseprint: a CNN-based camera model fingerprint},
  author={Cozzolino, Davide and Verdoliva, Luisa},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={144--159},
  year={2019},
  publisher={IEEE}
}
```


85. Zhou P, Han X, Morariu VI, Davis LS. Learning rich features for image manipulation detection. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2018. 1053−1061.

Zhou, P., Han, X., Morariu, V. I., & Davis, L. S. (2018). Learning rich features for image manipulation detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1053-1061).

Link : https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Learning_Rich_Features_CVPR_2018_paper.html

Note : Zhou 等人提出了基于双流的 Faster R-CNN 网络

```
Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. 

We propose a two-stream Faster R-CNN network and train it end-to- end to detect the tampered regions given a manipulated image. 

One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to find tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. 

The other is a noise stream that leverages the noise features extracted from a steganalysis rich model filter layer to discover the noise inconsistency between authentic and tampered regions. 

We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. 

Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression.
```

圖像操作檢測不同於傳統的語義對象檢測，因為它更關注篡改偽影而不是圖像內容，這表明需要學習更豐富的特徵，研究者提出了一個雙流 Faster R-CNN 網絡並對其進行端到端訓練，以檢測給定操縱圖像的篡改區域。兩個流之一是 RGB 流，其目的是從 RGB 圖像輸入中提取特徵，以發現篡改偽影，如強對比度差異、不自然的篡改邊界等。另一種是噪聲流，它利用從隱寫分析豐富的模型過濾層中提取的噪聲特徵來發現真實區域和篡改區域之間的噪聲不一致。然後，研究者通過雙線性池化層融合來自兩個流的特徵，以進一步結合這兩種模式的空間共現。對四個標準圖像處理數據集的實驗表明，其雙流框架優於每個單獨的流，並且與具有調整大小和壓縮魯棒性的替代方法相比，還實現了最先進的性能。

Bibliography

```
@inproceedings{zhou2018learning,
  title={Learning rich features for image manipulation detection},
  author={Zhou, Peng and Han, Xintong and Morariu, Vlad I and Davis, Larry S},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1053--1061},
  year={2018}
}
```


86. Rao Y, Ni J. A deep learning approach to detection of splicing and copy-move forgeries in images. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). IEEE, 2016. 1−6.

Rao, Y., & Ni, J. (2016, December). A deep learning approach to detection of splicing and copy-move forgeries in images. In 2016 IEEE International Workshop on Information Forensics and Security (WIFS) (pp. 1-6). IEEE.

Link : https://ieeexplore.ieee.org/document/7823911

Note : 融合两条流的特征进行学习两个模态空间的信息.利用深度学习技术提取关键取证特征的工作也被不断探究

```
In this paper, we present a new image forgery detection method based on deep learning technique, which utilizes a convolutional neural network (CNN) to automatically learn hierarchical representations from the input RGB color images. 

The proposed CNN is specifically designed for image splicing and copy-move detection applications. 

Rather than a random strategy, the weights at the first layer of our network are initialized with the basic high-pass filter set used in calculation of residual maps in spatial rich model (SRM), which serves as a regularizer to efficiently suppress the effect of image contents and capture the subtle artifacts introduced by the tampering operations. 

The pre-trained CNN is used as patch descriptor to extract dense features from the test images, and a feature fusion technique is then explored to obtain the final discriminative features for SVM classification. 

The experimental results on several public datasets show that the proposed CNN based model outperforms some state-of-the-art methods.
```

該研究提出了一種基於深度學習技術的新圖像偽造檢測方法，該方法利用卷積神經網絡 (CNN) 從輸入的 RGB 彩色圖像中自動學習層次表示，所提出的 CNN 專為圖像拼接和復制移動檢測應用而設計，其網絡第一層的權重不是隨機策略，而是使用空間豐富模型 (SRM) 中殘差圖計算中使用的基本高通濾波器集進行初始化，作為正則化器，可以有效地抑制圖像內容並捕獲由篡改操作引入的細微偽影，將預訓練的 CNN 作為補丁描述符從測試圖像中提取密集特徵，然後探索特徵融合技術以獲得 SVM 分類的最終判別特徵。在幾個公共數據集上的實驗結果表明，所提出的基於 CNN 的模型優於一些最先進的方法。

Bibliography

```
@inproceedings{rao2016deep,
  title={A deep learning approach to detection of splicing and copy-move forgeries in images},
  author={Rao, Yuan and Ni, Jiangqun},
  booktitle={2016 IEEE International Workshop on Information Forensics and Security (WIFS)},
  pages={1--6},
  year={2016},
  organization={IEEE}
}
```


87. Liu B, Pun CM. Deep fusion network for splicing forgery localization. In: Proc. of the European Conf. on Computer Vision (ECCV). 2018. 237−251.

Liu, B., & Pun, C. M. (2018). Deep fusion network for splicing forgery localization. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (pp. 0-0).

Link : https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Liu_Deep_fusion_network_for_splicing_forgery_localization_ECCVW_2018_paper.pdf

Note : Liu 等人提出一个新的深度融合网络通过追踪边界来定位篡改区域

```
Digital splicing is a common type of image forgery: some regions of an image are replaced with contents from other images. 

To locate altered regions in a tampered picture is a challenging work because the difference is unknown between the altered regions and the original regions and it is thus necessary to search the large hypothesis space for a convincing result. 

In this paper, we proposed a novel deep fusion network to locate tampered area by tracing its border. 

A group of deep convolutional neural networks called Base-Net were firstly trained to response the certain type of splicing forgery respectively. 

Then, some layers of the Base-Net are selected and combined as a deep fusion neural network (Fusion-Net). 

After fine-tuning by a very small number of pictures, Fusion-Net is able to discern whether an image block is synthesized from different origins. 

Experiments on the benchmark datasets show that our method is effective in various situations and outperform state-of-the-art methods
```
數字拼接是一種常見的圖像偽造類型：圖像的某些區域被其他圖像的內容替換，在篡改圖片中定位更改區域是一項具有挑戰性的工作，因為更改區域和原始區域之間的差異是未知的，因此有必要在大的假設空間中搜索令人信服的結果。該研究提出了一種新穎的深度融合網絡，通過跟踪其邊界來定位篡改區域，首先訓練了一組稱為 Base-Net 的深度卷積神經網絡來分別響應某種類型的拼接偽造。然後，選擇 Base-Net 的一些層並組合為深度融合神經網絡（Fusion-Net），經過極少量的圖片微調後，Fusion-Net 能夠辨別圖像塊是否是從不同來源合成的。在基準數據集上的實驗表明，該研究方法在各種情況下都是有效的，並且優於最先進的方法

Bibliography

```
@inproceedings{liu2018deep,
  title={Deep fusion network for splicing forgery localization},
  author={Liu, Bo and Pun, Chi-Man},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}
```


88. Huh M, Liu A, Owens A, Efros A. Fighting fake news: Image splice detection via learned self-consistency. In: Proc. of the European Conf. on Computer Vision (ECCV). 2018. 101−117.

Huh, M., Liu, A., Owens, A., & Efros, A. A. (2018). Fighting fake news: Image splice detection via learned self-consistency. In Proceedings of the European conference on computer vision (ECCV) (pp. 101-117).

Link : https://openaccess.thecvf.com/content_ECCV_2018/html/Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper.html

Note : Minyoung 等人通过训练照片所包含的相机 EXIF 源数据指纹信息来区分图片是否被拼接

```
Advances in photo editing and manipulation tools have made it significantly easier to create fake imagery, highlighting the need for better visual forensics algorithms.

However, learning to detect manipulations from labelled training data is difficult due to the lack of good datasets of manipulated visual content. 

In this paper, we introduce a self-supervised method for learning to detect a visual manipulations using only unlabeled data. 

Given a large collection of real photographs with automatically recorded EXIF meta-data, we train a model to determine whether an image is self-consistent -- that is, whether its content could have been produced by a single imaging pipeline. We apply this self-supervised learning method to the task of localizing spliced image content. 

Our forensics model achieves state of the art results on many benchmarks, despite being trained without examples of actual manipulations, and without modeling specific detection cues. 

Beyond handcrafted benchmarks, we also show promising results spotting fakes on Reddit and The Onion, as well as detecting computer-generated splices.
```

照片編輯和操作工具的進步使得創建假圖像變得更加容易，突出了對更好的視覺取證算法的需求，然而，由於缺乏被操縱的視覺內容的良好數據集，學習從標記的訓練數據中檢測操縱是困難的，該研究介紹了一種自我監督方法，用於學習僅使用未標記數據來檢測視覺操作。給定大量帶有自動記錄的 EXIF 元數據的真實照片，研究者訓練一個模型來確定圖像是否是自洽的——也就是說，它的內容是否可以由單個成像管道產生。研究者將這種自我監督學習方法應用於定位拼接圖像內容的任務。其取證模型在許多基准上都取得了最先進的結果，儘管在沒有實際操作示例的情況下進行了訓練，也沒有對特定的檢測線索進行建模。除了手工製作的基準之外，研究者還展示了在 Reddit 和 The Onion 上發現假貨以及檢測計算機生成的拼接的有希望的結果。

Bibliography

```
@inproceedings{huh2018fighting,
  title={Fighting fake news: Image splice detection via learned self-consistency},
  author={Huh, Minyoung and Liu, Andrew and Owens, Andrew and Efros, Alexei A},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={101--117},
  year={2018}
}
```


89. Cun X, Pun CM. Image splicing localization via semi-global network and fully connected conditional random fields. In: Proc. of the European Conf. on Computer Vision (ECCV). 2018. 252−266.

Cun, X., & Pun, C. M. (2018). Image splicing localization via semi-global network and fully connected conditional random fields. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (pp. 0-0).

Link : https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Cun_Image_Splicing_Localization_via_Semi-Global_Network_and_Fully_Connected_Conditional_ECCVW_2018_paper.pdf

Note : Xiaodong 等人根据全局与局部块的特征不一致性学习一个半-全局网络实现拼接定位

```
We address the problem of image splicing localization: given an input image, localizing the spliced region which is cut from another image. 

We formulate this as a classification task but, critically, instead of classifying the spliced region by local patch, we leverage the features from whole image and local patch together to classify patch. 

We call this structure Semi-Global Network. 

Our approach exploits the observation that the spliced region should not only highly relate to local features (spliced edges), but also global features (semantic information, illumination, etc.) from the whole image. 

Furthermore, we first integrate Fully Connected Conditional Random Fields as post-processing technique in image splicing to improve the consistency between the input image and the output of the network. 

We show that our method outperforms other state-of-the-art methods in three popular datasets.
```

該研究解決了圖像拼接定位的問題：給定一張輸入圖像，定位從另一張圖像中剪下的拼接區域，並將其製定為分類任務，但關鍵的是，我們不是通過局部補丁對拼接區域進行分類，而是利用整個圖像和局部補丁的特徵來對補丁進行分類。而研究者們稱這種結構為 Semi-Global Network，其方法利用了拼接區域不僅應與局部特徵（拼接邊緣）高度相關，還應與整個圖像的全局特徵（語義信息、照明等）高度相關的觀察結果。此外，該研究首先將全連接條件隨機場作為圖像拼接中的後處理技術，以提高輸入圖像和網絡輸出之間的一致性。


Bibliography

```
@inproceedings{cun2018image,
  title={Image splicing localization via semi-global network and fully connected conditional random fields},
  author={Cun, Xiaodong and Pun, Chi-Man},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}
```


90. Cozzolino D, Poggi G, Verdoliva L. Recasting residual-based local descriptors as convolutional neural networks: An application to image forgery detection. In: Proc. of the 5th ACM Workshop on Information Hiding and Multimedia Security. 2017. 159−164.

Cozzolino, D., Poggi, G., & Verdoliva, L. (2017, June). Recasting residual-based local descriptors as convolutional neural networks: an application to image forgery detection. In Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security (pp. 159-164).

Link : https://arxiv.org/abs/1703.04615

Note : Cozzolino 等人提出使用卷积神经网络来学习基于残差的特征,此类特征可以有效提升取证检测和定位的性能

```
Local descriptors based on the image noise residual have proven extremely effective for a number of forensic applications, like forgery detection and localization. 

Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shifting on deep learning. 

In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). 

Then, by relaxing the constraints, and fine-tuning the net on a relatively small training set, we obtain a significant performance improvement with respect to the conventional detector.
```

基於圖像噪聲殘差的局部描述符已被證明對於許多取證應用非常有效，例如偽造檢測和定位，儘管如此，在計算機視覺領域取得可喜成果的推動下，研究界的重點現在正在轉向深度學習。該研究展示了一類基於殘差的描述符實際上可以被視為一個簡單的約束卷積神經網絡（CNN）。通過放鬆約束，並在相對較小的訓練集上微調網絡，其成果相對於傳統檢測器獲得了顯著的性能提升。

Bibliography

```
@inproceedings{cozzolino2017recasting,
  title={Recasting residual-based local descriptors as convolutional neural networks: an application to image forgery detection},
  author={Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa},
  booktitle={Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security},
  pages={159--164},
  year={2017}
}
```


91. Chen C, McCloskey S, Yu J. Focus manipulation detection via photometric histogram analysis. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2018. 1674−1682.

Chen, C., McCloskey, S., & Yu, J. (2018). Focus manipulation detection via photometric histogram analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1674-1682).

Link : https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Focus_Manipulation_Detection_CVPR_2018_paper.pdf

Note : Chen 等人则利用神经网络学习自然模糊和人为模糊带来的光直方图不一致性

```
With the rise of misinformation spread via social media channels, enabled by the increasing automation and realism of image manipulation tools, image forensics is an increasingly relevant problem. 

Classic image forensic methods leverage low-level cues such as metadata, sensor noise fingerprints, and others that are easily fooled when the image is re-encoded upon upload to facebook, etc. 

This necessitates the use of higher-level physical and semantic cues that, once hard to estimate reliably in the wild, have become more effective due to the increasing power of computer vision. 

In particular, we detect manipulations introduced by artificial blurring of the image, which creates inconsistent photometric relationships between image intensity and various cues. 

We achieve 98% accuracy on the most challenging cases in a new dataset of blur manipulations, where the blur is geometrically correct and consistent with the scene’s
physical arrangement. 

Such manipulations are now easily generated, for instance, by smartphone cameras having hardware to measure depth, e.g. ‘Portrait Mode’ of the iPhone7Plus. 

We also demonstrate good performance on a challenge dataset evaluating a wider range of manipulations in imagery representing ‘in the wild’ conditions.
```

隨著通過社交媒體渠道傳播的錯誤信息的興起，以及圖像處理工具的自動化和真實性的提高，圖像取證成為一個越來越重要的問題，經典的圖像取證方法利用低級線索，例如元數據、傳感器噪聲指紋等，當圖像在上傳到 Facebook 等時重新編碼時很容易被愚弄。這需要使用更高級別的物理和語義線索，這些線索曾經在野外難以可靠地估計，但由於計算機視覺的能力越來越強，它們變得更加有效。特別是，該研究的檢測由圖像的人工模糊引入的操作，這會在圖像強度和各種線索之間產生不一致的光度關係。在一個新的模糊操作數據集中，研究者在最具挑戰性的情況下實現了 98% 的準確率，其中模糊在幾何上是正確的並且與場景的一致
物理安排。這種操作現在很容易生成，例如，通過具有硬件來測量深度的智能手機相機，例如。 iPhone7Plus 的“人像模式”。最後還在挑戰數據集上展示了良好的性能，該數據集評估了代表“野外”條件的圖像中更廣泛的操作。

Bibliography

```
@inproceedings{chen2018focus,
  title={Focus manipulation detection via photometric histogram analysis},
  author={Chen, Can and McCloskey, Scott and Yu, Jingyi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1674--1682},
  year={2018}
}
```


92. Zhou P, Han X, Morariu VI, Davis LS. Two-stream neural networks for tampered face detection. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE, 2017. 1831−1839.

Zhou, P., Han, X., Morariu, V. I., & Davis, L. S. (2017, July). Two-stream neural networks for tampered face detection. In 2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW) (pp. 1831-1839). IEEE.

Link : https://arxiv.org/abs/1803.11276

Note : Zhou 等人将隐写噪声特征和卷积网络学习边界特征结合,提出了一个双流神经网络的方法.具体是用一个脸分类流训练一个 GoogleNet 检测篡改的人工痕迹,利用捕捉的局部噪音特征和拍照特征训练一个基于块的三元组(triplet) 网络,用这两条流的得分,综合判断是否图像被篡改.这是因为基于同一张图像的隐藏特征是相似的,距离小;不 同图像的块之间的隐藏特征距离大,用三元组训练出块的距离编码后,用一个 SVM 分类得到概率分数. 

```
We propose a two-stream network for face tampering detection. 

We train GoogLeNet to detect tampering artifacts in a face classification stream, and train a patch based triplet network to leverage features capturing local noise residuals and camera characteristics as a second stream. 

In addition, we use two different online face swapping applications to create a new dataset that consists of 2010 tampered images, each of which contains a tampered face. 

We evaluate the proposed two-stream network on our newly collected dataset. Experimental results demonstrate the effectiveness of our method.
```

研究者提出了一種用於人臉篡改檢測的雙流網絡，其訓練 GoogLeNet 以檢測人臉分類流中的篡改偽影，並訓練基於補丁的三元組網絡以利用捕獲局部噪聲殘差和相機特徵的特徵作為第二個流。此外，研究者使用兩個不同的在線人臉交換應用程序來創建一個新的數據集，該數據集由 2010 個篡改圖像組成，每個圖像都包含一個被篡改的人臉。最後在新收集的數據集上評估提出的雙流網絡。 實驗結果證明了其方法的有效性。

Bibliography

```
@inproceedings{zhou2017two,
  title={Two-stream neural networks for tampered face detection},
  author={Zhou, Peng and Han, Xintong and Morariu, Vlad I and Davis, Larry S},
  booktitle={2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW)},
  pages={1831--1839},
  year={2017},
  organization={IEEE}
}
```


93. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2015. 1−9.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).

Link : https://arxiv.org/abs/1409.4842

Note : 谷歌提出的神经网络框架 GoogLeNet

```
We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). 

The main hallmark of this architecture is the improved utilization of the computing resources inside the network. 

This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. 

To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. 

One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.
```

該研究提出了一種代號為“Inception”的深度卷積神經網絡架構，它負責在 2014 年 ImageNet 大規模視覺識別挑戰賽 (ILSVRC 2014) 中為分類和檢測設定新的技術水平，這種架構的主要標誌是提高了網絡內部計算資源的利用率。這是通過精心設計的設計實現的，該設計允許增加網絡的深度和寬度，同時保持計算預算不變，而為了優化質量，架構決策基於赫布原理和多尺度處理的直覺。研究者為 ILSVRC 2014 提交的文件中使用的一個特殊化身稱為 GoogLeNet，這是一個 22 層深度網絡，其質量在分類和檢測的上下文中進行評估。

Bibliography

```
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
```


94. Yang X, Li Y, Lyu S. Exposing deep fakes using inconsistent head poses. In: Proc. of the IEEE Int’l Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019. 8261−8265.

Yang, X., Li, Y., & Lyu, S. (2019, May). Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 8261-8265). IEEE.

Link : https://arxiv.org/abs/1811.00661

Note : Yang 等人认为 Deepfakes 创造的是分离的合成脸区域,这样在计算 3D 头部姿态评估的时候就会引入错误.

因为 Deepfakes 是交换中心脸区域的脸,脸外围关键点的位置仍保持不变,中心和外围位置的关键点坐标不匹配,会导致 3D 头部姿态评估的不一致,故用中心区域的关键点计算一个头方向向量,整个脸计算的头方向向量,衡量这两个向量之间的差异.
针对视频计算所有帧的头部姿态差异,最后训练一个支持向量机(SVM)分类器来学习这种差异

```
In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes). 

Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. 

We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue. 

Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes.
```

該研究提出了一種新方法來暴露 AI 生成的假人臉圖像或視頻（通常稱為 Deep Fakes），其方法是基於觀察到 Deep Fakes 是通過將合成的人臉區域拼接到原始圖像中來創建的，並且在這樣做的過程中，當從人臉圖像估計 3D 頭部姿勢時可以發現錯誤。研究者進行實驗來證明這種現象，並進一步開發基於這種線索的分類方法。使用基於此線索的特徵，使用一組真實面部圖像和 Deep Fakes 評估 SVM 分類器。

Bibliography

```
@inproceedings{yang2019exposing,
  title={Exposing deep fakes using inconsistent head poses},
  author={Yang, Xin and Li, Yuezun and Lyu, Siwei},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8261--8265},
  year={2019},
  organization={IEEE}
}
```


95. Yang X, Li Y, Qi H, Lyu S. Exposing GAN-synthesized faces using landmark locations. In: Proc. of the ACM Workshop on Information Hiding and Multimedia Security. 2019. 113−118.

Yang, X., Li, Y., Qi, H., & Lyu, S. (2019, July). Exposing gan-synthesized faces using landmark locations. In Proceedings of the ACM Workshop on Information Hiding and Multimedia Security (pp. 113-118).

Link : https://arxiv.org/abs/1904.00167

Note : Yang 等人同时发现,GAN 网络生成的假人脸在关键点位置分布上与真实人脸不尽相同,尽管生成的假人脸在脸部细节上与真人相似,但是自然性和连贯性还是与真人有很大的不同之处,通过将关键点归一化的位置坐标作为特征喂入 SVM 分类器进行学习

```
Generative adversary networks (GANs) have recently led to highly realistic image synthesis results. 

In this work, we describe a new method to expose GAN-synthesized images using the locations of the facial landmark points. 

Our method is based on the observations that the facial parts configuration generated by GAN models are different from those of the real faces, due to the lack of global constraints. 

We perform experiments demonstrating this phenomenon, and show that an SVM classifier trained using the locations of facial landmark points is sufficient to achieve good classification performance for GAN-synthesized faces.
```

生成對抗網絡（GAN）最近導致了高度逼真的圖像合成結果。在這項工作中，研究者描述了一種使用面部標誌點的位置來展示 GAN 合成圖像的新方法。其方法是基於觀察到，由於缺乏全局約束，GAN 模型生成的面部部件配置與真實面部不同。研究者進行了演示這種現象的實驗，並表明使用面部標誌點的位置訓練的 SVM 分類器足以為 GAN 合成的面部實現良好的分類性能。

Bibliography

```
@inproceedings{yang2019exposing,
  title={Exposing gan-synthesized faces using landmark locations},
  author={Yang, Xin and Li, Yuezun and Qi, Honggang and Lyu, Siwei},
  booktitle={Proceedings of the ACM Workshop on Information Hiding and Multimedia Security},
  pages={113--118},
  year={2019}
}
```


96. Li Y, Chang MC, Lyu S. In ictu oculi: Exposing AI created fake videos by detecting eye blinking. In: Proc. of the IEEE Int’l Workshop on Information Forensics and Security (WIFS). IEEE, 2018. 1−7.

LIY, C. M., & InIctuOculi, L. Y. U. S. (2018). ExposingAICreated FakeVideosbyDetectingEyeBlinking. In 2018IEEEInterG national Workshop on Information Forensics and Security (WIFS). IEEE.

Link : https://ieeexplore.ieee.org/document/8630787

Note : Li 等人发现,正常人的眨眼频率和时间都有一定的范围,而 Deepfakes 伪造视频的人基本没有眨眼现象,或者频率跟正常视频有较大差别,这可能是伪造视频在生成时没有丰富多样的眨眼素材导致的

```
The new developments in deep generative networks have significantly improve the quality and efficiency in generating realistically-looking fake face videos. 

In this work, we describe a new method to expose fake face videos generated with deep neural network models. 

Our method is based on detection of eye blinking in the videos, which is a physiological signal that is not well presented in the synthesized fake videos. 

Our method is evaluated over benchmarks of eye-blinking detection datasets and shows promising performance on detecting videos generated with DNN based software DeepFake.
```

深度生成網絡的新發展顯著提高了生成逼真的假人臉視頻的質量和效率。在這項工作中，研究者描述了一種新方法來暴露由深度神經網絡模型生成的假人臉視頻，其方法基於檢測視頻中的眨眼，這是一種生理信號，在合成的假視頻中沒有很好地呈現。該方法在眨眼檢測數據集的基准上進行了評估，並在檢測使用基於 DNN 的軟件 DeepFake 生成的視頻方面表現出良好的性能。

Bibliography

```
@inproceedings{liy2018exposingaicreated,
  title={ExposingAICreated FakeVideosbyDetectingEyeBlinking},
  author={LIY, CHANG M and InIctuOculi, LYUS},
  booktitle={2018IEEEInterG national Workshop on Information Forensics and Security (WIFS). IEEE},
  year={2018}
}
```


97. Ciftci UA, Demir I. FakeCatcher: Detection of synthetic portrait videos using biological signals. arXiv preprint arXiv:1901.02212, 2019.

Ciftci, U. A., Demir, I., & Yin, L. (2020). Fakecatcher: Detection of synthetic portrait videos using biological signals. IEEE Transactions on Pattern Analysis and Machine Intelligence.

Link : https://arxiv.org/abs/1901.02212

Note : Ciftci 等人从脸部抽取 3 块区域来测量光电容积脉搏波信号,并将信号转换为一致性和连贯性特征,最后使用 SVM 对特征进行二分类

```
The recent proliferation of fake portrait videos poses direct threats on society, law, and privacy. 

Believing the fake video of a politician, distributing fake pornographic content of celebrities, fabricating impersonated fake videos as evidence in courts are just a few real world consequences of deep fakes. 

We present a novel approach to detect synthetic content in portrait videos, as a preventive solution for the emerging threat of deep fakes. 

In other words, we introduce a deep fake detector. 

We observe that detectors blindly utilizing deep learning are not effective in catching fake content, as generative models produce formidably realistic results. 

Our key assertion follows that biological signals hidden in portrait videos can be used as an implicit descriptor of authenticity, because they are neither spatially nor temporally preserved in fake content. 

To prove and exploit this assertion, we first engage several signal transformations for the pairwise separation problem, achieving 99.39% accuracy. 

Second, we utilize those findings to formulate a generalized classifier for fake content, by analyzing proposed signal transformations and corresponding feature sets.

Third, we generate novel signal maps and employ a CNN to improve our traditional classifier for detecting synthetic content. Lastly, we release an "in the wild" dataset of fake portrait videos that we collected as a part of our evaluation process. 

We evaluate FakeCatcher on several datasets, resulting with 96%, 94.65%, 91.50%, and 91.07% accuracies, on Face Forensics, Face Forensics++, CelebDF, and on our new Deep Fakes Dataset respectively. 

We also analyze signals from various facial regions, under image distortions, with varying segment durations, from different generators, against unseen datasets, and under several dimensionality reduction techniques.
```

最近虛假肖像視頻的氾濫對社會、法律和隱私構成了直接威脅。相信政客的假視頻，散佈名人的假色情內容，在法庭上偽造假冒的假視頻作為證據，這些只是深度造假在現實世界中的一些後果。研究者提出了一種檢測肖像視頻中合成內容的新方法，作為針對深度偽造威脅的預防性解決方案。換句話說，該研究引入了一個深度偽造檢測器，研究觀察到，盲目地利用深度學習的檢測器無法有效捕捉虛假內容，因為生成模型會產生非常逼真的結果。其研究者的關鍵斷言是，隱藏在肖像視頻中的生物信號可以用作真實性的隱含描述符，因為它們既不會在空間上也不會在時間上保留在虛假內容中。為了證明和利用這一斷言，該研究首先對成對分離問題進行了幾次信號轉換，達到了 99.39% 的準確率。其次，通過分析建議的信號轉換和相應的特徵集，利用這些發現為虛假內容制定通用分類器。第三，生成新的信號圖並使用 CNN 來改進我們用於檢測合成內容的傳統分類器。最後，發布了一個“在野外”的假肖像視頻數據集，而後在評估過程中收集了該數據集。研究在幾個數據集上評估 FakeCatcher，分別在人臉取證、人臉取證++、CelebDF 和研究本身新的 Deep Fakes 數據集上獲得 96%、94.65%、91.50% 和 91.07% 的準確率。該研究還分析了來自不同面部區域的信號，在圖像失真下，具有不同的片段持續時間，來自不同的生成器，針對看不見的數據集，以及在幾種降維技術下。

Bibliography

```
@article{ciftci2020fakecatcher,
  title={Fakecatcher: Detection of synthetic portrait videos using biological signals},
  author={Ciftci, Umur Aybars and Demir, Ilke and Yin, Lijun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}
```


98. Fernandes S, Raj S, Ortiz E, Vintila I, Salter M, Urosevic G, Jha S. Predicting heart rate variations of Deepfake videos using neural ODE. In: Proc. of the IEEE Int’l Conf. on Computer Vision Workshops. 2019. 1721−1729.

Fernandes, S., Raj, S., Ortiz, E., Vintila, I., Salter, M., Urosevic, G., & Jha, S. (2019). Predicting heart rate variations of deepfake videos using neural ode. In Proceedings of the IEEE/CVF international conference on computer vision workshops (pp. 0-0).

Link : https://ieeexplore.ieee.org/document/9022055

Note : Fernandes 等人利用心率生物信号来区分伪造视频,先通过血流造成的脸部皮肤颜色变化、前额的平均光密度、欧拉影像变化等 3 种方法来提取心率,然后采用神经常微分方程模型训练,最后测试 Deepfakes 视频时,主要依据正常视频与异常视频的心率分布不同.

```
Deepfake is a technique used to manipulate videos using computer code. 

It involves replacing the face of a person in a video with the face of another person. 

The automation of video manipulation means that deepfakes are becoming more prevalent and easier to implement. 

This can be credited to the emergence of apps like FaceApp and FakeApp, which allow users to create their own deepfake videos using their smartphones. 

It has hence become essential to detect fake videos, to avoid the spread of false information. 

A recent study shows that the heart rate of fake videos can be used to distinguish original and fake videos. 

In the study presented, we obtained the heart rate of original videos and trained the state-of-the-art Neural Ordinary Differential Equations (Neural-ODE) model. 

We then created deepfake videos using commercial software. 

The average loss obtained for ten original videos is 0.010927, and ten donor videos are 0.010041. 

The trained Neural-ODE was able to predict the heart rate of our 10 deepfake videos generated using commercial software and 320 deepfake videos of deepfakeTIMI database.

To best of our knowledge, this is the first attempt to train a Neural-ODE on original videos to predict the heart rate of fake videos.
```

Deepfake 是一種使用計算機代碼操縱視頻的技術，它涉及用另一個人的臉替換視頻中的一個人的臉。視頻處理的自動化意味著 deepfakes 變得越來越普遍和更容易實施，這可以歸功於 FaceApp 和 FakeApp 等應用程序的出現，這些應用程序允許用戶使用智能手機創建自己的 deepfake 視頻。因此，檢測虛假視頻以避免虛假信息的傳播變得至關重要，最近的一項研究表明，假視頻的心率可以用來區分原始視頻和假視頻。在所展示的研究中，研究者獲得了原始視頻的心率並訓練了最先進的神經常微分方程 (Neural-ODE) 模型。然後，研究者使用商業軟件製作了 deepfake 視頻，其十個原始視頻獲得的平均損失為 0.010927，十個捐贈視頻為 0.010041，經過訓練的 Neural-ODE 能夠預測我們使用商業軟件生成的 10 個 deepfake 視頻和 deepfakeTIMI 數據庫的 320 個 deepfake 視頻的 heart rate，據該研究者所知，這是首次嘗試在原始視頻上訓練 Neural-ODE 來預測假視頻的 heart rate。

Bibliography

```
@inproceedings{fernandes2019predicting,
  title={Predicting heart rate variations of deepfake videos using neural ode},
  author={Fernandes, Steven and Raj, Sunny and Ortiz, Eddy and Vintila, Iustina and Salter, Margaret and Urosevic, Gordana and Jha, Sumit},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision workshops},
  pages={0--0},
  year={2019}
}
```


99. Li Y, Lyu S. Exposing Deepfake videos by detecting face warping artifacts. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops, 2019. 46−52.

Li, Y., & Lyu, S. (2018). Exposing deepfake videos by detecting face warping artifacts. arXiv preprint arXiv:1811.00656.

Link : https://arxiv.org/abs/1811.00656

Note : Li 等人认为 Deepfakes 算法生成的图像分辨率有限,之后需要被转换到匹配替换的脸,这使得Deepfakes 的视频中留下更多可以辨别的人工痕迹,这个可以被深度神经网络有效地捕捉

```
In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as {\em DeepFake} videos hereafter) from real videos. 

Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. 

Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). 

Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classifier, our method does not need DeepFake generated images as negative training examples since we target the artifacts in affine face warping as the distinctive feature to distinguish real and fake images. 

The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example.

Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. 

Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice.
```

在這項工作中，研究者描述了一種新的基於深度學習的方法，可以有效地將 AI 生成的假視頻（以下稱為 {\em DeepFake} 視頻）與真實視頻區分開來。其方法基於當前 DeepFake 算法只能生成分辨率有限的圖像的觀察結果，這些圖像需要進一步變形以匹配源視頻中的原始人臉，這種變換在生成的 DeepFake 視頻中留下了獨特的偽影，我們證明它們可以被捲積神經網絡 (CNN) 有效地捕獲，與以前使用大量真實和 DeepFake 生成的圖像來訓練 CNN 分類器的方法相比，該方法不需要 DeepFake 生成的圖像作為負訓練示例，因為研究將仿射面部扭曲中的偽影作為區分真假的顯著特徵圖片。其研究方法的優點有兩個：（1）可以直接對圖像使用簡單的圖像處理操作來模擬這種偽影，使其成為反例。由於訓練 DeepFake 模型生成負樣本既費時又需要資源，因此該研究的方法在訓練數據收集方面節省了大量時間和資源； (2) 由於此類偽影普遍存在於來自不同來源的 DeepFake 視頻中，因此該研究的方法與其他方法相比更加穩健，研究的方法在兩組 DeepFake 視頻數據集上進行了評估，以了解其在實踐中的有效性。

Bibliography

```
@article{li2018exposing,
  title={Exposing deepfake videos by detecting face warping artifacts},
  author={Li, Yuezun and Lyu, Siwei},
  journal={arXiv preprint arXiv:1811.00656},
  year={2018}
}
```


100. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2016. 770−778. 

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).

Link : https://arxiv.org/abs/1512.03385

Note : ResNet 

```
Deeper neural networks are more difficult to train. 

We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. 

We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. 

We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. 

On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8x deeper than VGG nets but still having lower complexity. 

An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. 

This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.

The depth of representations is of central importance for many visual recognition tasks. 

Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. 

Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
```

更深層次的神經網絡更難訓練。研究者提出了一個殘差學習框架，以簡化比以前使用的網絡更深的網絡的訓練，該研究明確地將層重新定義為參考層輸入的學習殘差函數，而不是學習未參考的函數。其提供了全面的經驗證據，表明這些殘差網絡更容易優化，並且可以從顯著增加的深度中獲得準確性。在 ImageNet 數據集上，研究者評估深度高達 152 層的殘差網絡——比 VGG 網絡深 8 倍，但仍然具有較低的複雜度，這些殘差網絡的集合在 ImageNet 測試集上實現了 3.57% 的誤差。該結果在 ILSVRC 2015 分類任務中獲得第一名。我們還對具有 100 層和 1000 層的 CIFAR-10 進行了分析。表示的深度對於許多視覺識別任務至關重要，其研究在 COCO 對象檢測數據集上獲得了 28% 的相對改進，深度殘差網絡是該研究提交 ILSVRC 和 COCO 2015 比賽的基礎，研究者還在 ImageNet 檢測、ImageNet 定位、COCO 檢測和 COCO 分割任務中獲得了第一名。

Bibliography

```
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
```


101. Deepfakes. 2019. https://github.com/deepfakes/faceswap

Link : https://github.com/deepfakes/faceswap

Note : Deepfakes 换脸技术

Bibliography

```
@online{list1101,
    title     = "Deepfakes.",
    url       = "https://github.com/deepfakes/faceswap"
}
```


102. Zao app. 2019. https://zaodownload.com/

Link : https://zaodownload.com/

Note : Zao APP 提供大众换脸娱乐服务等

Bibliography

```
@online{list1102,
    title     = "Zao app.",
    url       = "https://zao-app.com/"
}
```


103. Deepfake detection challenge. 2020. https://www.kaggle.com/c/deepfake-detection-challenge

Link : https://www.kaggle.com/c/deepfake-detection-challenge

Note : Deepfakes 检测竞赛

Bibliography

```
@online{list1103,
    title     = "Deepfake detection challenge.",
    url       = "https://www.kaggle.com/c/deepfake-detection-challenge"
}
```


104. Girish N, Nandini C. A review on digital video forgery detection techniques in cyber forensics. Science, Technology and Development, 2019,3(6):235−239.

Girish, N., & Nandini, C. (2019). A review on digital video forgery detection techniques in cyber forensics. Science, Technology and Development, 3(6), 235-239.

Link : http://journalstd.com/gallery/28-sep2019a.pdf

Note : 只有针对早期图像篡改工作的一些总结

```
In this propelled day and age, we are abstractly developing with the mixed media content, especially digitized images and videos, to give a strong affirmation of occasion of events. 

However, less attention has been paid to the forensic video. 

Be that as it may, the availability of a couple of cutting edge ones has been created now to adjust or alter content/video altering and the product has created exceptional concern with respect to the unwavering quality of such content. 

Subsequently, over the last few years, forensic analysis of visual media has become a fundamental field of research, which essentially deals with the development of tools and systems that help decide whether advanced content under thinking is genuine or authentic, that is, a real and unaltered representation. 

Over the last few decades, this field of research has shown tremendous development and progress. 

However, existing contributions in this field are based on the manual comparison of the structure and content of the video container, which is time-consuming and
error-prone. 

The current work shows an exhaustive and exhaustive tendency to distributed writing in the field of cyber forensic investigation for the detection of video forgery, with an essential focus on the detection of forgery / manipulation, forgery of video capture and copying, and anti -forensic and enemy video counter of the crime scene investigation. 

Video Forensics continues to develop new technologies to verify the authenticity and integrity of digital videos. 
While most of the existing methods are based on the analysis of video data flow, recently, a new line of research was introduced to investigate the video life cycle based on video container analysis.
```

在過去的幾十年裡，這一研究領域取得了巨大的發展和進步。然而，該領域現有的貢獻是基於手動比較視頻容器的結構和內容，耗時且費力且容易出錯。目前的工作在網絡取證調查領域顯示出一種詳盡和詳盡的分佈式寫作趨勢，用於檢測視頻偽造，重點關注偽造/篡改檢測、視頻捕獲和復制偽造以及反取證和犯罪現場調查的敵人視頻計數器，Video Forensics 不斷開發新技術來驗證數位影像的真實性和完整性，雖然現有的大多數方法都是基於影像數據流分析的，但最近引入了一種新的研究方向來研究基於視頻容器分析的視頻生命週期。

Bibliography

```
@article{girish2019review,
  title={A review on digital video forgery detection techniques in cyber forensics},
  author={Girish, N and Nandini, C},
  journal={Science, Technology and Development},
  volume={3},
  number={6},
  pages={235--239},
  year={2019}
}
```


105. Nguyen TT, Nguyen CM, Nguyen DT, Nguyen DT, Nahavandi S. Deep learning for Deepfakes creation and detection. arXiv preprint arXiv:1909.11573, 2019.

Nguyen, T. T., Nguyen, Q. V. H., Nguyen, C. M., Nguyen, D., Nguyen, D. T., & Nahavandi, S. (2019). Deep learning for deepfakes creation and detection: A survey. arXiv preprint arXiv:1909.11573.

Link : https://arxiv.org/abs/1909.11573

Note : 2019 年只有针对早期图像篡改工作的一些总结，該 arXiv 有持續更新至 2022。

```
Deep learning has been successfully applied to solve various complex problems ranging from big data analytics to computer vision and human-level control. 

Deep learning advances however have also been employed to create software that can cause threats to privacy, democracy and national security. 

One of those deep learning-powered applications recently emerged is deepfake. 

Deepfake algorithms can create fake images and videos that humans cannot distinguish them from authentic ones. 

The proposal of technologies that can automatically detect and assess the integrity of digital visual media is therefore indispensable. 

This paper presents a survey of algorithms used to create deepfakes and, more importantly, methods proposed to detect deepfakes in the literature to date. 

We present extensive discussions on challenges, research trends and directions related to deepfake technologies. 

By reviewing the background of deepfakes and state-of-the-art deepfake detection methods, this study provides a comprehensive overview of deepfake techniques and facilitates the development of new and more robust methods to deal with the increasingly challenging deepfakes.
```

深度學習已成功應用於解決從大數據分析到計算機視覺和人類水平控制的各種複雜問題，然而，深度學習的進步也被用於創建可能對隱私、民主和國家安全造成威脅的軟件，最近出現的深度學習驅動的應用之一是 deepfake。其 Deepfake 演算法可以創建人類無法區分真實圖像和視頻的虛假圖像和視頻，因此，能夠自動檢測和評估數字視覺媒體完整性的技術的提議是必不可少的，該研究介紹了用於創建深度偽造的算法的調查，更重要的是，介紹了迄今為止文獻中提出的用於檢測深度偽造的方法。研究者對與深度偽造技術相關的挑戰、研究趨勢和方向進行了廣泛的討論。通過回顧 deepfake 的背景和最先進的 deepfake 檢測方法，本研究提供了 deepfake 技術的全面概述，並有助於開發新的、更強大的方法來處理日益具有挑戰性的 deepfake。

Bibliography

```
@article{nguyen2019deep,
  title={Deep learning for deepfakes creation and detection: A survey},
  author={Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Cuong M and Nguyen, Dung and Nguyen, Duc Thanh and Nahavandi, Saeid},
  journal={arXiv preprint arXiv:1909.11573},
  year={2019}
}
```


106. Zollhöfer M, Thies J, Garrido P, Bradley D, Beeler T, Perez P, Stamminger M, Niessner M, Theobalt C. State of the art on monocular 3D face reconstruction, tracking, and applications. Computer Graphics Forum, 2018,37(2):523−550.

Zollhöfer, M., Thies, J., Garrido, P., Bradley, D., Beeler, T., Pérez, P., ... & Theobalt, C. (2018, May). State of the art on monocular 3D face reconstruction, tracking, and applications. In Computer Graphics Forum (Vol. 37, No. 2, pp. 523-550).

Link : https://studios.disneyresearch.com/wp-content/uploads/2019/03/State-of-the-Art-on-Monocular-3D-Face-Reconstruction-Tracking-and-Applications-1.pdf

Note : Zollhofer 等人综述了当前比较主流的 3D 模型重建追踪等技术

```
The computer graphics and vision communities have dedicated long standing efforts in building computerized tools for reconstructing, tracking, and analyzing human faces based on visual input. 

Over the past years rapid progress has been made, which led to novel and powerful algorithms that obtain impressive results even in the very challenging case of reconstruction from a single RGB or RGB-D camera. 

The range of applications is vast and steadily growing as these technologies are further improving in speed, accuracy, and ease of use.

Motivated by this rapid progress, this state-of-the-art report summarizes recent trends in monocular facial performance capture and discusses its applications, which range from performance-based animation to real-time facial reenactment. 

We focus our discussion on methods where the central task is to recover and track a three dimensional model of the human face using optimization-based reconstruction algorithms. 

We provide an in-depth overview of the underlying concepts of real-world image formation, and we discuss common assumptions and simplifications that make these algorithms practical. 

In addition, we extensively cover the priors that are used to better constrain the under-constrained monocular reconstruction problem, and discuss the optimization techniques that are employed to recover dense, photo-geometric 3D face models from monocular 2D data. 

Finally, we discuss a variety of use cases for the reviewed algorithms in the context of motion capture, facial animation, as well as image and video editing.
```

計算機圖形和視覺社區長期致力於構建計算機化工具，以基於視覺輸入重建、跟踪和分析人臉，在過去幾年中取得了快速進展，這導致了新穎而強大的算法，即使在從單個 RGB 或 RGB-D 相機進行重建的極具挑戰性的情況下也能獲得令人印象深刻的結果，隨著這些技術在速度、準確性和易用性方面的進一步提高，應用範圍廣泛且穩步增長，在這一快速進展的推動下，這份最新的報告總結了單眼面部表演捕捉的最新趨勢，並討論了其應用，從基於表演的動畫到實時面部重演。研究將討論重點放在中心任務是使用基於優化的重建算法來恢復和跟踪人臉的三維模型的方法上，同時對現實世界圖像形成的基本概念進行了深入的概述，並討論了使這些算法實用的常見假設和簡化。此外，該研究廣泛涵蓋了用於更好地約束欠約束單目重建問題的先驗，並討論了用於從單目 2D 數據中恢復密集的照片幾何 3D 人臉模型的優化技術。最後，在動作捕捉、面部動畫以及圖像和視頻編輯的背景下討論了所審查算法的各種用例。

Bibliography

```
@inproceedings{zollhofer2018state,
  title={State of the art on monocular 3D face reconstruction, tracking, and applications},
  author={Zollh{\"o}fer, Michael and Thies, Justus and Garrido, Pablo and Bradley, Derek and Beeler, Thabo and P{\'e}rez, Patrick and Stamminger, Marc and Nie{\ss}ner, Matthias and Theobalt, Christian},
  booktitle={Computer Graphics Forum},
  volume={37},
  number={2},
  pages={523--550},
  year={2018},
  organization={Wiley Online Library}
}
```


107. FaceSwap. 2019. https://github.com/MarekKowalski/FaceSwap/

Link : https://github.com/MarekKowalski/FaceSwap/

Note : FaceSwap 是基于图形学的换脸方法,首先获取人脸关键点,然后通过 3D 模型对获取到的人脸关键点位置进行渲染,不断缩小目标形状和关键点定位间的差异,最后将渲染模型的图像进行混合,并利用色彩校正技术获取最终的图像

Bibliography

```
@online{list1107,
    title     = "FaceSwap.",
    url       = "https://github.com/MarekKowalski/FaceSwap/"
}
```


108. Dale K, Sunkavalli K, Johnson MK, Vlasic D, Matusik W, Pfister H. Video face replacement. In: Proc. of the SIGGRAPH Asia Conf. 2011. 1−10.

Dale, K., Sunkavalli, K., Johnson, M. K., Vlasic, D., Matusik, W., & Pfister, H. (2011, December). Video face replacement. In Proceedings of the 2011 SIGGRAPH Asia conference (pp. 1-10).

Link : https://vcg.seas.harvard.edu/publications/video-face-replacement

Note : Kevin 等提出了在视频里自动换脸的 3D 方法,不需要大量的手动操作和硬件采集,只需要一个单相机视频,通过用 3D 多线性模型追踪视频中的人脸,并用相应的 3D 形状将源人脸仿射到目标人脸

```
We present a method for replacing facial performances in video. 

Our approach accounts for differences in identity, visual appearance, speech, and timing between source and target videos. 

Unlike prior work, it does not require substantial manual operation or complex acquisition hardware, only single-camera video. 

We use a 3D multilinear model to track the facial performance in both videos. 

Using the corresponding 3D geometry, we warp the source to the target face and retime the source to match the target performance. 

We then compute an optimal seam through the video volume that maintains temporal consistency in the final composite. 

We showcase the use of our method on a variety of examples and present the result of a user study that suggests our results are difficult to distinguish from real video footage.
```

該研究提出了一種替換視頻中面部表演的方法ㄝ研究的方法考慮了源視頻和目標視頻之間在身份、視覺外觀、語音和時間方面的差異。與以前的工作不同，它不需要大量的手動操作或複雜的採集硬件，只需要單機視頻，研究者使用 3D 多線性模型來跟踪兩個視頻中的面部表現，使用相應的 3D 幾何，我們將源扭曲到目標面並重新定時源以匹配目標性能。然後，研究者通過視頻體積計算最佳接縫，以保持最終合成中的時間一致性。

Bibliography

```
@inproceedings{dale2011video,
  title={Video face replacement},
  author={Dale, Kevin and Sunkavalli, Kalyan and Johnson, Micah K and Vlasic, Daniel and Matusik, Wojciech and Pfister, Hanspeter},
  booktitle={Proceedings of the 2011 SIGGRAPH Asia conference},
  pages={1--10},
  year={2011}
}
```


109. Garrido P, Valgaerts L, Rehmsen O, Thormae T, Perez P, Theobalt C. Automatic face reenactment. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2014. 4217−4224.

Garrido, P., Valgaerts, L., Rehmsen, O., Thormahlen, T., Perez, P., & Theobalt, C. (2014). Automatic face reenactment. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4217-4224).

Link : https://vcai.mpi-inf.mpg.de/projects/FaceReenactment/files/FaceReenactment.pdf

Note : Pablo 等人用类似的3D方法来替换目标视频中演员的人脸,而保留原始的表情

```
We propose an image-based, facial reenactment system that replaces the face of an actor in an existing target video with the face of a user from a source video, while preserving the original target performance. 

Our system is fully automatic and does not require a database of source expressions.

Instead, it is able to produce convincing reenactment results from a short source video captured with an off-theshelf camera, such as a webcam, where the user performs
arbitrary facial gestures. 

Our reenactment pipeline is conceived as part image retrieval and part face transfer: 

The image retrieval is based on temporal clustering of target frames and a novel image matching metric that combines appearance and motion to select candidate frames from the source video, while the face transfer uses a 2D warping strategy that preserves the user’s identity. 

Our system excels in simplicity as it does not rely on a 3D face model, it is robust under head motion and does not require the source and target performance to be similar. 

We show convincing reenactment results for videos that we recorded ourselves and for low-quality footage taken from the Internet.
```

研究提出了一種基於圖像的面部重演系統，該系統將現有目標視頻中的演員面部替換為源視頻中用戶的面部，同時保留原始目標表現，其系統是全自動的，不需要源表達式數據庫。相反，它能夠從使用現成相機（例如網絡攝像頭）捕獲的短源視頻中產生令人信服的重演結果，用戶在其中執行任意的面部表情，研究者的重演流程被設想為部分圖像檢索和部分面部轉移：圖像檢索基於目標幀的時間聚類和一種新穎的圖像匹配度量，該度量結合了外觀和運動以從源視頻中選擇候選幀，而面部轉移使用保留用戶身份的 2D 變形策略。其系統在簡單性方面表現出色，因為它不依賴於 3D 人臉模型，它在頭部運動下很穩健，並且不需要源和目標性能相似。


Bibliography

```
@inproceedings{garrido2014automatic,
  title={Automatic face reenactment},
  author={Garrido, Pablo and Valgaerts, Levi and Rehmsen, Ole and Thormahlen, Thorsten and Perez, Patrick and Theobalt, Christian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4217--4224},
  year={2014}
}
```


110. Garrido P, Valgaerts L, Sarmadi H, Steiner I, Varanasi K, Perez P, Theobalt C. VDub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. Computer Graphics Forum, 2015,34(2):193−204.

Garrido, P., Valgaerts, L., Sarmadi, H., Steiner, I., Varanasi, K., Perez, P., & Theobalt, C. (2015, May). Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. In Computer graphics forum (Vol. 34, No. 2, pp. 193-204).

Link : https://vcai.mpi-inf.mpg.de/files/EuroGraphics2015/dubbing_high.pdf

Note : Pablo等人设计了一个系统,通过高质量的 3D 人脸捕捉技术,改变人脸从而匹配嘴巴的动作

```
In many countries, foreign movies and TV productions are dubbed, i.e., the original voice of an actor is replaced with a translation that is spoken by a dubbing actor in the country’s own language. 

Dubbing is a complex process that requires specific translations and accurately timed recitations such that the new audio at least coarsely adheres to the mouth motion in the video. 

However, since the sequence of phonemes and visemes in the original and the dubbing language are different, the video-to-audio match is never perfect, which is a major source of visual discomfort. 

In this paper, we propose a system to alter the mouth motion of an actor in a video, so that it matches the new audio track. 

Our paper builds on high-quality monocular capture of 3D facial performance, lighting and albedo of the dubbing and target actors, and uses audio analysis in combination with a space-time retrieval method to synthesize a new photo-realistically rendered and highly detailed 3D shape model of the mouth region to replace the target performance. 

We demonstrate plausible visual quality of our results compared to footage that has been professionally dubbed in the traditional way, both qualitatively and through a user study.
```

在許多國家，外國電影和電視作品被配音，即演員的原聲被配音演員用該國自己的語言所說的翻譯代替，配音是一個複雜的過程，需要特定的翻譯和準確定時的朗誦，以使新音頻至少粗略地貼合視頻中的嘴巴動作。然而，由於原作和配音語言中的音素和視位序列不同，視頻與音頻的匹配永遠不會完美，這是視覺不適的主要來源，在本文中，研究者提出了一種系統來改變視頻中演員的嘴部動作，使其與新的音軌相匹配。其研究建立在對配音和目標演員的 3D 面部表演、照明和反照率的高質量單目捕捉的基礎上，並結合使用音頻分析和時空檢索方法來合成一個新的照片般逼真的渲染和高度詳細的 3D 形狀嘴區域模型來替換目標性能。


Bibliography

```
@inproceedings{garrido2015vdub,
  title={Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track},
  author={Garrido, Pablo and Valgaerts, Levi and Sarmadi, Hamid and Steiner, Ingmar and Varanasi, Kiran and Perez, Patrick and Theobalt, Christian},
  booktitle={Computer graphics forum},
  volume={34},
  number={2},
  pages={193--204},
  year={2015},
  organization={Wiley Online Library}
}
```


111. Nirkin Y, Masi I, Tuan AT, Hassner T, Medioni G. On face segmentation, face swapping, and face perception. In: Proc. of the 13th IEEE Int’l Conf. on Automatic Face and Gesture Recognition (FG 2018). IEEE, 2018. 98−105.

Nirkin, Y., Masi, I., Tuan, A. T., Hassner, T., & Medioni, G. (2018, May). On face segmentation, face swapping, and face perception. In 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018) (pp. 98-105). IEEE.

Link : https://arxiv.org/abs/1704.06729

Note : Nirkin 等人用分割的思路促进换脸,通过网络分割出来的人脸估计 3D 人脸形状,最后融合源和目标这两个对齐的 3D 人脸形状

```
We show that even when face images are unconstrained and arbitrarily paired, face swapping between them is actually quite simple. 

To this end, we make the following contributions. 

(a) Instead of tailoring systems for face segmentation, as others previously proposed, we show that a standard fully convolutional network (FCN) can achieve remarkably fast and accurate segmentations, provided that it is trained on a rich enough example set. 

For this purpose, we describe novel data collection and generation routines which provide challenging segmented face examples. 

(b) We use our segmentations to enable robust face swapping under unprecedented conditions. 

(c) Unlike previous work, our swapping is robust enough to allow for extensive quantitative tests. 

To this end, we use the Labeled Faces in the Wild (LFW) benchmark and measure the effect of intra- and inter-subject face swapping on recognition. 

We show that our intra-subject swapped faces remain as recognizable as their sources, testifying to the effectiveness of our method. In line with well known perceptual studies, we show that better face swapping produces less recognizable inter-subject results. This is the first time this effect was quantitatively demonstrated for machine vision systems.
```

研究表明，即使人臉圖像不受約束且任意配對，它們之間的人臉交換實際上也非常簡單。為此，該研究做出以下貢獻。(a) 沒有像其他人之前提出的那樣為人臉分割定制系統，而是展示了標準的全卷積網絡 (FCN) 可以實現非常快速和準確的分割，前提是它在足夠豐富的示例集上進行訓練。為此，描述了新的數據收集和生成例程，這些例程提供了具有挑戰性的分割人臉示例。(b) 使用該研究的分割在前所未有的條件下實現強大的面部交換。(c) 與以前的工作不同，該研究的交換足夠強大，可以進行廣泛的定量測試。為此，研究者使用野外標記人臉 (LFW) 基準測試並測量對象內和對象間人臉交換對識別的影響。研究表明，其受試者內部交換的面孔仍然與其來源一樣可識別，證明了我們方法的有效性。與眾所周知的感知研究一致，而更好的面部交換會產生不太可識別的主體間結果。這是第一次在機器視覺系統中定量證明這種效果。

Bibliography

```
@inproceedings{nirkin2018face,
  title={On face segmentation, face swapping, and face perception},
  author={Nirkin, Yuval and Masi, Iacopo and Tuan, Anh Tran and Hassner, Tal and Medioni, Gerard},
  booktitle={2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
  pages={98--105},
  year={2018},
  organization={IEEE}
}
```


112. Lu Z, Li Z, Cao J, He R, Sun Z. Recent progress of face image synthesis. In: Proc. of the 4th IAPR Asian Conf. on Pattern Recognition (ACPR). IEEE, 2017. 7−12.

Lu, Z., Li, Z., Cao, J., He, R., & Sun, Z. (2017, November). Recent progress of face image synthesis. In 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR) (pp. 7-12). IEEE.

Link : https://arxiv.org/abs/1706.04717

Note : 深度学习在人脸篡改上的应用

```
Face synthesis has been a fascinating yet challenging problem in computer vision and machine learning. 

Its main research effort is to design algorithms to generate photo-realistic face images via given semantic domain. 

It has been a crucial prepossessing step of main-stream face recognition approaches and an excellent test of AI ability to use complicated probability distributions. 

In this paper, we provide a comprehensive review of typical face synthesis works that involve traditional methods as well as advanced deep learning approaches. 

Particularly, Generative Adversarial Net (GAN) is highlighted to generate photo-realistic and identity preserving results. 

Furthermore, the public available databases and evaluation metrics are introduced in details. 

We end the review with discussing unsolved difficulties and promising directions for future research.
```

人臉合成一直是計算機視覺和機器學習中一個引人入勝但具有挑戰性的問題，它的主要研究工作是設計算法以通過給定的語義域生成照片般逼真的人臉圖像。它一直是主流人臉識別方法的關鍵前置步驟，也是對人工智能使用複雜概率分佈能力的極好測試，該研究對涉及傳統方法和高級深度學習方法的典型人臉合成工作進行了全面回顧。特別是，Generative Adversarial Net (GAN) 被突出顯示以生成照片般逼真和身份保持的結果。此外，還詳細介紹了公開可用的數據庫和評估指標，研究以討論未解決的困難和未來研究的有希望的方向來結束審查。

Bibliography

```
@inproceedings{lu2017recent,
  title={Recent progress of face image synthesis},
  author={Lu, Zhihe and Li, Zhihang and Cao, Jie and He, Ran and Sun, Zhenan},
  booktitle={2017 4th IAPR Asian Conference on Pattern Recognition (ACPR)},
  pages={7--12},
  year={2017},
  organization={IEEE}
}
```


113. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courvile A, Bengio Y. Generative adversarial nets. In: Proc. of the Advances in Neural Information Processing Systems. 2014. 2672−2680.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.

Link : https://arxiv.org/abs/1406.2661

Note : GAN

```
We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: 

a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. 

The training procedure for G is to maximize the probability of D making a mistake. 

This framework corresponds to a minimax two-player game. 

In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. 

In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. 

There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. 

Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.
```

研究者提出了一個通過對抗過程估計生成模型的新框架，該研究同時訓練兩個模型：一個生成模型 G 捕獲數據分佈，一個判別模型 D 估計樣本來自訓練數據而不是 G 的概率。 G 的訓練過程是最大化 D 出錯的概率。這個框架對應於一個極小極大的兩人遊戲，在任意函數 G 和 D 的空間中，存在唯一解，G 恢復訓練數據分佈，D 處處等於 1/2。在 G 和 D 由多層感知器定義的情況下，整個系統可以通過反向傳播進行訓練。在訓練或生成樣本期間，不需要任何馬爾可夫鍊或展開的近似推理網絡，實驗通過對生成的樣本進行定性和定量評估，證明了框架的潛力。

Bibliography

```
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
```


114. Faceswap-GAN. 2019. https://github.com/shaoanlu/faceswap-GAN

Link : https://github.com/shaoanlu/faceswap-GAN

Note : Faceswap-GAN 就是增加了 GAN 技术的 Deepfakes,引入判别器的对抗损失函数,在生成的时候判别生成图像和原图的相似度,使得生成的图像质量有大幅度提高,另外引入了感知损失函数增加眼珠的转动效果

Bibliography

```
@online{list1113,
    title     = "Faceswap-GAN.",
    url       = "https://github.com/shaoanlu/faceswap-GAN"
}
```


115. Korshunova I, Shi W, Dambre J, Theis L. Fast face-swap using convolutional neural networks. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2017. 3677−3685.

Korshunova, I., Shi, W., Dambre, J., & Theis, L. (2017). Fast face-swap using convolutional neural networks. In Proceedings of the IEEE international conference on computer vision (pp. 3677-3685).

Link : https://arxiv.org/abs/1611.09577

Note : Korshunova 等人将换脸问题视为风格迁移问题,训练一个卷积神经网络,从非结构化的图片中学习这种外观,并设计内容损失和风格损失函数来保障生成高质量真实度的人脸图像.这些人脸转换还是依赖于大量的
源和目标人物的人脸图片训练,泛化性不强

```
We consider the problem of face swapping in images, where an input identity is transformed into a target identity while preserving pose, facial expression, and lighting. 

To perform this mapping, we use convolutional neural networks trained to capture the appearance of the target identity from an unstructured collection of his/her photographs.

This approach is enabled by framing the face swapping problem in terms of style transfer, where the goal is to render an image in the style of another one. 

Building on recent advances in this area, we devise a new loss function that enables the network to produce highly photorealistic results. 

By combining neural networks with simple pre- and post-processing steps, we aim at making face swap work in real-time with no input from the user.
```

研究考慮圖像中的人臉交換問題，其中輸入身份被轉換為目標身份，同時保留姿勢、面部表情和照明，為了執行這種映射，研究者使用經過訓練的捲積神經網絡從他/她的照片的非結構化集合中捕獲目標身份的外觀，此方法是通過在風格轉移方面構建人臉交換問題來實現的，其目標是用另一個風格來渲染圖像。基於該領域的最新進展，研究者設計了一種新的損失函數，使網絡能夠產生高度逼真的結果，通過將神經網絡與簡單的預處理和後處理步驟相結合，研究者的目標是讓面部交換實時工作，無需用戶輸入。

Bibliography

```
@inproceedings{korshunova2017fast,
  title={Fast face-swap using convolutional neural networks},
  author={Korshunova, Iryna and Shi, Wenzhe and Dambre, Joni and Theis, Lucas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3677--3685},
  year={2017}
}
```


116. Nirkin Y, Keller Y, Hassner T. FSGAN: Subject agnostic face swapping and reenactment. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 7184−7193.

Nirkin, Y., Keller, Y., & Hassner, T. (2019). Fsgan: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 7184-7193).

Link : https://arxiv.org/abs/1908.05932

Note : Yuval 等人基于 GAN 技术提出了一个主体无关的人脸替换和重建方法,通过引入特定域感知损失、重建损失和对抗损失,可以应用于成对的人脸,不需要在大量人脸上训练

```
We present Face Swapping GAN (FSGAN) for face swapping and reenactment. 

Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. 

To this end, we describe a number of technical contributions. 

We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. 

For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. 

Occluded face regions are handled by a face completion network. 

Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. 

This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior.
```

研究者提出了人臉交換 GAN (FSGAN) 用於人臉交換和重演，與以前的工作不同，FSGAN 與主題無關，可以應用於成對的面孔，而無需對這些面孔進行培訓。為此，研究者描述了一些技術貢獻，其推導出了一種新穎的基於循環神經網絡 (RNN) 的面部重演方法，該方法可針對姿勢和表情變化進行調整，並可應用於單個圖像或視頻序列，對於視頻序列，我們引入了基於重演、Delaunay 三角剖分和重心坐標的人臉視圖的連續插值，而被遮擋的人臉區域由人臉補全網絡處理。最後，研究者使用人臉融合網絡無縫融合兩張臉，同時保留目標膚色和光照條件。該網絡使用一種新穎的泊松混合損失，它將泊松優化與感知損失相結合。該研究將其方法與現有的最先進系統進行比較，並顯示研究的結果在質量和數量上都優越。

Bibliography

```
@inproceedings{nirkin2019fsgan,
  title={Fsgan: Subject agnostic face swapping and reenactment},
  author={Nirkin, Yuval and Keller, Yosi and Hassner, Tal},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7184--7193},
  year={2019}
}
```


117. Choi Y, Choi M, Kim M, Ha J, Kin S, Choo J. StarGAN: Unified generative adversarial networks for multi-domain image-toimage translation. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2018. 8789−8797.

Choi, Y., Choi, M., Kim, M., Ha, J. W., Kim, S., & Choo, J. (2018). Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8789-8797).

Link : https://arxiv.org/abs/1711.09020

Note : 技术被广泛用于生产虚拟的人脸和篡改人脸属性.如 StarGAN

```
Recent studies have shown remarkable success in image-to-image translation for two domains. 

However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. 

To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model.

Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. 

This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. 

We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.
```

最近的研究表明，在兩個領域的圖像到圖像轉換方面取得了顯著的成功。然而，現有方法在處理兩個以上的域時具有有限的可擴展性和魯棒性，因為應該為每對圖像域獨立構建不同的模型，為了解決這個限制，研究者提出了 StarGAN，這是一種新穎且可擴展的方法，可以僅使用單個模型為多個域執行圖像到圖像的轉換，StarGAN 的這種統一模型架構允許在單個網絡中同時訓練具有不同域的多個數據集。與現有模型相比，這導致 StarGAN 具有卓越的翻譯圖像質量，以及將輸入圖像靈活翻譯到任何所需目標域的新穎能力，研究者憑經驗證明了其方法在面部屬性轉移和麵部表情合成任務上的有效性。

Bibliography

```
@inproceedings{choi2018stargan,
  title={Stargan: Unified generative adversarial networks for multi-domain image-to-image translation},
  author={Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8789--8797},
  year={2018}
}
```


118. Zhang H, Xu T, Li H, Zhang S, Wang X, Huang X, Netaxas D. StackGAN++: Realistic image synthesis with stacked generative adversarial networks. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2018,41(8):1947−1962.

Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., & Metaxas, D. N. (2018). Stackgan++: Realistic image synthesis with stacked generative adversarial networks. IEEE transactions on pattern analysis and machine intelligence, 41(8), 1947-1962.

Link : https://www.researchgate.net/publication/320727533_StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks

Note : 技术被广泛用于生产虚拟的人脸和篡改人脸属性,Stackgan

```
Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. 

In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aimed at generating high-resolution photorealistic images. 

First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. 

The Stage-I GAN sketches primitive shape and colors of the object based on given text description, yielding low-resolution images. 

The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. 

Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. 

Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. 

StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. 

Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.
```

儘管生成對抗網絡 (GAN) 在各種任務中取得了顯著成功，但它們在生成高質量圖像方面仍然面臨挑戰。在研究提出了堆疊生成對抗網絡（StackGAN），旨在生成高分辨率照片級逼真圖像。首先，研究者提出了一種用於文本到圖像合成的兩階段生成對抗網絡架構 StackGAN-v1，Stage-I GAN 根據給定的文本描述勾勒出對象的原始形狀和顏色，產生低分辨率圖像，Stage-II GAN 將 Stage-I 結果和文本描述作為輸入，並生成具有照片般逼真細節的高分辨率圖像，其次，針對有條件和無條件的生成任務提出了一種先進的多階段生成對抗網絡架構 StackGAN-v2。研究者的 StackGAN-v2 由樹狀結構中的多個生成器和判別器組成；從樹的不同分支生成對應於同一場景的多個尺度的圖像，StackGAN-v2 通過聯合逼近多個分佈，顯示出比 StackGAN-v1 更穩定的訓練行為。大量實驗表明，所提出的堆疊生成對抗網絡在生成照片般逼真的圖像方面明顯優於其他最先進的方法。

Bibliography

```
@article{zhang2018stackgan++,
  title={Stackgan++: Realistic image synthesis with stacked generative adversarial networks},
  author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1947--1962},
  year={2018},
  publisher={IEEE}
}
```


119. Karras T, Aila T, Laine S, Lehtinen J. Progressive growing of GANs for improved quality, stability, and variation. In: Proc. of the 6th Int’l Conf. on Learning Representations (ICLR). 2018.

Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.

Link : https://arxiv.org/abs/1710.10196

Note : 技术被广泛用于生产虚拟的人脸和篡改人脸属性,PGAN

```
We describe a new training methodology for generative adversarial networks. 

The key idea is to grow both the generator and discriminator progressively: 

starting from a low resolution, we add new layers that model increasingly fine details as training progresses. 

This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. 

We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. 

Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. 

Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. 

As an additional contribution, we construct a higher-quality version of the CelebA dataset.
```

研究者描述了一種用於生成對抗網絡的新訓練方法。關鍵思想是逐步增長生成器和判別器：從低分辨率開始，我們添加了新的層，隨著訓練的進行，對越來越精細的細節進行建模。這既加快了訓練的速度，又極大地穩定了訓練，使研究者能夠生成質量前所未有的圖像，例如 1024^2 的 CelebA 圖像，其研究者還提出了一種簡單的方法來增加生成圖像的變化，並在無監督 CIFAR10 中實現 8.80 的創紀錄初始分數。此外，研究者描述了幾個實現細節，這些細節對於阻止生成器和判別器之間的不健康競爭很重要。最後，該研究建議在圖像質量和變化方面評估 GAN 結果的新指標，作為額外的貢獻，研究者構建了更高質量的 CelebA 數據集版本。

Bibliography

```
@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}
```


120. Antipov G, Baccouche M, Dugelay JL. Face aging with conditional generative adversarial networks. In: Proc. of the IEEE Int’l Conf. on Image Processing (ICIP). IEEE, 2017. 2089−2093.

Antipov, G., Baccouche, M., & Dugelay, J. L. (2017, September). Face aging with conditional generative adversarial networks. In 2017 IEEE international conference on image processing (ICIP) (pp. 2089-2093). IEEE.

Link : https://arxiv.org/abs/1702.01983

Note : Grigory 等人利用 conditional-GAN技术改变人的年龄

```
It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of exceptional visual fidelity. 

In this work, we propose the GAN-based method for automatic face aging. 

Contrary to previous works employing GANs for altering of facial attributes, we make a particular emphasize on preserving the original person's identity in the aged version of his/her face. 

To this end, we introduce a novel approach for "Identity-Preserving" optimization of GAN's latent vectors. 

The objective evaluation of the resulting aged and rejuvenated face images by the state-of-the-art face recognition and age estimation solutions demonstrate the high potential of the proposed method.
```

最近表明，生成對抗網絡 (GAN) 可以生成具有出色視覺保真度的合成圖像，在這項工作中，研究者提出了基於 GAN 的自動人臉老化方法。與以前使用 GAN 來改變面部屬性的工作相反，研究者特別強調在他/她的面部老化版本中保留原始人的身份。為此，該研究引入了一種新的方法來優化 GAN 的潛在向量的“身份保持”。通過最先進的人臉識別和年齡估計解決方案對產生的老化和恢復活力的人臉圖像進行客觀評估，證明了所提出方法的巨大潛力。

Bibliography

```
@inproceedings{antipov2017face,
  title={Face aging with conditional generative adversarial networks},
  author={Antipov, Grigory and Baccouche, Moez and Dugelay, Jean-Luc},
  booktitle={2017 IEEE international conference on image processing (ICIP)},
  pages={2089--2093},
  year={2017},
  organization={IEEE}
}
```


121. Mirza M, Osindero S. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.

Mirza, M., & Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.

Link : https://arxiv.org/abs/1411.1784

Note : conditional-GAN

```
Generative Adversarial Nets were recently introduced as a novel way to train generative models. 

In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. 

We show that this model can generate MNIST digits conditioned on class labels. 

We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.
```
生成對抗網絡最近作為一種訓練生成模型的新方法被引入，在這項工作中，研究者介紹了生成對抗網絡的條件版本，它可以通過簡單地輸入數據 y 來構建，我們希望同時對生成器和判別器進行條件處理。該研究展示了該模型可以生成以類標籤為條件的 MNIST 數字。其研究還說明了該模型如何用於學習多模態模型，並提供了圖像標記應用的初步示例，其中研究者演示了該方法如何生成不屬於訓練標籤的描述性標籤。

Bibliography

```
@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}
```


122. Huang R, Zhang S, Li T, He R. Beyond face rotation: Global and local perception GAN for photorealistic and identity preserving frontal view synthesis. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2017. 2439−2448.

Huang, R., Zhang, S., Li, T., & He, R. (2017). Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings of the IEEE international conference on computer vision (pp. 2439-2448).

Link : https://arxiv.org/abs/1704.04086

Note : 用 GAN 生成不同的人脸视角而保持全局的结构和局部细节

```
Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. 

Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. 

This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. 

Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoder-decoder network. 

Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. 

The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. 

Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. 

Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition.
```

從單張人臉圖像合成逼真的正面視圖在人臉識別領域具有廣泛的應用，儘管已經提出數據驅動的深度學習方法通過從大量面部數據中尋求解決方案來解決這個問題，但這個問題仍然具有挑戰性，因為它本質上是不適定的。該研究提出了一種雙路徑生成對抗網絡（TP-GAN），用於通過同時感知全局結構和局部細節來進行逼真的正面視圖合成。除了常用的全局編碼器-解碼器網絡之外，還提出了四個具有里程碑意義的補丁網絡來處理局部紋理。除了新穎的架構外，研究者通過引入對抗性損失、對稱性損失和身份保持損失的組合來很好地約束這個不適定問題，組合的損失函數利用正面人臉分佈和預訓練的判別式深度人臉模型來指導從側面看正面視圖的身份保持推斷。與之前主要依賴中間特徵進行識別的深度學習方法不同，該研究的方法直接利用合成的身份保持圖像來完成人臉識別和屬性估計等下游任務，其實驗結果表明，該研究的方法不僅提供了令人信服的感知結果，而且在大姿勢人臉識別方面也優於最先進的結果。

Bibliography

```
@inproceedings{huang2017beyond,
  title={Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis},
  author={Huang, Rui and Zhang, Shu and Li, Tianyu and He, Ran},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2439--2448},
  year={2017}
}
```


123. Thies J, Zollhöfer M, Nießner M, Valgaerts L, Stamminger M, Theobalt C. Real-time expression transfer for facial reenactment. ACM Trans. on Graphics (TOG), 2015,34(6):Article No.183.

Thies, J., Zollhöfer, M., Nießner, M., Valgaerts, L., Stamminger, M., & Theobalt, C. (2015). Real-time expression transfer for facial reenactment. ACM Trans. Graph., 34(6), 183-1.

Link : http://www.graphics.stanford.edu/~niessner/papers/2015/10face/thies2015realtime.pdf

Note : Thies 等人基于一个消费级的 RGB-D 相机,重建、追踪源和目标演员的 3D 模型并最后融合,从而进行实时的表情迁移

```
We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor. 

The novelty of our approach lies in the transfer and photorealistic re-rendering of facial deformations and detail into the target video in a way that the newly-synthesized expressions are virtually indistinguishable from a real video. 

To achieve this, we accurately capture the facial performances of the source and target subjects in real-time using a commodity RGB-D sensor. 

For each frame, we jointly fit a parametric model for identity, expression, and skin reflectance to the input color and depth data, and also reconstruct the scene lighting. 

For expression transfer, we compute the difference between the source and target expressions in parameter space, and modify the target parameters to match the source expressions. 

A major challenge is the convincing re-rendering of the synthesized target face into the corresponding video stream. 

This requires a careful consideration of the lighting and shading design, which both must correspond to the real-world environment. 

We demonstrate our method in a live setup, where we modify a video conference feed such that the facial expressions of a different person (e.g., translator) are matched in real-time.
```

研究提出了一種將面部表情從源視頻中的演員實時傳輸到目標視頻中的演員的方法，從而能夠對目標演員的面部表情進行臨時控制。其方法的新穎之處在於將面部變形和細節的轉移和逼真的重新渲染到目標視頻中，新合成的表情與真實視頻幾乎沒有區別。為了實現這一點，該研究使用商品 RGB-D 傳感器實時準確地捕捉源對象和目標對象的面部表現，對於每一幀，研究者將身份、表情和皮膚反射率的參數模型與輸入顏色和深度數據聯合擬合，並重建場景照明。對於表達式轉移，該研究計算參數空間中源表達式和目標表達式之間的差異，並修改目標參數以匹配源表達式。一個主要挑戰是將合成的目標人臉重新渲染到相應的視頻流中，這需要仔細考慮照明和陰影設計，兩者都必須與現實世界環境相對應。研究者在現場設置中演示了其方法，並修改了視頻會議來源，以便實時匹配不同人（例如翻譯）的面部表情。

Bibliography

```
@article{thies2015real,
  title={Real-time expression transfer for facial reenactment.},
  author={Thies, Justus and Zollh{\"o}fer, Michael and Nie{\ss}ner, Matthias and Valgaerts, Levi and Stamminger, Marc and Theobalt, Christian},
  journal={ACM Trans. Graph.},
  volume={34},
  number={6},
  pages={183--1},
  year={2015}
}
```


124. Thies J, Zollhofer M, Stamminger M, Theobalt C, Niebner M. Face2face: Real-time face capture and reenactment of RGB videos. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 2016. 2387−2395.

Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C., & Nießner, M. (2016). Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2387-2395).

Link : https://arxiv.org/abs/2007.14808

Note : Thies 等人提出了 Face2Face,通过利用 3D 重建技术和图像渲染技术,能够在商业视频流中进行人脸移动表情的修改

```
We present Face2Face, a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). 

The source sequence is also a monocular video stream, captured live with a commodity webcam. 

Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. 

To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. 

At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. 

Reenactment is then achieved by fast and efficient deformation transfer between source and target. 

The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. 

Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. 

We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.
```

研究提出了 Face2Face，這是一種用於對單目目標視頻序列（例如 Youtube 視頻）進行實時面部重演的新穎方法，源序列也是單目視頻流，使用商品網絡攝像頭實時捕獲。其目標是通過源演員為目標視頻的面部表情製作動畫，並以照片般逼真的方式重新渲染經過處理的輸出視頻。為此，研究者首先通過基於非剛性模型的捆綁解決了從單目視頻中恢復面部身份的約束不足問題。同時運行時使用密集的光度一致性測量來跟踪源視頻和目標視頻的面部表情。然後通過源和目標之間的快速有效的變形傳遞來實現重演，從目標序列中檢索與重新定位的表情最匹配的嘴巴內部，並對其進行扭曲以產生準確的擬合。最後，研究者令人信服地在相應的視頻流之上重新渲染合成的目標人臉，使其與現實世界的照明無縫融合。


Bibliography

```
@inproceedings{thies2016face2face,
  title={Face2face: Real-time face capture and reenactment of rgb videos},
  author={Thies, Justus and Zollhofer, Michael and Stamminger, Marc and Theobalt, Christian and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2387--2395},
  year={2016}
}
```


125. Thies J, Zollhöfer M, Theobalt C, Stamminger M, Niessner M. Headon: Real-time reenactment of human portrait videos. ACM Trans. on Graphics (TOG), 2018,37(4):1−13.

Thies, J., Zollhöfer, M., Theobalt, C., Stamminger, M., & Nießner, M. (2018). Headon: Real-time reenactment of human portrait videos. ACM Transactions on Graphics (TOG), 37(4), 1-13.

Link : https://arxiv.org/pdf/1805.11729.pdf

Note : Head on 通过修改视角和姿态独立的纹理实现视频级的渲染方法,从而实现完整的人重建方法,包括表情眼睛、头部移动等

```
We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. 

Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. 

A novel realtime reenactment algorithm employs this proxy to photo-realistically map the captured motion from the source actor to the target actor. 

On top of the coarse geometric proxy, we propose a video-based rendering technique that composites the modified target portrait video via view- and pose-dependent texturing, and creates photo-realistic imagery of the target actor under novel torso and head poses, facial expressions, and gaze directions. 

To this end, we propose a robust tracking of the face and torso of the source actor. 

We extensively evaluate our approach and show significant improvements in enabling much greater flexibility in creating realistic reenacted output videos.
```

研究提出了 HeadOn，這是第一個實時的源到目標重演方法，用於完整的人類肖像視頻，可以傳輸軀乾和頭部運動、面部表情和眼睛注視。給定目標演員的簡短 RGB-D 視頻，其研究者自動構建個性化幾何代理，該代理嵌入參數化頭部、眼睛和運動學軀幹模型。而一種新穎的實時重演算法使用此代理將捕獲的運動從源演員逼真地映射到目標演員。在粗略的幾何代理之上，研究者提出了一種基於視頻的渲染技術，該技術通過與視圖和姿勢相關的紋理合成修改後的目標肖像視頻，並在新穎的軀乾和頭部姿勢下創建目標演員的照片般逼真的圖像，面部表情和注視方向。為此，研究者建議對源演員的面部和軀幹進行穩健的跟踪。


Bibliography

```
@article{thies2018headon,
  title={Headon: Real-time reenactment of human portrait videos},
  author={Thies, Justus and Zollh{\"o}fer, Michael and Theobalt, Christian and Stamminger, Marc and Nie{\ss}ner, Matthias},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--13},
  year={2018},
  publisher={ACM New York, NY, USA}
}
```


126. Kim H, Garrido P, Tewari A, Xu W, Thies J, Niessner M, Perez P, Richardt C, Zollhofer M, Theobalt C. Deep video portraits. ACM Trans. on Graphics (TOG), 2018,37(4):1−14.

Kim, H., Garrido, P., Tewari, A., Xu, W., Thies, J., Niessner, M., ... & Theobalt, C. (2018). Deep video portraits. ACM Transactions on Graphics (TOG), 37(4), 1-14.

Link : https://gvv.mpi-inf.mpg.de/projects/DeepVideoPortraits/

Note : Kim 等人利用含有时空架构的生成网络将合成的渲染图转换成真实图,并能迁移头部表情等动作.尽管现有的图形学方法可以较好地合成或重建图像,但是严重依赖于高质量的 3D 内容

```
We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. 

In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. 

The core of our approach is a generative neural network with a novel space-time architecture. 

The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. 

The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. 

In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. 

With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. 

For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. 

To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect.
```

研究者提出了一種新穎的方法，該方法僅使用輸入視頻就可以對肖像視頻進行逼真的重新動畫處理。與僅限於面部表情操作的現有方法相比，研究者率先將完整的 3D 頭部位置、頭部旋轉、面部表情、眼睛注視和眨眼從源演員轉移到目標的肖像視頻演員，其方法的核心是具有新穎時空架構的生成神經網絡。該網絡將參數化人臉模型的合成渲染作為輸入，並據此預測給定目標演員的照片般逼真的視頻幀，這種從渲染到視頻的傳輸的真實性是通過仔細的對抗訓練來實現的，因此，研究者可以創建修改後的目標視頻，以模仿合成創建的輸入的行為。為了實現源到目標視頻的重新動畫，研究者使用從源視頻中重建的頭部動畫參數渲染合成目標視頻，並將其輸入到訓練好的網絡中，從而完全控制目標，憑藉自由重組源參數和目標參數的能力，研究者能夠演示各種視頻重寫應用程序，而無需明確建模頭髮、身體或背景。例如，該研究可以使用交互式用戶控制的編輯來重新製作完整的頭部，並實現高保真視覺配音。為了證明其成果輸出的高質量，研究者進行了一系列廣泛的實驗和評估，例如，用戶研究表明該研究的視頻編輯很難檢測到。

Bibliography

```
@article{kim2018deep,
  title={Deep video portraits},
  author={Kim, Hyeongwoo and Garrido, Pablo and Tewari, Ayush and Xu, Weipeng and Thies, Justus and Niessner, Matthias and P{\'e}rez, Patrick and Richardt, Christian and Zollh{\"o}fer, Michael and Theobalt, Christian},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--14},
  year={2018},
  publisher={ACM New York, NY, USA}
}
```


127. Thies J, Zollhöfer M, Nießner M. Deferred neural rendering: Image synthesis using neural textures. ACM Trans. on Graphics (TOG), 2019,38(4):1−12.

Thies, J., Zollhöfer, M., & Nießner, M. (2019). Deferred neural rendering: Image synthesis using neural textures. ACM Transactions on Graphics (TOG), 38(4), 1-12.

Link : https://arxiv.org/abs/1904.12356

Note : Thies 等人提出了延迟神经渲染的框架,与渲染网络一起优化神经纹理而生成合成的图像,此方法可以在不完美的3D内容上操作

```
The modern computer graphics pipeline can synthesize images at remarkable visual quality; 

however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. 

To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. 

Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. 

Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; 

however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. 

Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. 

In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. 

For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. 

This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. 

We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.
```

現代計算機圖形流水線可以以卓越的視覺質量合成圖像；但是，它需要定義明確的高質量 3D 內容作為輸入。在這項工作中，研究者探索了不完美的 3D 內容的使用，例如，從具有噪聲和不完整表面幾何的光度重建中獲得，同時仍然旨在產生照片般逼真的（重新）渲染。為了解決這個具有挑戰性的問題，研究者引入了延遲神經渲染，這是一種將傳統圖形管道與可學習組件相結合的圖像合成新範例。具體來說，該研究提出了神經紋理，它是作為場景捕獲過程的一部分進行訓練的學習特徵圖。與傳統紋理類似，神經紋理作為貼圖存儲在 3D 網格代理之上；然而，高維特徵圖包含更多信息，可以通過我們新的延遲神經渲染管道進行解釋，神經紋理和延遲神經渲染器都是端到端訓練的，即使原始 3D 內容不完美，我們也能夠合成照片般逼真的圖像。與傳統的黑盒 2D 生成神經網絡相比，研究者的 3D 表示使我們能夠明確控制生成的輸出，並允許廣泛的應用領域。例如，該研究可以合成時間一致的視頻重新渲染記錄的 3D 場景，因為研究者表示固有地嵌入在 3D 空間中。通過這種方式，可以利用神經紋理在靜態和動態環境中以實時速率連貫地重新渲染或操作現有視頻內容。該研究在幾個關於新視圖合成、場景編輯和麵部重演的實驗中展示了其方法的有效性，並與利用標準圖形管道和傳統生成神經網絡的最先進方法進行了比較。

Bibliography

```
@article{thies2019deferred,
  title={Deferred neural rendering: Image synthesis using neural textures},
  author={Thies, Justus and Zollh{\"o}fer, Michael and Nie{\ss}ner, Matthias},
  journal={ACM Transactions on Graphics (TOG)},
  volume={38},
  number={4},
  pages={1--12},
  year={2019},
  publisher={ACM New York, NY, USA}
}
```


128. Suwajanakorn S, Seitz SM, Kemelmacher-Shlizerman I. Synthesizing Obama: Learning lip sync from audio. ACM Trans. on Graphics (TOG), 2017,36(4):1−13.

Suwajanakorn, S., Seitz, S. M., & Kemelmacher-Shlizerman, I. (2017). Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics (ToG), 36(4), 1-13.

Link : https://grail.cs.washington.edu/projects/AudioToObama/

Note : Suwajanakorn等人利用循环神经网络建立语音到嘴型动作的映射,可以匹配输入的语音合成嘴型指定纹理动作

```
Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. 

Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. 

Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. 

Our approach produces photorealistic results.
```

鑑於美國總統巴拉克奧巴馬的音頻，研究者合成了一段高質量的他講話的視頻，並具有準確的口型同步，並合成到目標視頻剪輯中，一個循環神經網絡在他每週的演講視頻中經過數小時的訓練，可以學習從原始音頻特徵到嘴形的映射。給定每個時刻的嘴巴形狀，研究者合成高質量的嘴巴紋理，並將其與適當的 3D 姿勢匹配合成，以改變他在目標視頻中所說的內容，以匹配輸入音軌。其方法產生逼真的結果。

Bibliography

```
@article{10.1145/3072959.3073640,
author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
title = {Synthesizing Obama: Learning Lip Sync from Audio},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073640},
doi = {10.1145/3072959.3073640},
abstract = {Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {95},
numpages = {13},
keywords = {audiovisual speech, LSTM, face synthesis, videos, audio, big data, uncanny valley, RNN, lip sync}
}
```


129. Zakharov E, Shysheya A, Burkov E, Lempitsky V. Few-shot adversarial learning of realistic neural talking head models. In: Proc. of the IEEE Int’l Conf. on Computer Vision. 2019. 9459−9468.

Zakharov, E., Shysheya, A., Burkov, E., & Lempitsky, V. (2019). Few-shot adversarial learning of realistic neural talking head models. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9459-9468).

Link : https://arxiv.org/abs/1905.08233

Note : 有针对人物特写镜头中的图像合成

```
Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. 

In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. 

However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. 

Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. 

Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. 

We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.
```

最近的幾項工作表明，如何通過訓練卷積神經網絡來生成高度逼真的人頭圖像，為了創建個性化的說話頭模型，這些工作需要對單個人的大型圖像數據集進行訓練。然而，在許多實際場景中，這種個性化的說話頭部模型需要從一個人的幾個圖像視圖中學習，甚至可能是單個圖像。在這裡，研究者展示了一個具有如此少鏡頭能力的系統。它在大型視頻數據集上執行冗長的元學習，然後能夠將以前看不見的人的神經說話頭模型的少量和一次性學習構建為具有高容量生成器和鑑別器的對抗性訓練問題。至關重要的是，該系統能夠以特定於人的方式初始化生成器和判別器的參數，因此儘管需要調整數千萬個參數，但訓練可以僅基於幾張圖像并快速完成。其研究表明，這種方法能夠學習高度逼真和個性化的新人說話頭部模型，甚至是肖像畫。

Bibliography

```
@InProceedings{Zakharov_2019_ICCV,
author = {Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and Lempitsky, Victor},
title = {Few-Shot Adversarial Learning of Realistic Neural Talking Head Models},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}
```


130. Fried O, Tewari A, Zollhöfer M, Finkelstein A, Shechtman E, Goldman D, Genova K, Jin Z, Theobalt C, Agrawala M. Text-based editing of talking-head video. ACM Trans. on Graphics (TOG), 2019,38(4):1−14.

Fried, O., Tewari, A., Zollhöfer, M., Finkelstein, A., Shechtman, E., Goldman, D. B., ... & Agrawala, M. (2019). Text-based editing of talking-head video. ACM Transactions on Graphics (TOG), 38(4), 1-14.

Link : https://arxiv.org/abs/1906.01524

Note : 有针对人物特写镜头中的图像合成

```
Editing talking-head video to change the speech content or to remove filler words is challenging. 

We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). 

Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame.

To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. 

The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. 

Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. 

We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.
```

編輯說話頭視頻以更改語音內容或刪除填充詞具有挑戰性，研究者提出了一種新穎的方法來編輯說話頭視頻的腳本，以生成逼真的輸出視頻，其中說話者的對話已被修改，同時保持無縫的視聽流（即沒有跳轉）。其方法使用音素、視位、3D 面部姿勢和幾何形狀、反射率、表情和每幀場景照明自動註釋輸入的說話頭視頻，要編輯視頻，用戶只需編輯腳本，然後優化策略選擇輸入語料庫的片段作為基礎材料。與所選片段對應的註釋參數無縫拼接在一起，並用於生成中間視頻表示，其中臉部的下半部分用參數化臉部模型渲染，最後該循環視頻生成網絡將此表示轉換為與編輯後的文字記錄相匹配的逼真視頻。研究者展示了各種各樣的編輯，例如單詞的添加、刪除和更改，以及令人信服的語言翻譯和完整的句子合成。

Bibliography

```
@article{10.1145/3306346.3323028,
author = {Fried, Ohad and Tewari, Ayush and Zollh\"{o}fer, Michael and Finkelstein, Adam and Shechtman, Eli and Goldman, Dan B and Genova, Kyle and Jin, Zeyu and Theobalt, Christian and Agrawala, Maneesh},
title = {Text-Based Editing of Talking-Head Video},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323028},
doi = {10.1145/3306346.3323028},
abstract = {Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {68},
numpages = {14},
keywords = {visemes, neural rendering, dubbing, face parameterization, talking heads, face tracking, text-based video editing}
}
```


131. Averbuch-Elor H, Cohen-Or D, Kopf J, Cohen M. Bringing portraits to life. ACM Trans. on Graphics (TOG), 2017,36(6):Article No.196.

Averbuch-Elor, H., Cohen-Or, D., Kopf, J., & Cohen, M. F. (2017). Bringing portraits to life. ACM Transactions on Graphics (TOG), 36(6), 1-13.

Link : https://dl.acm.org/doi/abs/10.1145/3130800.3130818

Note : 基于 2D 仿射的源演员表情匹配

```
We present a technique to automatically animate a still portrait, making it possible for the subject in the photo to come to life and express various emotions. 

We use a driving video (of a different subject) and develop means to transfer the expressiveness of the subject in the driving video to the target portrait. 

In contrast to previous work that requires an input video of the target face to reenact a facial performance, our technique uses only a single target image. 

We animate the target image through 2D warps that imitate the facial transformations in the driving video. 

As warps alone do not carry the full expressiveness of the face, we add fine-scale dynamic details which are commonly associated with facial expressions such as creases and wrinkles. 

Furthermore, we hallucinate regions that are hidden in the input target face, most notably in the inner mouth. 

Our technique gives rise to reactive profiles, where people in still images can automatically interact with their viewers. 

We demonstrate our technique operating on numerous still portraits from the internet.
```

研究者提出了一種自動為靜止肖像製作動畫的技術，使照片中的主體能夠栩栩如生並表達各種情感，該研究使用（不同主題的）駕駛視頻，並開發了將駕駛視頻中主題的表現力轉移到目標肖像的方法。與之前需要目標面部的輸入視頻來重現面部表現的工作相比，我們的技術僅使用單個目標圖像，研究者通過模仿駕駛視頻中的面部變換的 2D 扭曲對目標圖像進行動畫處理。由於單獨的經線不能承載面部的全部表現力，因此研究者添加了通常與面部表情相關的精細動態細節，例如摺痕和皺紋。此外，該研究對隱藏在輸入目標面部中的區域產生幻覺，尤其是在嘴巴內。其技術產生了反應式配置文件，靜止圖像中的人可以自動與他們的觀眾互動，最後展示了該研究在互聯網上的大量靜態肖像上運行的技術。

Bibliography

```
@article{10.1145/3130800.3130818,
author = {Averbuch-Elor, Hadar and Cohen-Or, Daniel and Kopf, Johannes and Cohen, Michael F.},
title = {Bringing Portraits to Life},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3130800.3130818},
doi = {10.1145/3130800.3130818},
abstract = {We present a technique to automatically animate a still portrait, making it possible for the subject in the photo to come to life and express various emotions. We use a driving video (of a different subject) and develop means to transfer the expressiveness of the subject in the driving video to the target portrait. In contrast to previous work that requires an input video of the target face to reenact a facial performance, our technique uses only a single target image. We animate the target image through 2D warps that imitate the facial transformations in the driving video. As warps alone do not carry the full expressiveness of the face, we add fine-scale dynamic details which are commonly associated with facial expressions such as creases and wrinkles. Furthermore, we hallucinate regions that are hidden in the input target face, most notably in the inner mouth. Our technique gives rise to reactive profiles, where people in still images can automatically interact with their viewers. We demonstrate our technique operating on numerous still portraits from the internet.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {196},
numpages = {13},
keywords = {face animation, facial reenactment}
}
```


132. Lample G, Zeghidour N, Usunier N, Bordes A, Denoyer L, Ranzato M. Fader networks: Manipulating images by sliding attributes. In: Proc. of the Advances in Neural Information Processing Systems. 2017. 5967−5976. 

Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., & Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. Advances in neural information processing systems, 30.

Link : https://arxiv.org/abs/1706.00409

Note : 基于网络编码空间的属性修改的表情迁移

```
This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. 

As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. 

By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image. 

This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. 

Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. 

We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.
```

該研究介紹了一種新的編碼器-解碼器架構，該架構經過訓練，可通過直接在潛在空間中解開圖像的顯著信息和屬性值來重建圖像。因此，經過訓練，其模型可以通過改變屬性值來生成輸入圖像的不同真實版本。通過使用連續的屬性值，研究者可以選擇在生成的圖像中可以感知多少特定屬性。此屬性可以允許用戶使用滑動旋鈕修改圖像的應用程序，例如混合控制台上的推子，以更改肖像的面部表情或更新某些對象的顏色。與主要依賴於通過在訓練時更改屬性值來訓練像素空間中的對抗性網絡的最先進技術相比，該研究的方法產生了更簡單的訓練方案並且可以很好地擴展到多個屬性。其提供的證據表明，該模型可以顯著改變屬性的感知價值，同時保持圖像的自然性。

Bibliography

```
@inproceedings{NIPS2017_3fd60983,
 author = {Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and DENOYER, Ludovic and Ranzato, Marc\textquotesingle Aurelio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fader Networks:Manipulating Images by Sliding Attributes},
 url = {https://proceedings.neurips.cc/paper/2017/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf},
 volume = {30},
 year = {2017}
}
```


133. Van Den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, Senior AW, Kavukcuoglu K. Wavenet: A generative model for raw audio. In: Proc. of the 9th Speech Synthesis Workshop. 2016.

Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.

Link : https://arxiv.org/abs/1609.03499

Note : WaveNet ,这是第一个端到端的语音合成器,一种基于音频生成模型,能够产生于人相似的音频

```
This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. 

The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; 

nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. 

When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. 

A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. 

When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.
```

該研究介紹 WaveNet，一種用於生成原始音頻波形的深度神經網絡。該模型是完全概率和自回歸的，每個音頻樣本的預測分佈都以所有先前的樣本為條件；儘管如此，該研究證明它可以在每秒數万個音頻樣本的數據上進行有效訓練。當應用於文本到語音時，它產生了最先進的性能，人類聽眾認為它比英語和普通話的最佳參數和連接系統聽起來更自然。單個 WaveNet 可以以相同的保真度捕獲許多不同說話者的特徵，並且可以通過調節說話者身份在它們之間切換。當訓練為音樂建模時，我們發現它會生成新穎且通常高度逼真的音樂片段。 其研究者還表明它可以用作判別模型，為音素識別返回有希望的結果。

Bibliography

```
@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}
```


134. Arik S, Chrzanowski M, Coates A, Diamos G, Kang Y, Li X, Miller J, Ng A, Raiman J, Sengupta S, Shoeybi M. Deep voice: Real-time neural text-to-speech. In: Proc. of the 34th Int’l Conf. on Machine Learning. 2017. 195−204.

Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., ... & Shoeybi, M. (2017, July). Deep voice: Real-time neural text-to-speech. In International Conference on Machine Learning (pp. 195-204). PMLR.

Link : https://arxiv.org/abs/1702.07825

Note : 文本到语音合成系统 Deep voice

```
We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. 

Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. 

The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. 

For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. 

For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. 

By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. 

Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.
```

研究者展示了 Deep Voice，這是一個完全由深度神經網絡構建的生產質量的文本到語音系統，Deep Voice 為真正的端到端神經語音合成奠定了基礎。該系統包括五個主要構建塊：定位音素邊界的分割模型、字素到音素的轉換模型、音素持續時間預測模型、基頻預測模型和音頻合成模型。對於分割模型，該研究提出了一種使用連接主義時間分類 (CTC) 損失的深度神經網絡執行音素邊界檢測的新方法，對於音頻合成模型，研究者實現了 WaveNet 的變體，它需要的參數更少，訓練速度比原始模型快。通過為每個組件使用神經網絡，研究者的系統比傳統的文本到語音系統更簡單、更靈活，傳統的文本到語音系統的每個組件都需要費力的特徵工程和廣泛的領域專業知識。最後，研究者展示其系統的推理可以比實時更快地執行，並描述了在 CPU 和 GPU 上優化的 WaveNet 推理內核，與現有實現相比可實現高達 400 倍的加速。

Bibliography

```
@inproceedings{arik2017deep,
  title={Deep voice: Real-time neural text-to-speech},
  author={Ar{\i}k, Sercan {\"O} and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and others},
  booktitle={International Conference on Machine Learning},
  pages={195--204},
  year={2017},
  organization={PMLR}
}
```


135. Wang Y, Skerry-Ryan RJ, Stanton D, Wu Y, Weiss R, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S, Le Q, Agiomyrgiannakis Y, Clark B, Saurous R. Tacotron: Towards end-to-end speech synthesis. In: Proc. of the Interspeech 2017, 18th Annual Conf. of the Int’l Speech Communication Association. 2017. 4006−4010.

Wang, Y., Skerry-Ryan, R. J., Stanton, D., Wu, Y., Weiss, R. J., Jaitly, N., ... & Saurous, R. A. (2017). Tacotron: Towards end-to-end speech synthesis. arXiv preprint arXiv:1703.10135.

Link : https://arxiv.org/abs/1703.10135

Note : 文本到语音合成系统 Tacotron

```
A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. 

Building these components often requires extensive domain expertise and may contain brittle design choices. 

In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. 

Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. 

Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. 

In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.
```
文本到語音合成系統通常由多個階段組成，例如文本分析前端、聲學模型和音頻合成模塊，構建這些組件通常需要廣泛的領域專業知識，並且可能包含脆弱的設計選擇。該研究介紹了 Tacotron，這是一種直接從字符合成語音的端到端生成文本到語音模型。給定 text and audio 對，可以通過隨機初始化完全從頭開始訓練模型。 研究者提出了幾種關鍵技術，以使序列到序列框架在這項具有挑戰性的任務中表現良好。Tacotron 在美國英語上的主觀 5 級平均意見得分為 3.82，在自然度方面優於生產參數係統。此外，由於 Tacotron 在幀級別生成語音，因此它比樣本級別的自回歸方法要快得多。


Bibliography

```
@article{wang2017tacotron,
  title={Tacotron: Towards end-to-end speech synthesis},
  author={Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and others},
  journal={arXiv preprint arXiv:1703.10135},
  year={2017}
}
```


136. Arik S, Diamos G, Gibiansky A, Miller J, Peng K, Ping W, Raiman J, Zhou Y. Deep voice 2: Multi-speaker neural text-to-speech. In: Proc. of the Advances in Neural Information Processing Systems. 2017. 2962−2970.

Gibiansky, A., Arik, S., Diamos, G., Miller, J., Peng, K., Ping, W., ... & Zhou, Y. (2017). Deep voice 2: Multi-speaker neural text-to-speech. Advances in neural information processing systems, 30.

Link : https://arxiv.org/abs/1705.08947

Note : 百度对 Deep voice 进行了扩展,提出了 Deep voice2,通过使用低维度可训练的说话者编码来增强文本到语音的转换,使得单个模型能生成不同的声音

```
We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. 

As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: 

Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. 

We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. 

We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.
```

該研究介紹了一種使用低維可訓練說話人嵌入來增強神經文本到語音 (TTS) 的技術，以從單個模型生成不同的聲音。作為起點，研究者展示了對單說話人神經 TTS 的兩種最先進方法的改進：Deep Voice 1 和 Tacotron。該研究介紹了 Deep Voice 2，它基於與 Deep Voice 1 類似的管道，但使用更高性能的構建塊構建，並展示了與 Deep Voice 1 相比顯著的音頻質量改進，其研究通過引入後處理神經聲碼器來改進 Tacotron，並展示了顯著的音頻質量改進。然後，研究者在兩個多說話人 TTS 數據集上演示了該研究的 Deep Voice 2 和 Tacotron 多說話人語音合成技術。並展示了單個神經 TTS 系統可以從每個說話者不到半小時的數據中學習數百個獨特的聲音，同時實現高音頻質量合成並幾乎完美地保留說話者的身份。

Bibliography

```
@article{gibiansky2017deep,
  title={Deep voice 2: Multi-speaker neural text-to-speech},
  author={Gibiansky, Andrew and Arik, Sercan and Diamos, Gregory and Miller, John and Peng, Kainan and Ping, Wei and Raiman, Jonathan and Zhou, Yanqi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
```


137. Ping W, Peng K, Gibiansky A, Arik S, Kannan A, Narang S. Deep voice 3: 2000-speaker neural text-to-speech. In: Proc. of the ICLR. 2018. 214−217.

Ping, W., Peng, K., Gibiansky, A., Arik, S. Ö., Kannan, A., Narang, S., ... & Miller, J. (2017). Deep Voice 3: 2000-Speaker Neural Text-to-Speech.

Link : https://arxiv.org/abs/1710.07654

Note : Ping 等人提出的 Deep voice3[37]进一步改进了之前的 Deep voice 系列,Deep voice3 是一个基于注意力机制的全卷积 TTS 系统,通过设计字符到频谱图的结构,能够实现完全并行的计算,在不降低合成性能的情况下,速度更加快

```
We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. 

Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. 

In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. 

We also describe how to scale inference to ten million queries per day on one single-GPU server.
```

研究者展示了 Deep Voice 3，這是一種基於全卷積注意力的神經文本到語音 (TTS) 系統，Deep Voice 3 在自然度上與最先進的神經語音合成系統相匹配，同時訓練速度提高了十倍。 該研究將 Deep Voice 3 擴展到 TTS 前所未有的數據集大小，對來自 2000 多個揚聲器的超過 800 小時的音頻進行訓練。此外，研究確定了基於注意力的語音合成網絡的常見錯誤模式，演示瞭如何減輕它們，並比較了幾種不同的波形合成方法，最後還描述瞭如何在一台單 GPU 服務器上將推理擴展到每天一千萬個查詢。

Bibliography

```
@article{ping2017deep,
  title={Deep Voice 3: 2000-Speaker Neural Text-to-Speech.},
  author={Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan {\"O}mer and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
  year={2017}
}
```


138. Pascual S, Bonafonte A, Serra J. SEGAN: Speech enhancement generative adversarial network. In: Proc. of the Interspeech 2017, 18th Annual Conf. of the Int’l Speech Communication Association. 2017. 3642−3646.

Pascual, S., Bonafonte, A., & Serra, J. (2017). SEGAN: Speech enhancement generative adversarial network. arXiv preprint arXiv:1703.09452.

Link : https://arxiv.org/abs/1703.09452

Note : Santiago 等人则利用 GAN 技术对语音的噪音进行过滤,提高了生成语音的质量

```
Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. 

The majority of them tackle a limited number of noise conditions and rely on first-order statistics. 

To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. 

In this work, we propose the use of generative adversarial networks for speech enhancement. 

In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. 

We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. 

The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. 

With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.
```

當前的語音增強技術在頻譜域上運行和/或利用一些更高級別的特徵，它們中的大多數處理有限數量的噪聲條件並依賴一階統計數據。為了規避這些問題，由於深度網絡能夠從大型示例集中學習複雜功能，因此越來越多地使用它們。在這項工作中，研究建議使用生成對抗網絡進行語音增強。與當前技術相比，該研究在波形級別上操作，端到端訓練模型，並將 28 個揚聲器和 40 種不同的噪聲條件合併到同一模型中，以便在它們之間共享模型參數。研究者使用具有兩個揚聲器和 20 種替代噪聲條件的獨立、看不見的測試集來評估所提出的模型，增強的樣本證實了所提出模型的可行性，客觀和主觀評價都證實了它的有效性。有了這個，研究者開始探索用於語音增強的生成架構，這可能會逐漸結合進一步的以語音為中心的設計選擇，以提高它們的性能。

Bibliography

```
@article{pascual2017segan,
  title={SEGAN: Speech enhancement generative adversarial network},
  author={Pascual, Santiago and Bonafonte, Antonio and Serra, Joan},
  journal={arXiv preprint arXiv:1703.09452},
  year={2017}
}
```


139. Donahue C, McAuley J, Puckette M. Adversarial audio synthesis. In: Proc. of the 7th Int’l Conf. on Learning Representations (ICLR). 2019.

Donahue, C., McAuley, J., & Puckette, M. (2018). Adversarial audio synthesis. arXiv preprint arXiv:1802.04208.

Link : https://arxiv.org/abs/1802.04208

Note : Chris 等人提出了无监督音频合成模型,能够从小规模语音库中学习生成可理解的词汇

```
Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. 

Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. 

In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. 

WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. 

Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. 

We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.
```

音頻信號以高時間分辨率進行採樣，學習合成音頻需要在一系列時間尺度上捕獲結構。生成對抗網絡 (GAN) 在生成本地和全局連貫的圖像方面取得了廣泛成功，但它們在音頻生成方面的應用卻很少。該研究的研究者介紹了 WaveGAN，這是將 GAN 應用於原始波形音頻的無監督合成的首次嘗試，WaveGAN 能夠合成一秒鐘的具有全局相干性的音頻波形切片，適用於音效生成。其實驗表明，在沒有標籤的情況下，WaveGAN 在小詞彙量語音數據集上訓練時學會了生成可理解的單詞，並且還可以合成來自其他領域的音頻，例如鼓、鳥的發聲和鋼琴。研究者將 WaveGAN 與將設計用於圖像生成的 GAN 應用於類似圖像的音頻特徵表示的方法進行比較，發現這兩種方法都有前景。

Bibliography

```
@article{donahue2018adversarial,
  title={Adversarial audio synthesis},
  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
  journal={arXiv preprint arXiv:1802.04208},
  year={2018}
}
```


140. Li XR, Yu K. A Deepfakes detection technique based on two-stream network. Journal of Cyber Security, 2020,5(2):84−91 (in Chinese with English abstract).

LI, X., & YU, K. (2020). A Deepfakes detection technique based on two-stream network. Journal of Cyber Security, 5(2), 84-91.

Link : https://jcs.iie.ac.cn/xxaqxben/ch/reader/view_abstract.aspx?file_no=20200208

Note : 深度伪造生成技术开源工具与商业软件做了部分总结

```
With the rapid development of deep learning technology, Deep forgery techniques, such as Deepfakes, are beginning to fill every corner of the Internet. 

By utilizing the generative adversarial networks and auto-encoder technology, the Deepfakes replace faces and tamper with facial expressions easily. 

The Deepfakes can produce fake pornography, spread rumors, spread fake news, and even influence political elections, leading to disastrous social consequences. 

However, the detection technology for this kind of fake videos is still far behind the generation technology, and the existing works have some limitations. 

This paper first summarizes the existing generation and detection works, and analyzes the defects of the existing works, then we propose the two-stream network detection framework based on the EfficientNet. 

By testing on a large open source dataset, FaceForensics++, our detection method was able to detect fake videos with an average accuracy of over 99%, and improve the ability of the model to resist compression to a certain extent.
```

隨著深度學習技術的飛速發展，Deepfakes等深度偽造技術開始充斥互聯網的每一個角落，通過利用生成對抗網絡和自動編碼器技術，Deepfakes 可以輕鬆替換面部並篡改面部表情。Deepfakes 可以製作假色情、散佈謠言、散佈假新聞，甚至影響政治選舉，導致災難性的社會後果。但是，對於此類假視頻的檢測技術仍遠遠落後於生成技術，現有作品存在一定的局限性。該研究首先總結了現有的生成和檢測工作，分析了現有工作的缺陷，然後提出了基於 EfficientNet 的雙流網絡檢測框架。通過在大型開源數據集 FaceForensics++ 上進行測試，其研究者的檢測方法能夠檢測出平均超過 99% 的假視頻，並在一定程度上提高了模型的抗壓縮能力。

Bibliography

```
@article{li2020deepfakes,
  title={A Deepfakes detection technique based on two-stream network},
  author={LI, Xurong and YU, Kun},
  journal={Journal of Cyber Security},
  volume={5},
  number={2},
  pages={84--91},
  year={2020},
  publisher={Beijing Zhongke Journal Publising Co. Ltd.}
}
```


141. FakeApp. 2019. https://www.deepfakescn.com

Link : https://www.deepfakescn.com

Note : 商业化工具, 深度伪造生成技术

Bibliography

```
@online{list1141,
    title     = "FakeApp.",
    url       = "https://www.deepfakescn.com"
}
```


142. Faceapp. 2019. https://www.faceapp.com/

Link : https://www.faceapp.com/

Note : 商业化工具, 深度伪造生成技术


Bibliography

```
@online{list1142,
    title     = "Faceapp.",
    url       = "https://www.faceapp.com/"
}
```


143. DeepFaceLab. 2019. https://github.com/iperov/DeepFaceLab

Link : https://github.com/iperov/DeepFaceLab

Note : DeepfaceLab 对 Faceswap 项目的模型进行扩充,对人脸模型进行扩充


Bibliography

```
@online{list1143,
    title     = "DeepFaceLab.",
    url       = "https://github.com/iperov/DeepFaceLab"
}
```


144. Dfaker. 2019. https://github.com/dfaker/df

Link : https://github.com/dfaker/df

Note : DFaker, 使用 DSSIM loss 函数

Bibliography

```
@online{list1144,
    title     = "Dfaker.",
    url       = "https://github.com/dfaker/df"
}
```


145. DeepFake-tf. 2019. https://github.com/StromWine/DeepFake-tf

Link : https://github.com/StromWine/DeepFake-tf

Note : DeepFake-tf,同 Dfakeer 项目,使用 tensorflow 实现

Bibliography

```
@online{list1145,
    title     = "DeepFake-tf.",
    url       = "https://github.com/StromWine/DeepFake-tf"
}
```


146. Faceswap-Deepfake-Pytorch. 2019. https://github.com/Oldpan/Faceswap-Deepfake-Pytorch

Link :  https://github.com/Oldpan/Faceswap-Deepfake-Pytorch

Note : Faceswap-DeepfakePytorch,原理同 Faceswap 项目,使用 Pytorch 实现

Bibliography

```
@online{list1146,
    title     = "Faceswap-Deepfake-Pytorch.",
    url       = "https://github.com/Oldpan/Faceswap-Deepfake-Pytorch"
}
```



147. Deep-voice-conversion. 2020. https://github.com/andabi/deep-voice-conversion

Link : https://github.com/andabi/deep-voice-conversion

Note : Deep-voicevconversion, 只需要目标说话者的音波素材, 即可转换成特定目标人物的声音


Bibliography

```
@online{list1147,
    title     = "Deep-voice-conversion.",
    url       = "https://github.com/andabi/deep-voice-conversion"
}
```


148. MelNet. 2020. https://sjvasquez.github.io/blog/melnet/

Link : https://sjvasquez.github.io/blog/melnet/

Note : MelNet 基于频谱图的端到端语音生成


Bibliography

```
@online{list1148,
    title     = "MelNet.",
    url       = "https://sjvasquez.github.io/blog/melnet/"
}
```


149. Matern F, Riess C, Stamminger M. Exploiting visual artifacts to expose Deepfakes and face manipulations. In: Proc. of the IEEE Winter Applications of Computer Vision Workshops (WACVW). IEEE, 2019. 83−92.

Matern, F., Riess, C., & Stamminger, M. (2019, January). Exploiting visual artifacts to expose deepfakes and face manipulations. In 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW) (pp. 83-92). IEEE.

Link : https://ieeexplore.ieee.org/document/8638330

Note : 深度伪造开源数据集, UADFV, FakeAPP, 早期视频数据,量小

```
High quality face editing in videos is a growing concern and spreads distrust in video content. 

However, upon closer examination, many face editing algorithms exhibit artifacts that resemble classical computer vision issues that stem from face tracking and editing.

As a consequence, we wonder how difficult it is to expose artificial faces from current generators? 

To this end, we review current facial editing methods and several characteristic artifacts from their processing pipelines. 

We also show that relatively simple visual artifacts can be already quite effective in exposing such manipulations, including Deepfakes and Face2Face. 

Since the methods are based on visual features, they are easily explicable also to non-technical experts. 

The methods are easy to implement and offer capabilities for rapid adjustment to new manipulation types with little data available. 

Despite their simplicity, the methods are able to achieve AUC values of up to 0.866.
```

視頻中的高質量人臉編輯越來越受到關注，並傳播了對視頻內容的不信任。然而，經過仔細檢查，許多人臉編輯算法表現出類似於經典計算機視覺問題的偽影，這些問題源於人臉跟踪和編輯。因此，研究者想知道從當前的生成器中暴露人造面孔有多困難？為此，該研究回顧了當前的面部編輯方法和來自其處理管道的幾個特徵工件。其研究還表明，相對簡單的視覺偽影在暴露此類操作方面已經非常有效，包括 Deepfakes 和 Face2Face。由於這些方法基於視覺特徵，因此對於非技術專家也很容易解釋。這些方法易於實施，並提供了在可用數據很少的情況下快速調整新操作類型的能力。儘管它們很簡單，但這些方法能夠實現高達 0.866 的 AUC 值。

Bibliography

```
@inproceedings{matern2019exploiting,
  title={Exploiting visual artifacts to expose deepfakes and face manipulations},
  author={Matern, Falko and Riess, Christian and Stamminger, Marc},
  booktitle={2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)},
  pages={83--92},
  year={2019},
  organization={IEEE}
}
```


