# DMSASD - 数字媒体软件与系统开发 - Digital Media Software And System Development

> 2101212850 干皓丞

PKU 2022 個人實驗報告作業

## Details

Defending Your Voice: Adversarial Attack on Voice Conversion

https://arxiv.org/abs/2005.08781

Huang, C. Y., Lin, Y. Y., Lee, H. Y., & Lee, L. S. (2021, January). Defending your voice: Adversarial attack on voice conversion. In 2021 IEEE Spoken Language Technology Workshop (SLT) (pp. 552-559). IEEE.

```
Substantial improvements have been achieved in recent years in voice conversion, which converts the speaker characteristics of an utterance into those of another speaker without changing the linguistic content of the utterance. 

Nonetheless, the improved conversion technologies also led to concerns about privacy and authentication.

It thus becomes highly desired to be able to prevent one’s voice from being improperly utilized with such voice conversion technologies.

This is why we report in this paper the first known attempt to perform adversarial attack on voice conversion.

We introduce human imperceptible noise into the utterances of a speaker whose voice is to be defended. 

Given these adversarial examples, voice conversion models cannot convert other utterances so a to sound like being produced by the defended speaker. 

Preliminary experiments were conducted on two currently stateof-the-art zero-shot voice conversion models.

Objective and subjective evaluation results in both white-box and black-box scenarios are reported.

It was shown that the speaker characteristics of the converted utterances were made obviously different from those of the defended speaker, while the adversarial examples of the defended speaker are not distinguishable from the authentic utterances.
```

近年來在語音轉換方面取得了實質性的改進，將一個話語的說話者特徵轉換為另一個說話者的特徵，而不改變話語的語言內容。儘管如此，改進的轉換技術也導致了對隱私和身份驗證的擔憂。因此，非常希望能夠通過這種語音轉換技術來防止一個人的語音被不當使用。這就是為什麼該研究報告了對語音轉換執行對抗性攻擊的第一次已知嘗試，研究者將人類難以察覺的噪音引入說話人的話語中，而說話人的聲音要受到保護。鑑於這些對抗性示例，語音轉換模型無法將其他話語轉換為聽起來像是被防御者發出的聲音。在兩個當前最先進的零樣本語音轉換模型上進行了初步實驗。報告了白盒和黑盒場景中的客觀和主觀評估結果。結果表明，轉換後的話語的說話人特徵與被辯護人的說話人特徵明顯不同，而被辯護人的對抗樣本與真實話語沒有區別。

Index Terms:  voice conversion, adversarial attack, speaker verification, speaker representation

```
@INPROCEEDINGS{9383529,
  author={Huang, Chien-yu and Lin, Yist Y. and Lee, Hung-yi and Lee, Lin-shan},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Defending Your Voice: Adversarial Attack on Voice Conversion}, 
  year={2021},
  volume={},
  number={},
  pages={552-559},
  doi={10.1109/SLT48900.2021.9383529}}
```

## Code

1. https://yistlin.github.io/attack-vc-demo/

2. https://github.com/cyhuang-tw/attack-vc

3. https://github.com/resemble-ai/Resemblyzer


1. INTRODUCTION 前言

Voice conversion aims to alter some specific acoustic characteristics of an utterance, such as the speaker identity, while preserving the linguistic content. 

These technologies were made much more powerful by deep learning , but the improved technologies also led to concerns about privacy and authentication.

One’s identity may be counterfeited by voice conversion and exploited in improper ways, which is only one of the many deepfake problems observed today generated by deep learning, such as synthesized fake photos or fake voice. 

Detecting any of such artifacts or defending against such activities is thus increasingly important, which applies equally to voice conversion.

On the other hand, it has been widely known that neural networks are fragile in the presence of some specific noise, or prone to yield different or incorrect results if the input is disturbed by such subtle perturbations imperceptible to humans. 

Adversarial attack is to generate such subtle perturbations that can fool the neural networks.

It has been successful on some discriminative models, but less reported on generative models.

In this paper, we propose to perform adversarial attack on voice conversion to prevent one’s speaker characteristics from being improperly utilized with voice conversion. 

Humanimperceptible perturbations are added to the utterances produced by the speaker to be defended. 

Three different approaches, the end-to-end attack, embedding attack, and feedback attack are proposed, such that the speaker characteristics of the converted utterances would be made very different from those of the defended speaker. 

We conducted objective and subjective evaluations on two recent state-of-the-art zeroshot voice conversion models.

Objective speaker verification showed the converted utterances were significantly different from those produced by the defended speaker, which was then verified by subjective similarity test. 

The effectiveness of the proposed approaches was also verified for black-box attack via a proxy model closer to the real application scenario.

> 語音轉換旨在改變話語的某些特定聲學特徵，例如說話者身份，同時保留語言內容。這類技術通過深度學習變得更加強大，但改進的技術也導致了對隱私和身份驗證的擔憂，而一個人的身份可能會被語音轉換偽造並以不正當的方式被利用，這只是當今觀察到的由深度學習產生的許多 deepfake 問題之一，例如合成的假照片或假語音。因此，檢測任何此類偽影或防禦此類活動變得越來越重要，這同樣適用於語音轉換。
> 
> 另一方面，眾所周知地在於神經網絡在存在某些特定噪聲的情況下是脆弱的，或者如果輸入受到人類無法察覺的細微擾動的干擾，則容易產生不同或不正確的結果。其對抗性攻擊是產生可以欺騙神經網絡的微妙擾動。它在一些判別模型上取得了成功，但在生成模型上的報導較少。該研究建議對語音轉換執行對抗性攻擊，以防止一個人的說話者特徵被不恰當地用於語音轉換。人類難以察覺的擾動被添加到要防禦的說話者產生的話語中。提出了三種不同的方法，端到端攻擊、嵌入攻擊和反饋攻擊，使得轉換後的話語的說話人特徵與防禦說話人的說話人特徵大不相同。
>
> 研究者對兩個最近最先進的 zeroshot 語音轉換模型進行了客觀和主觀評估，客觀說話人驗證表明轉換後的話語與被辯護說話人產生的話語有顯著差異，然後通過主觀相似性測試進行驗證。通過更接近真實應用場景的代理模型，還驗證了所提出方法的有效性，用於黑盒攻擊。

2. RELATED WORKS 相關工作

2.1. Voice conversion 語音轉換

Traditionally, parallel data are required for voice conversion, or the training utterances of the two speakers must be paired and aligned. 

To overcome this problem, Chou et al. obtained disentangled representations respectively for linguistic content and speaker information with adversarial training; CycleGAN-VC used cycle-consistency to ensure the converted speech to be linguistically meaningful with the target speaker’s features; and StarGAN-VC introduced conditional input for many-to-many voice conversion. 

All these are limited to speakers seen in training.

Zero-shot approaches then tried to convert utterances to any speaker given only one example utterance without finetuning, and the target speaker is not necessarily seen before.

Chou et al. employed adaptive instance normalization for this purpose; AUTOVC integrated a pre-trained d-vector and an encoder bottleneck, achieving the state-of-the-art results.

> 傳統上，語音轉換需要並行數據，或者兩個說話者的訓練話語必須配對和對齊。為了克服這個問題，Chou et al. 通過對抗訓練分別獲得語言內容和說話者信息的解耦表示； CycleGAN-VC 使用循環一致性來確保轉換後的語音對目標說話者的特徵具有語言意義； StarGAN-VC 為多對多語音轉換引入了條件輸入。
>
> 零樣本方法嘗試將話語轉換為僅給定一個示例話語而不進行微調的任何說話者，並且目標說話者不一定以前見過。
>
> Chou et al. 為此目的採用了自適應實例規範化； AUTOVC 集成了預訓練的 d 向量和編碼器瓶頸，實現了最先進的結果。


Fig. 1: The encoder-decoder based voice conversion model and the three proposed approaches. 

Perturbations are updated on the utterances providing speaker characteristics, as the blue dashed lines indicate.

> 圖 1：基於編解碼器的語音轉換模型和提出的三種方法。如藍色虛線所示，擾動在提供說話者特徵的話語上更新。

2.2. Attacking and defending voice 進攻和防守的聲音

Automatic speech recognition (ASR) systems have been shown to be prone to adversarial attacks. 

Applying perturbations on the waveforms, spectrograms, or MFCC features was able to make ASR systems fail to recognize the speech correctly. 

Similar goals were achieved on speaker recognition by generating adversarial examples to fool automatic speaker verification (ASV) systems to predict that these examples had been uttered by a specific speaker.

Different approaches for spoofing ASV were also proposed to show the vulnerabilities of such systems. 

But to our knowledge, applying adversarial attacks on voice conversion has not been reported yet.

On the other hand, many approaches were proposed to defend one’s voice when ASV systems were shown to be vulnerable to spoofing attacks. 

In addition to the ASVspoof challenges for spoofing techniques and countermeasures, Liu et al. conducted adversarial attacks on those countermeasures, showing the fragility of them. 

Obviously, all neural network models are under the threat of adversarial attacks, which led to the idea of attacking voice conversion models as proposed here.


3. METHODOLOGIES 方法

A widely used model for voice conversion adopted an encoderdecoder structure, in which the encoder is further divided into a content encoder and a speaker encoder, as shown in Fig. 1.

This paper is also based on this model. 

The content encoder $E_{c}$ extracts the content information from an input utterance t yielding $E_{c}(t)$, while the speaker encoder $E_s$ embeds the speaker characteristics of an input utterance $x$ as a latent vector $E_s(x)$, as in the left part of Fig. 1. 

Taking $E_{c}(t)$ and $E_{s}(x)$ as the input, the decoder D generates a spectrogram F(t, x) with content information based on $E_{c}(t)$ and speaker characteristics based on $E_{s}(x)$.

Here we only focus on the utterances fed into the speaker encoder, since we are defending the speaker characteristics provided by such utterances. 

Motivated by the prior work, here we present three approaches to performing the attack, with the target being either the output spectrogram $F(t, x)$ (Sec. 3.1), or the speaker embedding $E_{s}(x)$ (Sec. 3.2), or the combination of the two (Sec. 3.3), as shown in Fig. 1.

一種廣泛使用的語音轉換模型採用編碼器解碼器結構，其中編碼器進一步分為內容編碼器和揚聲器編碼器，如圖 1 所示。

該研究也是基於這個模型。
內容編碼器 Ec 從輸入話語t中提取內容信息，產生 Ec(t)，而說話者編碼器 Es 嵌入輸入話語 x 的說話者特徵作為潛在向量 Es(x)，如圖 1 的左側部分。

以 Ec(t) 和 Es(x) 為輸入，解碼器 D 根據 Ec(t) 生成帶有內容信息的頻譜圖 F(t,x) 和基於 Es(x) 的說話人特徵。在此我們只關注輸入到說話者編碼器中的話語，因為我們正在捍衛這些話語提供的說話者特徵。受先前工作的啟發，這裡我們提出了三種執行攻擊的方法，目標是輸出頻譜圖 F(t, x)（第 3.1 節），或說話者嵌入 Es(x)（第 3.2 節），或兩者的組合（第 3.3 節），如圖 1 所示。


3.1. End-to-end attack 端到端攻擊

A straight-forward approach to perform adversarial attack on the above model in Fig. 1 is to take the decoder output $F(\boldsymbol{t}, \boldsymbol{x})$ as the target, referred to as end-to-end attack also shown in Fig. 1. 

Denote the original spectrogram of an utterance produced by the speaker to be defended as $\boldsymbol{x} \in \mathbb{R}^{M \times T}$ and the adversarial perturbation on $\boldsymbol{x})$ as $\boldsymbol{\delta} \in \mathbb{R}^{M \times T}$, where $M$ and $T$ are the total number of frequency components and time frames respectively. 

An untargeted attack simply aims to alter the output of the voice conversion model and can be expressed as:

無針對性的攻擊只是旨在改變語音轉換模型的輸出，可以表示為：

$\underset{\delta}{\operatorname{maximize}} \mathcal{L}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta}), F(\boldsymbol{t}, \boldsymbol{x}))$

subject to $\|\boldsymbol{\delta}\|_{\infty}<\epsilon$

$\mathcal{L}(\cdot, \cdot)$

$\mathcal{L}(\cdot, \cdot)$ is the distance between two vectors or the spectrograms for two signals and $\epsilon$ is a constraint making the perturbation subtle. 

The signal $\boldsymbol{t}$ can be arbitrary offering the content of the output utterance, on which we do not focus here.

Given a certain utterance $\boldsymbol{y}$ produced by a target speaker, we can formulate a targeted attack for output signal with specific speaker characteristics:

$\begin{array}{cc}\underset{\delta}{\operatorname{minimize}} & \mathcal{L}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta}), F(\boldsymbol{t}, \boldsymbol{y})) \\ & -\lambda \mathcal{L}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta}), F(\boldsymbol{t}, \boldsymbol{x}))\end{array}$
subject to $\|\boldsymbol{\delta}\|_{\infty}<\epsilon$


The first term in the first expression in (2) aims to make the model output sound like being produced by the speaker of $\boldsymbol{y}$, while the second term is to eliminate the original speaker identity in $\boldsymbol{x}$. 

$\lambda$ is a positive valued hyperparameter balancing the importance between source and target.

To effectively constrain the range of perturbation within $[-\epsilon, \epsilon]$ while solving (2), we adopt the approach of Change of variable as was done previously using tanh(·) function.

In this way (2) above becomes (3) below:

$\begin{array}{cc}\underset{\boldsymbol{w}}{\operatorname{minimize}} & \mathcal{L}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta}), F(\boldsymbol{t}, \boldsymbol{y})) \\ & -\lambda \mathcal{L}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta}), F(\boldsymbol{t}, \boldsymbol{x}))\end{array}$

subject to $\boldsymbol{\delta}=\epsilon \cdot \tanh (\boldsymbol{w})$

where $\boldsymbol{w} \in \mathbb{R}^{M \times T}$. 

The clipping function is not needed here.

3.2. Embedding attack 嵌入攻擊

The speaker encoder $E_s$ in Fig. 1 embeds an utterance into a latent vector. 

These latent vectors for utterances produced by the same speaker tend to cluster closely together, while those by different speakers tend to be separated apart. 

The second approach proposed here is focused on the speaker encoder by directly changing the speaker embeddings of the utterances, referred to as embedding attack also in Fig. 1. 

As the decoder $D$ produces the output F(\boldsymbol{t}, \boldsymbol{x}) with speaker characteristics based on the speaker embedding $E_{s}(x)$ as in Fig. 1, changing the speaker embeddings thus alters the output of the decoder.

Following the notations and expressions in (3), we have:

$\begin{array}{cc}\underset{\boldsymbol{w}}{\operatorname{minimize}} & \mathcal{L}\left(E_{s}(\boldsymbol{x}+\boldsymbol{\delta}), E_{s}(\boldsymbol{y})\right) \\ & \left.-\lambda \mathcal{L}\left(E_{s}(\boldsymbol{x}+\boldsymbol{\delta}), E_{s}(\boldsymbol{x})\right)\right)\end{array}$
subject to $\boldsymbol{\delta}=\epsilon \cdot \tanh (\boldsymbol{w})$

where the adversarial attack is now performed with the speaker encoder $E_s$ only.

Since only the speaker encoder is involved, it is therefore more efficient.

3.3. Feedback attack 反饋攻擊

The third approach proposed here tries to combine the above two approaches by feeding the output spectrogram $F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta})$ from the decoder $D$ back to the speaker encoder $E_s$ (the red feedback loop in Fig. 1), and consider the speaker embedding obtained in this way.

More specifically, $E_{s}(\boldsymbol{x}+\boldsymbol{\delta})$ in (4) is replaced by $E_{s}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta}))$ in (5). 

This is referred to as the feedback attack also in Fig. 1.

$\begin{array}{cc}\underset{\boldsymbol{w}}{\operatorname{minimize}} & \mathcal{L}\left(E_{s}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta})), E_{s}(\boldsymbol{y})\right) \\ & \left.-\lambda \mathcal{L}\left(E_{s}(F(\boldsymbol{t}, \boldsymbol{x}+\boldsymbol{\delta})), E_{s}(\boldsymbol{x})\right)\right)\end{array}$
subject to $\boldsymbol{\delta}=\epsilon \cdot \tanh (\boldsymbol{w})$

4. EXPERIMENTAL SETTINGS 實驗設置

We conducted experiments on the model proposed by Chou et al. (referred to as Chou’s model below) and AUTOVC.

Both were able to perform zero-shot voice conversion on unseen speakers given their few utterances without finetuning, considered suitable for our scenarios. 

Models such as StarGAN-VC, on the other hand, are limited to the voice produced by speakers seen during training, which makes it less likely to counterfeit other speakers’ voices, and we thus do not consider them here.

我們對 Chou et al. 提出的模型進行了實驗。  (以下簡稱 Chou 模型) 和 AUTOVC。鑑於他們的話語很少而無需微調，兩者都能夠對看不見的說話者執行零樣本語音轉換，這被認為適合我們的場景。另一方面，諸如 StarGAN-VC 之類的模型僅限於訓練過程中看到的說話者發出的聲音，這使得偽造其他說話者的聲音的可能性較小，因此我們在這裡不考慮它們。

4.1. Speaker encoders Speaker 編碼器

For Chou’s model, all modules were trained jointly from scratch on the CSTR VCTK Corpus. 

The speaker encoder took a 512-dim mel spectrogram and generated a 128-dim speaker embedding. 

AUTOVC utilized pre-trained d-vector as speaker encoder, with 80-dim mel spectrogram as input and 256-dim speaker embedding as output, pre-trained on VoxCeleb1 and LibriSpeech but generalizable to unseen speakers.

4.2. Vocoders 聲碼器

In inference, Chou’s model leveraged Griffin-Lim algorithm to synthesize the audio. 

AUTOVC previously adopted WaveNet as the spectrogram inverter, but here we used WaveRNN-based vocoder pre-trained on the VCTK corpus to generate waveforms with similar quality due to time limitation.

As we introduced perturbation on spectrogram, vocoders converting the spectrogram into waveform were necessary. 

We respectively adopted Griffin-lim algorithm and WaveRNN-based vocoder for attacks on Chou’s model and AUTOVC.

在推理中，Chou 的模型利用 Griffin-Lim 算法來合成音頻。AUTOVC 之前採用 WaveNet 作為頻譜圖逆變器，但由於時間限制，我們使用在 VCTK 語料庫上預訓練的基於 WaveRNN 的聲碼器生成具有相似質量的波形。當我們在頻譜圖上引入擾動時，將頻譜圖轉換為波形的聲碼器是必要的。我們分別採用 Griffin-lim 算法和基於 WaveRNN 的聲碼器對 Chou 的模型和 AUTOVC 進行攻擊。

4.3. Attack scenarios 攻擊場景

Two scenarios were tested here. 

In the first scenario, the attacker has full access to the model to be attacked.

With the complete architecture plus all trained parameters of the model being available, we can apply adversarial attack directly. 

This scenario is referred to as white-box scenario, in which all experiments were conducted on Chou’s model and AUTOVC with their publicly available network parameters and evaluated on exactly the same models.

The second one is referred to as black-box scenario, in which the attacker cannot directly access the parameters of the model to be attacked, or the architecture might even be unknown. 

For attacking Chou’s model, we trained a new model with the same architecture but different initialization, whereas for AUTOVC we trained a new speaker encoder with the architecture similar to the one in the original AUTOVC. 

These newly-trained models are then used as proxy models to generate adversarial examples to be evaluated with the publicly available ones in the same way as in white-box scenario.

Fig. 2: Speaker verification accuracy for the defended speaker with Chou’s ($\epsilon = 0.075$) and AUTOVC ($\epsilon = 0.05$) by the three proposed approaches under the white-box scenario.

這裡測試了兩個場景。在第一種情況下，攻擊者可以完全訪問要攻擊的模型。有了完整的架構加上模型的所有訓練參數可用，我們可以直接應用對抗性攻擊。

這種場景被稱為白盒場景，其中所有實驗都是在 Chou 的模型和 AUTOVC 上進行的，並使用它們公開可用的網絡參數，並在完全相同的模型上進行評估。第二種被稱為黑盒場景，攻擊者無法直接訪問被攻擊模型的參數，甚至可能架構未知。為了攻擊 Chou 的模型，我們訓練了一個具有相同架構但初始化不同的新模型，而對於 AUTOVC，我們訓練了一個新的揚聲器編碼器，其架構類似於原始 AUTOVC 中的架構。然後將這些新訓練的模型用作代理模型，以生成對抗性示例，以與白盒場景相同的方式與公開可用的示例進行評估。

圖 2：白盒場景下三種建議方法對 Chou ($\epsilon = 0.075$) 和 AUTOVC ($\epsilon = 0.05$) 的防禦說話人的說話人驗證準確度。

4.4. Attack procedure 攻擊過程

We selected $l_2$ norm as $L(·, ·)$, and $\lambda = 0.1$ for all experiments. 

w in (3), (4), (5) was initialized from a standard normal distribution. 

The perturbations to be added to the utterances was $\lambda tanh(w)$. 

Adam optimizer was adopted to update w iteratively according to the loss function defined in (3), (4), (5) with the learning rate being 0.001 and the number of iterations being 1500.

我們選擇 $l_2$ norm 作為 $L(·,·)$，並且 $\lambda = 0.1$ 用於所有實驗。(3)、(4)、(5) 中的 w 從標準正態分佈初始化。要添加到話語中的擾動是$\lambda tanh(w)$。採用 Adam 優化器根據（3）、（4）、（5）中定義的損失函數進行迭代更新，學習率為 0.001，迭代次數為 1500。

5. RESULTS 結果

5.1. Objective tests 客觀測試

For automatic evaluation, we adopted speaker verification accuracy as a reliable metric.

The speaker verification system used here first encoded two input utterances into embeddings and then computed the similarity between the two.

The two utterances were considered to be uttered by the same speaker if the similarity exceeds a threshold. 

In the test, each time we compared a machine-generated utterance with a real utterance providing the speaker characteristics in the experiments (x in Fig. 1), and the speaker verification accuracy used below is defined as the percentage of the cases that the two are considered to be produced by the same speaker by the speaker verification system.

The verification system used here was based on a pretrained d-vector model 1 which was different from the speaker encoders of the two attacked models. 

The threshold was determined based on the equal error rate (EER) when verifying utterance pairs randomly sampled from the VCTK corpus in the following way. 

We sampled 256 utterances for each speaker in the dataset, half of which were used as positive samples and the other half as negative ones. 

For positive samples, the similarity was computed with random utterances of the authentic speaker, whereas the negative ones were computed against random utterances produced by other randomly selected speakers. 

This gave the threshold of 0.683 with the EER being 0.056.

We randomly collected enough number of utterances offering the speaker characteristics (x in Fig. 1) from 109 speakers in the VCTK corpus and enough number of utterances offering the content (t in Fig. 1), and performed voice conversion with both Chou’s and AUTOVC to generate the original outputs (F(t, x) in Fig. 1). 

We randomly collected 100 of such pairs (x, F(t, x)) produced by both Chou’s and AUTOVC which were considered by the speaker verification system mentioned above to be produced by the same speaker to be used for the test below. 

Thus the speaker verification accuracy of all these original outputs (F(t, x)) is 1.00. 

We then created corresponding adversarial examples (x + δ in (3), (4), (5)), targeting speakers with gender opposite to the defended speaker, and performed speaker verification respectively on these adversarial example utterances (referred to as adversarial input) and the converted utterances F(t, x + δ) (referred to as adversarial output). 

The same examples were used in the test for Chou’s and AUTOVC.

對於自動評估採用說話人驗證準確度作為可靠指標。這裡使用的說話人驗證系統首先將兩個輸入話語編碼到嵌入中，然後計算兩者之間的相似性。

如果相似度超過閾值，則認為這兩個話語是由同一說話者發出的。

在測試中，我們每次將機器生成的話語與提供實驗中說話人特徵的真實話語（圖 1 中的 x）進行比較，下面使用的說話人驗證準確度定義為兩者的案例百分比被說話人驗證系統認為是由同一說話人產生的。這裡使用的驗證系統基於預訓練的 d 向量模型 1，該模型與兩個被攻擊模型的說話者編碼器不同。

在驗證從 VCTK 語料庫中隨機採樣的話語對時，閾值是基於等錯誤率 (EER) 確定的，方法如下。從數據集中的每個說話者抽取了 256 個話語，其中一半用作正樣本，另一半用作負樣本。對於正樣本，相似性是用真實說話者的隨機話語計算的，而負樣本是根據其他隨機選擇的說話者產生的隨機話語計算的。

從 VCTK 語料庫中的 109 個說話者中隨機收集了足夠數量的提供說話者特徵的話語（圖 1 中的 x）和足夠數量的提供內容的話語（圖 1 中的 t），並使用 Chou's 和 AUTOVC 進行語音轉換生成原始輸出（圖 1 中的 F(t, x)）。

隨機收集了 100 個這樣的對 (x, F(t, x)) 由 Chou's 和 AUTOVC 生成，上述說話人驗證系統認為它們是由同一說話人生成的，用於下面的測試。

因此，所有這些原始輸出 (F(t, x)) 的說話人驗證準確度為 1.00。然後我們創建了相應的對抗樣本（x + δ in (3), (4), (5)），針對與被辯護人性別相反的說話人，並分別對這些對抗樣本話語進行說話人驗證（稱為對抗性輸入) 和轉換後的話語 F(t, x + δ)（稱為對抗性輸出）。在 Chou 和 AUTOVC 的測試中使用了相同的示例。

Fig. 2(a) shows the speaker verification accuracy for the adversarial input and output utterances evaluated with respect to the defended speaker under the white-box scenario for Chou’s model. 

圖 2（a）顯示了在 Chou 模型的白盒場景下，針對被防禦說話者評估的對抗性輸入和輸出話語的說話者驗證準確性。

Results for the three approaches mentioned in Sec. 3 are in the three sections (i) (ii) (iii), with the blue crosshatch-dotted bar and the red diagonal-lined bar respectively for adversarial input and adversarial output.

Similarly in Fig. 2(b) for AUTOVC. 

Fig. 3: The speaker verification accuracy for different perturbation $\epsilon$ on Chou’s model under black-box scenario for the three proposed approaches: (a) end-to-end, (b) embedding, and (c) feedback attacks.

We can see the adversarial inputs sounded very close to the defended speaker or the perturbation δ almost imperceptible (the blue bars very close to 1.00), while the converted utterances sounded as from a different speaker (the red bars much lower). 

All the three approaches were effective, although feedback attack worked the best for Chou’s (section (iii) in Fig. 2(a)), while embedding attack worked very well for both Chou’s and AUTOVC with respect to both adversarial input and output (section (ii) of each chart).

秒中提到的三種方法的結果。圖 3 位於 (i) (ii) (iii) 三個部分中，藍色交叉陰影線條和紅色對角線條分別用於對抗性輸入和對抗性輸出。AUTOVC 與圖 2(b) 類似。

圖 3：黑盒場景下 Chou 模型上不同擾動 $\epsilon$ 的說話人驗證準確度，用於三種提議的方法：（a）端到端，（b）嵌入和（c）反饋攻擊。

我們可以看到對抗性輸入聽起來非常接近被防禦的說話者，或者擾動 δ 幾乎無法察覺（藍色條非常接近 1.00），而轉換後的話語聽起來像是來自不同的說話者（紅色條要低得多）。這三種方法都是有效的，儘管反饋攻擊對 Chou 的效果最好（圖 2(a) 中的第 (iii) 節），而嵌入攻擊對於 Chou 和 AUTOVC 在對抗性輸入和輸出方面都非常有效（第(ii) 每個圖表）。


For black-box scenario, we analyzed the same speaker verification accuracy as in Fig. 2(a) for Chou’s model only but with varying scale of the perturbations $\epsilon$, with results plotted in Fig. 3 (a) (b) (c) respectively for the three approaches proposed. 

We see when $\epsilon$  = 0.075 the adversarial inputs were kept almost intact (blue curves close to 1.0) while adversarial outputs were seriously disturbed (red curves much lower).

However, as $\epsilon$  ≥ 0.1 the speaker characteristics of the adversarial inputs were altered drastically (blue curves went down), although the adversarial outputs sounded very different (red curves went very low).

對於黑盒場景，我們僅針對 Chou 的模型分析了與圖 2(a) 相同的說話人驗證準確度，但擾動的尺度不同，結果繪製在圖 3 (a) (b) 中 (c) 分別針對所提出的三種方法。我們看到當 $\epsilon$ = 0.075 時，對抗性輸入幾乎保持不變（藍色曲線接近 1.0），而對抗性輸出受到嚴重干擾（紅色曲線低得多）。

然而當 $\epsilon$ ≥ 0.1 時，對抗性輸入的說話人特性發生了巨大變化（藍色曲線下降），儘管對抗性輸出聽起來非常不同（紅色曲線變得非常低）。

Fig. 4 shows the same results as in Fig. 3 except on AUTOVC with embedding attack only (as the other two methods did not work well in white-box scenario in Fig. 2(b)).

We see very similar results as in Fig. 3, and the embedding attack worked successfully with AUTOVC for good choices of (0.05 ≤ $\epsilon$  ≤ 0.075).

Among the three proposed approaches, the embedding attack turned out to be the most attractive, considering both defending effectiveness (as mentioned above) and time efficiency.

The feedback attack offered very good performance on Chou’s model, but less effective on AUTOVC.

It also took more time to apply the perturbation as one more complete encoder-to-decoder inference was required. 

It is interesting to note that the end-to-end attack offered performance comparable to the other two approaches, although it is based on the distance between spectrograms, very different from the distance between speaker embeddings, based on which the other two approaches relied on.

Fig. 4: Same as Fig. 3 except on AUTOVC and for embedding attack only.

圖 4 顯示了與圖 3 相同的結果，除了僅使用嵌入攻擊的 AUTOVC（因為其他兩種方法在圖 2（b）中的白盒場景中效果不佳）。看到與圖 3 非常相似的結果，並且嵌入攻擊成功地與 AUTOVC 一起使用，以獲得 (0.05 ≤ $\epsilon$ ≤ 0.075) 的良好選擇。

在提出的三種方法中，考慮到防禦有效性（如上所述）和時間效率，嵌入攻擊被證明是最具吸引力的。反饋攻擊在 Chou 的模型上提供了非常好的性能，但在 AUTOVC 上效果較差，但由於需要更完整的編碼器到解碼器推斷，因此應用擾動也需要更多時間。

有趣的是，端到端攻擊提供了與其他兩種方法相當的性能，儘管它基於頻譜圖之間的距離，與其他兩種方法所依賴的說話人嵌入之間的距離非常不同.
圖 4：除了 AUTOVC 和僅用於嵌入攻擊外，與圖 3 相同。


5.2. Subjective tests 主觀測試

The above speaker verification tests were objective but not necessarily adequate.

So we performed subjective evaluation here but only with the most attractive embedding attack on both Chou’s and AUTOVC for both white- and black-box scenarios. 

We randomly selected 50 out of the 100 example utterances (x) from the set used in objective evaluation described above. 

The corresponding adversarial inputs (x + δ), outputs (F(t, x + δ)) and the original outputs (F(t, x)) as used above were then reused in subjective evaluation here for = 0.075 and 0.05 respectively for Chou’s and AUTOVC.

上述說話人驗證測試是客觀的，但不一定足夠。因此，我們在這裡進行了主觀評估，但僅對 Chou 和 AUTOVC 進行了最有吸引力的嵌入攻擊，用於白盒和黑盒場景。從上述客觀評估中使用的集合中隨機選擇了 100 個示例話語 (x) 中的 50 個。

上面使用的相應對抗性輸入 (x + δ)、輸出 (F(t, x + δ)) 和原始輸出 (F(t, x)) 然後在主觀評估中重用，分別為 = 0.075 和 0.05 Chou’s 和AUTOVC。

The subjects were then asked to decide if two given utterances were from the same speaker by choosing one out of four: 

然後要求受試者通過從四個中選擇一個來確定兩個給定的話語是否來自同一說話者：

(I) Different, absolutely sure,

(II) Different, but not very sure,

(III) Same, but not very sure, and

(IV) Same, absolutely sure. 

(I) 不同的，絕對肯定的，

(II) 不同，但不是很確定，

(III) 相同，但不太確定，並且

(IV) 一樣，絕對確定。

Among the two utterances given, one is the original utterance x, whereas the other is the adversarial input, adversarial output, or original output. Each utterance pair was evaluated by 6 subjects.

To remove the possible outliers for sub-jective results, we deleted two extreme ballots on both ends out of the 6 ballots received for each utterance pair (delete a (I) if there is one, or delete a (II) if there is no (I)’s, etc.; similar for (IV) and (III)). 

In this way 4 ballots were collected for each utterance pair, and 200 ballots for the 50 utterance pairs.

The percentages of ballots choosing (I), (II), (III), (IV) out of these 200 are then shown in the bars (1) (2) for white-box, (3) (4) for black-box scenarios, and (5) for original output in Fig. 5 for Chou’s and AUTOVC respectively.

For Chou’s, we can see in Fig. 5(a) at least 70% - 78% of the ballots chose (IV) or considered the adversarial inputs preserved the original speaker characteristics very well (red parts in bars  (1)(3), yet at least 41% - 58% of the ballots chose (I) or considered adversarial outputs obviously from a different speaker (blue parts in bars (2)(4). 

Also for the original output at least 82% of the ballots considered them close to the original speaker ((III) plus (IV) in bar (5). 

As in Fig. 5(b) for AUTOVC, at least 85% - 90% of the ballots chose (IV) (red parts in bars (1)(3), yet more than 54% - 68% of the ballots chose (I) (blue parts in bars  (2)(4). 

However, only about 27% of the ballots considered that the original outputs are from the same speaker (red and orange parts in bar (5)). 

This is probably because the objective speaker verification system used here didn’t match human’s perception very well based on which the selected original output with similarity to the original utterance above the threshold may not sound to human subjects as produced by the same speaker. 

Also for both two models, the black-box scenario was in general more challenging than the white-box one (lower green and blue parts, (4) v.s. (2)), but the approach is still effective to a good extent.

在給定的兩個話語中，一個是原始話語 x，而另一個是對抗性輸入、對抗性輸出或原始輸出。每個話語對由 6 名受試者評估。為了消除主觀結果的可能異常值，我們從每個話語對收到的 6 個選票中刪除了兩端的兩個極端選票（如果有則刪除 a (I)，如果沒有則刪除 (II) (I) 等；(IV) 和 (III) 類似）。

以這種方式，為每個話語對收集了 4 張選票，為 50 個話語對收集了 200 張選票。然後從這 200 個中選擇 (I)、(II)、(III)、(IV) 的選票百分比顯示在條形 (1) (2) 中用於白框，(3) (4) 用於黑框-框場景，以及（5）分別用於圖 5 中 Chou 和 AUTOVC 的原始輸出。

對於 Chou，我們可以在圖 5(a) 中看到至少 70% - 78% 的選票選擇 (IV) 或認為對抗性輸入很好地保留了原始說話人的特徵（條形圖 (1)(3) 中的紅色部分） ，但至少 41% - 58% 的選票選擇 (I) 或考慮明顯來自不同說話者的對抗性輸出（條形圖中的藍色部分 (2)(4)。同樣對於原始輸出，至少 82% 的選票認為它們接近原始揚聲器（第（5）條中的（III）加（IV））。

如圖 5(b) 的 AUTOVC 所示，至少 85% - 90% 的選票選擇了 (IV)（條形圖中的紅色部分 (1)(3)，但超過 54% - 68% 的選票選擇了 (IV) I) （條 (2)(4) 中的藍色部分。

然而這只有大約 27% 的選票認為原始輸出來自同一揚聲器(bar (5) 中的紅色和橙色部分)。

這可能是因為這裡使用的客觀說話人驗證系統不能很好地匹配人類的感知，基於此，所選擇的與原始話語相似度高於閾值的原始輸出對於同一說話人，產生的人類受試者來說可能聽起來不像。

同樣對於這兩種模型，黑盒場景通常比白盒場景更具挑戰性（較低的綠色和藍色部分，（4）與（2）），但該方法在很大程度上仍然有效。

The demo samples can be found at https://yistlin.github.io/attack-vc-demo, and the source code is at https://github.com/cyhuang-tw/attack-vc.