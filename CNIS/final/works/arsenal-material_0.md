# CNIS - 密码编码学与网络信息安全 Cryptography and Network Information Security

> 2101212850 干皓丞

PKU 2022 個人實驗報告作業

## Details

Arsenal 中文素材

## Work

### 問題

1. 存在各式各样的换脸技术，这对检测技术的通用性会带来挑战。

2. 对视频的后期处理会严重影响检测技术的效果。

- 如果需要给执法单位参考，光是给出一个 binary 分类结果可能不够，需要结合一些可解释性，比如 换脸痕迹的定位 等等
使用传统 Forensics 的方式也能做，但是检测性能较差，同时不够robust。

- 該問題很复杂，其本身就是一项需要投入到实际应用中的技术，这个问题具有很大的挑戰。但是做研究没办法一次就把所有问题解决，可以找到一个点进行深度研究，能解决一个大问题中的一个小问题即可。綜上这两个问题在学术界以及工业界并没有得到很好的解决办法，所以可以重点关注。

### 工作目標

一、了解研究领域的现状

1. 阅读 2 - 3 篇 Survey

2. CVPR, ICCV, ECCV 几个会议有很多不错的工作，可以进行调研并阅读学习，至少阅读 25 以上现有论文。

二、找到切入点

1. 对他人的方法进行归纳总结，看看哪些工作是有源代码，并且工作是有缺陷的，可以在他人的工作之上进行改进。

2. 定位问题。

### 想法

在若想使自己对照片进行加躁，使攻击者的生成无法顺利生成，又或者是对自己伪造后的影像进行干扰使他人的检测无法顺利执行 !??


## 1. M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection

Wang, J., Wu, Z., Chen, J., & Jiang, Y. G. (2021). M2tr: Multi-modal multi-scale transformers for deepfake detection. arXiv preprint arXiv:2104.09770.

https://arxiv.org/abs/2104.09770

```
The widespread dissemination of forged images generated by Deepfake techniques has posed a serious threat to the trustworthiness of digital information. 

This demands effective approaches that can detect perceptually convincing Deepfakes generated by advanced manipulation techniques. 

Most existing approaches combat Deepfakes with deep neural networks by mapping the input image to a binary prediction without capturing the consistency among different pixels. 

In this paper, we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection. 

We achieve this with transformer models, which have recently demonstrated superior performance in modeling dependencies between pixels for a variety of recognition tasks in computer vision. 

In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale transformer that operates on patches of different sizes to detect the local inconsistency at different spatial levels. 

To improve the detection results and enhance the robustness of our method to image compression, M2TR also takes frequency information, which is further combined with RGB features using a cross modality fusion module. 

Developing and evaluating Deepfake detection methods requires large-scale datasets. However, we observe that samples in existing benchmarks contain severe artifacts and lack diversity. 

This motivates us to introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. 

On three Deepfake datasets, we conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods.
```

Deepfake 技術所產生的偽造圖像廣泛傳播對數位資訊的可信度構成了嚴重威脅，這需要有效的方法來檢測由先進技術所生成具有感知力的 Deepfake 成果。大多數現有方法通過將輸入圖像對應到二進制預測而不捕獲不同像素之間的一致性來使用深度神經網絡來對抗 Deepfakes 技術。在該研究中，研究者旨在為 Deepfake 檢測捕獲不同尺度的細微操作偽影，並通過轉換器模型實現了這一點，該模型最近在為計算機視覺中的各種識別任務建模像素之間的依賴關係方面表現出卓越的性能。同時研究者介紹了一種多模態多尺度變換器（M2TR），它使用多尺度變換器對不同大小的補丁進行操作，以檢測不同空間級別的局部不一致性，為了改善檢測結果並增強我們方法對圖像壓縮的魯棒性，M2TR 還獲取頻率信息，並使用交叉模態融合模塊將其與 RGB 特徵進一步結合。開發和評估 Deepfake 檢測方法需要大規模的數據集。此研究觀察到現有基準中的樣本包含嚴重的偽影並且缺乏多樣性，這促使此研究引入了一個高品質的 Deepfake 資料集 SR-DF，它由 4,000 個由最先進的面部交換和面部重演方法去生成的 DeepFake 組成影像，最後在三個 Deepfake 資料集上，研究者進行了廣泛的實驗來驗證所提出方法的有效性，該方法優於最先進的 Deepfake 檢測方法。

Bibliography

```
@article{wang2021m2tr,
  title={M2tr: Multi-modal multi-scale transformers for deepfake detection},
  author={Wang, Junke and Wu, Zuxuan and Chen, Jingjing and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2104.09770},
  year={2021}
}
```

## 2. Defending Your Voice: Adversarial Attack on Voice Conversion

https://arxiv.org/abs/2005.08781

Huang, C. Y., Lin, Y. Y., Lee, H. Y., & Lee, L. S. (2021, January). Defending your voice: Adversarial attack on voice conversion. In 2021 IEEE Spoken Language Technology Workshop (SLT) (pp. 552-559). IEEE.

```
Substantial improvements have been achieved in recent years in voice conversion, which converts the speaker characteristics of an utterance into those of another speaker without changing the linguistic content of the utterance. 

Nonetheless, the improved conversion technologies also led to concerns about privacy and authentication.

It thus becomes highly desired to be able to prevent one’s voice from being improperly utilized with such voice conversion technologies.

This is why we report in this paper the first known attempt to perform adversarial attack on voice conversion.

We introduce human imperceptible noise into the utterances of a speaker whose voice is to be defended. 

Given these adversarial examples, voice conversion models cannot convert other utterances so a to sound like being produced by the defended speaker. 

Preliminary experiments were conducted on two currently stateof-the-art zero-shot voice conversion models.

Objective and subjective evaluation results in both white-box and black-box scenarios are reported.

It was shown that the speaker characteristics of the converted utterances were made obviously different from those of the defended speaker, while the adversarial examples of the defended speaker are not distinguishable from the authentic utterances.
```

近年來在語音轉換方面取得了實質性的改進，將一個話語的說話者特徵轉換為另一個說話者的特徵，而不改變話語的語言內容。儘管如此，改進的轉換技術也導致了對隱私和身份驗證的擔憂。因此，非常希望能夠通過這種語音轉換技術來防止一個人的語音被不當使用。這就是為什麼該研究報告了對語音轉換執行對抗性攻擊的第一次已知嘗試，研究者將人類難以察覺的噪音引入說話人的話語中，而說話人的聲音要受到保護。鑑於這些對抗性示例，語音轉換模型無法將其他話語轉換為聽起來像是被防御者發出的聲音。在兩個當前最先進的零樣本語音轉換模型上進行了初步實驗。報告了白盒和黑盒場景中的客觀和主觀評估結果。結果表明，轉換後的話語的說話人特徵與被辯護人的說話人特徵明顯不同，而被辯護人的對抗樣本與真實話語沒有區別。

Index Terms:  voice conversion, adversarial attack, speaker verification, speaker representation

```
@INPROCEEDINGS{9383529,
  author={Huang, Chien-yu and Lin, Yist Y. and Lee, Hung-yi and Lee, Lin-shan},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Defending Your Voice: Adversarial Attack on Voice Conversion}, 
  year={2021},
  volume={},
  number={},
  pages={552-559},
  doi={10.1109/SLT48900.2021.9383529}}
```

## 3. Video Transformer for Deepfake Detection with Incremental Learning

Khan, S. A., & Dai, H. (2021, October). Video Transformer for Deepfake Detection with Incremental Learning. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 1821-1828).

https://dl.acm.org/doi/abs/10.1145/3474085.3475332?sid=SCITRUS

https://arxiv.org/abs/2108.05307

```
Face forgery by deepfake is widely spread over the internet and this raises severe societal concerns. 

In this paper, we propose a novel video transformer with incremental learning for detecting deepfake videos. 

To better align the input face images, we use a 3D face reconstruction method to generate UV texture from a single input face image. 

The aligned face image can also provide pose, eyes blink and mouth movement information that cannot be perceived in the UV texture image, so we use both face images and their UV texture maps to extract the image features. 

We present an incremental learning strategy to fine-tune the proposed model on a smaller amount of data and achieve better deepfake detection performance. 

The comprehensive experiments on various public deepfake datasets demonstrate that the proposed video transformer model with incremental learning achieves state-of-the-art performance in the deepfake video detection task with enhanced feature learning from the sequenced data.
```

Deepfake 的面部偽造在互聯網上廣泛傳播，這引起了嚴重的社會擔憂，在該研究中，研究者提出了一種具有增量學習功能的新型影像轉換器，用於檢測深度偽造影像，而為了更好地對齊輸入人臉圖像，我們使用 3D 人臉重建方法從單個輸入人臉圖像生成 UV 紋理。其對齊的人臉圖像還可以提供在 UV 紋理圖像中無法感知的姿勢、眨眼和嘴巴運動資訊，因此研究者使用人臉圖像及其 UV 紋理圖來提取圖像特徵。最後研究者提出了一種增量學習策略，可以在更少量的資料上微調所提出的模型，並實現更好的深度偽造檢測性能。對各種公共 deepfake 數據集的綜合實驗表明，所提出的具有增量學習的視頻轉換器模型通過對序列數據的增強特徵學習，在 deepfake 視頻檢測任務中實現了最先進的性能。

Bibliography

```
@inproceedings{khan2021video,
  title={Video Transformer for Deepfake Detection with Incremental Learning},
  author={Khan, Sohail Ahmed and Dai, Hang},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={1821--1828},
  year={2021}
}
```

## 4. Detecting spoofing attacks using VGG and SincNet: BUT-Omilia submission to ASVspoof 2019 challenge

Zeinali H, Stafylakis T, Athanasopoulou G, Rohdin J, Gkinis I, Burget L, Cernocky JH. Detecting spoofing attacks using VGG and SincNet: BUT-Omilia submission to ASVspoof 2019 challenge. In: Proc. of the 20th Annual Conf. of the Int’l Speech Communication Association. 2019. 1073−1077.

Zeinali, H., Stafylakis, T., Athanasopoulou, G., Rohdin, J., Gkinis, I., Burget, L., & Černocký, J. (2019). Detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge. arXiv preprint arXiv:1907.12908.

Link : https://arxiv.org/abs/1907.12908

Note : 應用不同架構來應對攻擊

Tag : CVPR

```
In this paper, we present the system description of the joint efforts of Brno University of Technology (BUT) and Omilia - Conversational Intelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge.

The primary submission for Physical access (PA) is a fusion of two VGG networks, trained on single and two-channels features.

For Logical access (LA), our primary system is a fusion of VGG and the recently introduced SincNet architecture.

The results on PA show that the proposed networks yield very competitive performance in all conditions and achieved 86\:\% relative improvement compared to the official baseline.

On the other hand, the results on LA showed that although the proposed architecture and training strategy performs very well on certain spoofing attacks, it fails to generalize to certain attacks that are unseen during training.

```

該研究介紹了布爾諾理工大學 (BUT) 和 Omilia 共同努力的系統描述 — ASVSpoof2019 Spoofing and Countermeasures Challenge 的對話智能，其物理訪問（PA）的主要提交是兩個 VGG 網絡的融合，並在單通道和雙通道特徵上進行了訓練。對於邏輯訪問 (LA)，研究者的主要系統是 VGG 和最近引入的 SincNet 架構的融合，其 PA 上的結果表明，所提出的網絡在所有條件下都產生了非常有競爭力的性能，並且與官方基線相比實現了 86\:\% 的相對改進。另一方面，LA 上的結果表明，儘管所提出的架構和訓練策略在某些欺騙攻擊上表現得非常好，但它無法推廣到在訓練期間看不見的某些攻擊下。

Bibliography

```
@article{zeinali2019detecting,
  title={Detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge},
  author={Zeinali, Hossein and Stafylakis, Themos and Athanasopoulou, Georgia and Rohdin, Johan and Gkinis, Ioannis and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and others},
  journal={arXiv preprint arXiv:1907.12908},
  year={2019}
}
```

## 5. Attacks on state-of-the-art face recognition using attentional adversarial attack generative network

Song Q, Wu Y, Yang L. Attacks on state-of-the-art face recognition using attentional adversarial attack generative network. arXiv preprint arXiv:1811.12026, 2018.

Yang, L., Song, Q., & Wu, Y. (2021). Attacks on state-of-the-art face recognition using attentional adversarial attack generative network. Multimedia Tools and Applications, 80(1), 855-875.

Link : https://arxiv.org/abs/1811.12026

Note : 使用注意力机制和生成对抗网络生成指定语义信息的假人脸,使得人脸识别器误判

```
With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked.

So, it is important to study how face recognition networks are subject to attacks.

In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously.

Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images.

For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces.

Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better.

The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.
```

隨著人臉識別的廣泛應用，它的弱點也逐漸暴露出來，即容易被攻擊。因此研究人臉識別網絡如何受到攻擊非常重要。在研究中，研究者專注於一種對人臉識別網絡進行攻擊的新穎方法，該方法會誤導網絡將某人識別為目標人，而不是不明顯地錯誤分類，同時，因為此緣故，研究者引入了一個特定的注意力對抗攻擊生成網絡來生成假人臉圖像。為了捕獲目標人的語義信息，這項工作添加了條件變分自動編碼器和注意模塊來學習人臉之間的實例級對應關係。與傳統的雙人 GAN 不同，這項工作引入了人臉識別網絡作為第三個參與者參與生成器和判別器之間的競爭，這使得攻擊者可以更好地模仿目標人。生成的結果難以引起旁觀者註意的人臉可以逃避最先進網絡的識別，並且大多數人都被識別為目標人。

Bibliography

```
@article{yang2021attacks,
  title={Attacks on state-of-the-art face recognition using attentional adversarial attack generative network},
  author={Yang, Lu and Song, Qing and Wu, Yingqi},
  journal={Multimedia Tools and Applications},
  volume={80},
  number={1},
  pages={855--875},
  year={2021},
  publisher={Springer}
}
```

## 6. Detection of GAN-generated fake images over social networks

Marra F, Gragnaniello D, Cozzolino D, Verdoliva L. Detection of GAN-generated fake images over social networks. In: Proc. of the IEEE Conf. on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2018. 384−389.

Marra, F., Gragnaniello, D., Cozzolino, D., & Verdoliva, L. (2018, April). Detection of gan-generated fake images over social networks. In 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR) (pp. 384-389). IEEE.

Link : https://ieeexplore.ieee.org/document/8397040

Note : 模拟了篡改图片在社交网络的场景中的检测,结果显示,现有的检测器在现实网络对抗环境下(未知压缩和未知类型等)表现很差

```
The diffusion of fake images and videos on social networks is a fast growing problem.

Commercial media editing tools allow anyone to remove, add, or clone people and objects, to generate fake images.

Many techniques have been proposed to detect such conventional fakes, but new attacks emerge by the day.

Image-to-image translation, based on generative adversarial networks (GANs), appears as one of the most dangerous, as it allows one to modify context and semantics of images in a very realistic way.

In this paper, we study the performance of several image forgery detectors against image-to-image translation, both in ideal conditions, and in the presence of compression, routinely performed upon uploading on social networks.

The study, carried out on a dataset of 36302 images, shows that detection accuracies up to 95% can be achieved by both conventional and deep learning detectors, but only the latter keep providing a high accuracy, up to 89%, on compressed data.
```

社交網絡上虛假圖像和視頻的傳播是一個快速增長的問題，商業媒體編輯工具允許任何人刪除、添加或克隆人和對象，以生成虛假圖像。已經提出了許多技術來檢測這種傳統的偽造品，但新的攻擊每天都在出現。基於生成對抗網絡 (GAN) 的圖像到圖像轉換似乎是最危險的方法之一，因為它允許人們以非常現實的方式修改圖像的上下文和語義。在該研究中，研究者們研究了幾種圖像偽造檢測器對圖像到圖像轉換的性能，無論是在理想條件下，還是在存在壓縮的情況下，通常在上傳到社交網絡時執行。該研究在 36302 張圖像的數據集上進行，表明傳統和深度學習檢測器都可以實現高達 95% 的檢測精度，但只有後者在壓縮數據上保持高達 89% 的高精度。

Bibliography

```
@inproceedings{marra2018detection,
  title={Detection of gan-generated fake images over social networks},
  author={Marra, Francesco and Gragnaniello, Diego and Cozzolino, Davide and Verdoliva, Luisa},
  booktitle={2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
  pages={384--389},
  year={2018},
  organization={IEEE}
}
```

## 7. Combating deepfake videos using blockchain and smart contracts

Hasan HR, Salah K. Combating deepfake videos using blockchain and smart contracts. IEEE Access, 2019,7:41596−41606.

Hasan, H. R., & Salah, K. (2019). Combating deepfake videos using blockchain and smart contracts. Ieee Access, 7, 41596-41606.

Link : https://ieeexplore.ieee.org/document/8668407

Note : 尝试用区块链技术对互联网上的视频进行追踪

```
With the rise of artificial intelligence (AI) and deep learning techniques, fake digital contents have proliferated in recent years.

Fake footage, images, audios, and videos (known as deepfakes) can be a scary and dangerous phenomenon and can have the potential of altering the truth and eroding trust by giving false reality.

Proof of authenticity (PoA) of digital media is critical to help eradicate the epidemic of forged content.

Current solutions lack the ability to provide history tracking and provenance of digital media.

In this paper, we provide a solution and a general framework using Ethereum smart contracts to trace and track the provenance and history of digital content to its original source even if the digital content is copied multiple times.

The smart contract utilizes the hashes of the interplanetary file system (IPFS) used to store digital content and its metadata.

Our solution focuses on video content, but the solution framework provided in this paper is generic enough and can be applied to any other form of digital content.

Our solution relies on the principle that if the content can be credibly traced to a trusted or reputable source, the content can then be real and authentic.

The full code of the smart contract has been made publicly available at Github.
```

隨著人工智能 (AI) 和深度學習技術的興起，近年來虛假數字內容激增。假鏡頭、圖像、音頻和視頻（稱為 deepfakes）可能是一種可怕且危險的現象，並且有可能通過提供虛假現實來改變真相並削弱信任，數位媒體的真實性證明 (PoA) 對於幫助消除偽造內容的流行至關重要，而當前的解決方案缺乏提供數字媒體的歷史跟踪和出處的能力。

在該研究中，研究者們提供了一個解決方案和一個通用框架，使用以太坊智能合約來追踪和跟踪數字內容的出處和歷史到其原始來源，即使數字內容被複製多次，其智能合約利用用於存儲數字內容及其元數據的行星際文件系統 (IPFS) 的哈希值。解決方案專注於視頻內容，且此研究提供的解決方案框架足夠通用，可以應用於任何其他形式的數位內容。其解決方案依賴於這樣一個原則，即如果內容可以可靠地追溯到可信或有信譽的來源，那麼內容就可以是真實的和真實的。

Bibliography

```
@article{hasan2019combating,
  title={Combating deepfake videos using blockchain and smart contracts},
  author={Hasan, Haya R and Salah, Khaled},
  journal={Ieee Access},
  volume={7},
  pages={41596--41606},
  year={2019},
  publisher={IEEE}
}
```