# DMSASD - æ•°å­—åª’ä½“è½¯ä»¶ä¸ç³»ç»Ÿå¼€å‘ - Digital Media Software And System Development

> 2101212850 å¹²çš“ä¸

PKU 2022 å€‹äººå¯¦é©—å ±å‘Šä½œæ¥­

## Details

M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection

Wang, J., Wu, Z., Chen, J., & Jiang, Y. G. (2021). M2tr: Multi-modal multi-scale transformers for deepfake detection. arXiv preprint arXiv:2104.09770.

https://arxiv.org/abs/2104.09770

```
The widespread dissemination of forged images generated by Deepfake techniques has posed a serious threat to the trustworthiness of digital information. 

This demands effective approaches that can detect perceptually convincing Deepfakes generated by advanced manipulation techniques. 

Most existing approaches combat Deepfakes with deep neural networks by mapping the input image to a binary prediction without capturing the consistency among different pixels. 

In this paper, we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection. 

We achieve this with transformer models, which have recently demonstrated superior performance in modeling dependencies between pixels for a variety of recognition tasks in computer vision. 

In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale transformer that operates on patches of different sizes to detect the local inconsistency at different spatial levels. 

To improve the detection results and enhance the robustness of our method to image compression, M2TR also takes frequency information, which is further combined with RGB features using a cross modality fusion module. 

Developing and evaluating Deepfake detection methods requires large-scale datasets. However, we observe that samples in existing benchmarks contain severe artifacts and lack diversity. 

This motivates us to introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. 

On three Deepfake datasets, we conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods.
```

Deepfake æŠ€è¡“æ‰€ç”¢ç”Ÿçš„å½é€ åœ–åƒå»£æ³›å‚³æ’­å°æ•¸ä½è³‡è¨Šçš„å¯ä¿¡åº¦æ§‹æˆäº†åš´é‡å¨è„…ï¼Œé€™éœ€è¦æœ‰æ•ˆçš„æ–¹æ³•ä¾†æª¢æ¸¬ç”±å…ˆé€²æŠ€è¡“æ‰€ç”Ÿæˆå…·æœ‰æ„ŸçŸ¥åŠ›çš„ Deepfake æˆæœã€‚å¤§å¤šæ•¸ç¾æœ‰æ–¹æ³•é€šéå°‡è¼¸å…¥åœ–åƒå°æ‡‰åˆ°äºŒé€²åˆ¶é æ¸¬è€Œä¸æ•ç²ä¸åŒåƒç´ ä¹‹é–“çš„ä¸€è‡´æ€§ä¾†ä½¿ç”¨æ·±åº¦ç¥ç¶“ç¶²çµ¡ä¾†å°æŠ— Deepfakes æŠ€è¡“ã€‚åœ¨è©²ç ”ç©¶ä¸­ï¼Œç ”ç©¶è€…æ—¨åœ¨ç‚º Deepfake æª¢æ¸¬æ•ç²ä¸åŒå°ºåº¦çš„ç´°å¾®æ“ä½œå½å½±ï¼Œä¸¦é€šéè½‰æ›å™¨æ¨¡å‹å¯¦ç¾äº†é€™ä¸€é»ï¼Œè©²æ¨¡å‹æœ€è¿‘åœ¨ç‚ºè¨ˆç®—æ©Ÿè¦–è¦ºä¸­çš„å„ç¨®è­˜åˆ¥ä»»å‹™å»ºæ¨¡åƒç´ ä¹‹é–“çš„ä¾è³´é—œä¿‚æ–¹é¢è¡¨ç¾å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åŒæ™‚ç ”ç©¶è€…ä»‹ç´¹äº†ä¸€ç¨®å¤šæ¨¡æ…‹å¤šå°ºåº¦è®Šæ›å™¨ï¼ˆM2TRï¼‰ï¼Œå®ƒä½¿ç”¨å¤šå°ºåº¦è®Šæ›å™¨å°ä¸åŒå¤§å°çš„è£œä¸é€²è¡Œæ“ä½œï¼Œä»¥æª¢æ¸¬ä¸åŒç©ºé–“ç´šåˆ¥çš„å±€éƒ¨ä¸ä¸€è‡´æ€§ï¼Œç‚ºäº†æ”¹å–„æª¢æ¸¬çµæœä¸¦å¢å¼·æˆ‘å€‘æ–¹æ³•å°åœ–åƒå£“ç¸®çš„é­¯æ£’æ€§ï¼ŒM2TR é‚„ç²å–é »ç‡ä¿¡æ¯ï¼Œä¸¦ä½¿ç”¨äº¤å‰æ¨¡æ…‹èåˆæ¨¡å¡Šå°‡å…¶èˆ‡ RGB ç‰¹å¾µé€²ä¸€æ­¥çµåˆã€‚é–‹ç™¼å’Œè©•ä¼° Deepfake æª¢æ¸¬æ–¹æ³•éœ€è¦å¤§è¦æ¨¡çš„æ•¸æ“šé›†ã€‚æ­¤ç ”ç©¶è§€å¯Ÿåˆ°ç¾æœ‰åŸºæº–ä¸­çš„æ¨£æœ¬åŒ…å«åš´é‡çš„å½å½±ä¸¦ä¸”ç¼ºä¹å¤šæ¨£æ€§ï¼Œé€™ä¿ƒä½¿æ­¤ç ”ç©¶å¼•å…¥äº†ä¸€å€‹é«˜å“è³ªçš„ Deepfake è³‡æ–™é›† SR-DFï¼Œå®ƒç”± 4,000 å€‹ç”±æœ€å…ˆé€²çš„é¢éƒ¨äº¤æ›å’Œé¢éƒ¨é‡æ¼”æ–¹æ³•å»ç”Ÿæˆçš„ DeepFake çµ„æˆå½±åƒï¼Œæœ€å¾Œåœ¨ä¸‰å€‹ Deepfake è³‡æ–™é›†ä¸Šï¼Œç ”ç©¶è€…é€²è¡Œäº†å»£æ³›çš„å¯¦é©—ä¾†é©—è­‰æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè©²æ–¹æ³•å„ªæ–¼æœ€å…ˆé€²çš„ Deepfake æª¢æ¸¬æ–¹æ³•ã€‚

Bibliography

```
@article{wang2021m2tr,
  title={M2tr: Multi-modal multi-scale transformers for deepfake detection},
  author={Wang, Junke and Wu, Zuxuan and Chen, Jingjing and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2104.09770},
  year={2021}
}
```

## Note

æ­¤å¤–è©²ç ”ç©¶æå‡ºäº†ä¸€ç¨®åç‚º M2TR çš„å¤šæ¨¡æ…‹å¤šå°ºåº¦è®Šå£“å™¨ ï¼Œé¦–å…ˆé‹ç”¨ CNN æ¨¡å‹æå–ç‰¹å¾µï¼Œç„¶å¾Œç”Ÿæˆä½œç‚º Transformer æ¨¡å‹çš„è¼¸å…¥ï¼Œç”¨æ–¼æ•æ‰ä¸åŒå€åŸŸåœ¨å¤šå°ºåº¦ä¸Šçš„ä¸ä¸€è‡´æ€§ï¼Œä¸¦åœ¨æ­¤åŸºç¤ä¸Šå¢åŠ äº†é »åŸŸä¿¡æ¯ï¼Œèåˆå¾Œè¼¸å‡ºçµæœã€‚

0. DCT éƒ¨åˆ†

DCT ä¹Ÿå°±æ˜¯é›¢æ•£é¤˜å¼¦è®Šæ›çš„ç¸®å¯«ï¼Œå°‡åœ–åƒå¾ç©ºåŸŸè½‰åŒ–åˆ°é »åŸŸï¼Œå…¶ç ”ç©¶è€…èªç‚ºï¼Œåœ–åƒçš„ä½é »ä¿¡æ¯é›†ä¸­åœ¨ 0-1/16 éƒ¨åˆ†ï¼Œä¸­é »ä¿¡æ¯é›†ä¸­åœ¨ 1/16-1/8ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†æ˜¯åœ–åƒçš„é«˜é »ä¿¡æ¯ï¼Œé€šéæ§‹é€ ä¸‰å€‹æ¿¾æ³¢å™¨ä¾†å®Œæˆæå–ä½ä¸­é«˜é »ä¿¡æ¯ã€‚é€šéç¶“é DCT è®Šæ›å¾Œçš„åœ–åƒä¸­é»‘è‰²çš„éƒ¨åˆ†è€Œæ¨æ£„ç™½è‰²çš„éƒ¨åˆ†ï¼Œå¾è€Œå¾—åˆ°æå–ä½é »ï¼Œä¸­é »ï¼Œé«˜é »ä¿¡æ¯ã€‚ä¸¦ç”¨å°‡å¾—åˆ°çš„ä¿¡æ¯é€²è¡Œé€†è®Šæ›ï¼Œå°‡ä½ä¸­é«˜é »ä¿¡æ¯çµ„åˆåœ¨ä¸€èµ·ï¼Œä½œç‚ºé »åŸŸä¿¡æ¯çš„è¼¸å…¥ï¼Œé€å…¥å·ç©ç¶²çµ¡ä¸­ï¼Œå°‡å…¶è¼¸å‡ºä½œç‚ºæå–åˆ°çš„é »åŸŸç‰¹å¾µã€‚


1. Multi-scale Transformer

å¸Œæœ›å®šä½ç¯¡æ”¹å½å½±èˆ‡å…¶ä»–å€åŸŸä¸ä¸€è‡´ï¼Œå› æ­¤éœ€è¦å»ºæ¨¡é•·æœŸé—œä¿‚ï¼Œè¨ˆç®—ç›¸ä¼¼åº¦ï¼Œå¼•å…¥å¤šå°ºåº¦çš„ Transformerï¼Œä¾†è¦†è“‹ä¸åŒå¤§å°çš„å€åŸŸã€‚


è¼¸å…¥åœ–ç‰‡å† backbone æå– shallow featureï¼Œç„¶å¾Œåˆ†æˆä¸åŒçš„å¤§å°å»è¨ˆç®— patch-wise çš„ self attentionï¼Œä¹Ÿå°±æ˜¯æ¯å€‹ Patchï¼ˆrh*rh*cï¼‰å±•é–‹æˆä¸€ç¶­å‘é‡ï¼Œä½¿ç”¨ FC å±¤ embed åˆ° Query Embeddingsï¼ŒåŒæ¨£å¾—åˆ° k å’Œ vï¼Œæœ€å¾Œé€šéçŸ©é™£ç›¸ä¹˜å¾—åˆ°ç›¸é€Ÿåº¦æœ€å¾Œé€šé Softmax è¼¸å‡ºã€‚

$$
\alpha_{i, j}^{h}=\operatorname{softmax}\left(\frac{\boldsymbol{q}_{i}^{h} \cdot\left(\boldsymbol{k}_{j}^{h}\right)^{T}}{\sqrt{r_{h} \times r_{h} \times C}}\right), 1 \leq i, j \leq N
$$

æœ€å¾Œç›¸ä¹˜ç›¸åŠ å¾—åˆ°æŸ¥è©¢ Patch çš„è¼¸å‡ºï¼š

$$
\boldsymbol{o}_{i}^{h}=\sum_{j=1}^{N} \alpha_{i, j}^{h} v_{j}^{h}
$$

æ¥æ”¶æ‰€æœ‰è¼¸å‡º Sitch and Reshape åŸæœ¬çš„ Resolutionï¼Œä¸åŒçš„é ­æ‹¼æ¥é€šé Residual Block å¾—åˆ°è¼¸å‡ºçµæœã€‚

$$
f_{m t} \in \mathbb{R}(H / 4) \times(W / 4) \times C
$$

é€™ä¸€éƒ¨åˆ†èˆ‡è¦–è¦ºæ³¨æ„åŠ›æ©Ÿåˆ¶é¡ä¼¼ï¼Œå”¯ä¸€çš„å€åˆ¥æ˜¯ï¼Œé€™ä¸€éƒ¨åˆ†ä½¿ç”¨å¤šç¨®ä¸åŒå°ºåº¦çš„ Patch å°åœ–åƒé€²è¡Œæ¡æ¨£ã€‚

èˆ‰å€‹ä¾‹å­ï¼Œå‡è¨­ä½¿ç”¨çš„æ˜¯åˆ†è¾¨ç‡ç‚º 56 * 56 çš„åœ–åƒï¼Œä¸€èˆ¬çš„è¦–è¦ºæ³¨æ„åŠ›æ©Ÿåˆ¶æœƒä½¿ç”¨ 14 * 14 çš„ Patch å°åœ–åƒé€²è¡Œæ¡æ¨£ï¼Œæ¡æ¨£å¾Œå°‡åœ–ç‰‡åˆ†ç‚º 4 * 4 ä¹Ÿå°±æ˜¯ 16 å€‹å¡Šï¼Œéš¨å¾Œå†å°é€™16å€‹å¡Šé€²è¡Œç·¨ç¢¼ï¼Œå¾—åˆ° 16 å€‹ tokenï¼Œæœ€å¾Œä½¿ç”¨è‡ªè¨»æ„åŠ›æ©Ÿåˆ¶ï¼Œé€šéæ¯å€‹ä¸²å¾—åˆ° qkvï¼Œæœ€å¾Œå¾—åˆ°è¼¸å‡ºçµæœã€‚

è€Œæ‰€è¬‚çš„å¤šå°ºåº¦æ˜¯å°‡é€™ä¸€éç¨‹é‡è¤‡æ•¸æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨çš„ Patch å¤§å°ä¸åŒï¼Œç¬¬ä¸€æ¬¡æˆ‘å€‘ä½¿ç”¨èˆ‡åœ–åƒç›¸åŒå¤§å°çš„ Patchï¼Œæ¡ç”¨å¾Œå¾—åˆ° 1 å€‹å¡Šã€‚ç¬¬äºŒæ¬¡ä½¿ç”¨ 28 * 28 å¤§å°çš„ Patchï¼Œå¾—åˆ° 2 * 2 ä¹Ÿå°±æ˜¯ 4 å€‹å¡Šã€‚ç¬¬ä¸‰æ¬¡ä½¿ç”¨ 14 * 14 å¤§å°çš„ Patchï¼Œå¾—åˆ° 4 * 4 = 16 å€‹å¡Šï¼Œæœ€å¾Œä½¿ç”¨ 7 * 7 å¤§å°çš„ Patchï¼Œå¾—åˆ° 8 * 8 = 64 å€‹å¡Šã€‚æ¯ä¸€å€‹å°ºåº¦éƒ½æœƒå†å„è‡ªå°ºåº¦åˆ†åˆ¥åˆ©ç”¨è‡ªè¨»æ„åŠ›æ©Ÿåˆ¶é€²è¡Œè¨ˆç®—ï¼Œå¾—åˆ°å„è‡ªå°ºåº¦çš„è¼¸å‡ºçµæœï¼Œæœ€å¾Œå°‡å„å€‹è¼¸å‡ºçµåˆå†ä¸€èµ·ï¼Œé€å…¥å·ç©ä¸­ï¼Œå¾—åˆ° Transformer éƒ¨åˆ†çš„ç‰¹å¾µæå–ã€‚


å¦å¤–å†è©³ç´°èªªæ˜å–®å€‹å¤šé ­è‡ªè¨»æ„åŠ› (Mutil-head Self Attention, MSA) çš„æ“ä½œéç¨‹ã€‚

åœ¨ VIT ä¸­ï¼Œé¦–å…ˆå°‡åœ–ç‰‡æŒ‰ç…§ä¸€å®šçš„å°ºå¯¸é€²è¡Œåˆ†å‰²ï¼Œè®Šæˆä¸€å€‹å€‹ Tokenï¼Œèˆ‰ä¾‹ä¾†èªªï¼Œç•¶è¼¸å…¥åœ–ç‰‡æ˜¯ 224 * 224ï¼Œ è€Œ Patch å¤§å°ç‚º 16 * 16ï¼Œå°±æœƒå¾—åˆ° 14 * 14 ä¹Ÿå°±æ˜¯ 196 å€‹å¡Šã€‚éš¨å¾Œå°‡æ¯å€‹å¡Šæ”¾å…¥ embed æ¨¡å¡Šï¼Œæ˜ å°„æˆ 196 å€‹ Tokenï¼Œæ¯å€‹ Token çš„é•·åº¦è¢«è¦å®šç‚º 768(14 * 14 * 3)ã€‚é€™æ™‚åœ–ç‰‡å¾ [b, 3, 224, 224] è®Šæˆäº† [b, 196, 768]ã€‚ç„¶å¾Œæ­¤æ™‚éœ€è¦å¢åŠ ä¸€å€‹ cls å’Œä¸€å€‹ä½ç½®ä¿¡æ¯ posï¼Œæ‰€è¬‚çš„ cls æ˜¯ä¸€å€‹ç‰¹æ®Šå­—ç¬¦ï¼Œå®ƒçš„å…·é«”ä½œç”¨æ˜¯ç”¨æ–¼åˆ†é¡ï¼Œåœ¨ nlp ä¸­ cls é€šå¸¸ä½æ–¼ç¬¬ä¸€ä½ï¼Œä½ç½®ä¿¡æ¯è¡¨ç¤ºæ¯ä¸€å€‹ Token çš„ä½ç½®ï¼Œç”±æ–¼æ”¹è®Š Token çš„ä½ç½®æœƒå°‡æ•´å¼µåœ–ç‰‡çš„èªç¾©ä¿¡æ¯æ”¹è®Šï¼Œæ‰€ä»¥æ·»åŠ ä½ç½®ä¿¡æ¯æ˜¯å¿…è¦çš„ã€‚ cls çš„å¤§å°æ˜¯ [b, 1, 768], ä½ç½®ä¿¡æ¯çš„å¤§å°æ˜¯ [ b, 197, 768]ï¼Œè‡³æ–¼æ˜¯ç”¨ 197 çš„åŸå› å‘¢ï¼Œæ˜¯å› ç‚ºåœ¨åŸå§‹ Token çš„åŸºç¤ä¸Šæœƒå°‡ cls èˆ‡ Token é€²è¡Œ concatenateï¼Œå¾è€Œä½¿ Token çš„å¤§å°è®Šæˆäº† [b, 196+1, 768]ï¼Œç„¶å¾Œå°‡ Token èˆ‡ä½ç½®ä¿¡æ¯ç›¸åŠ ï¼Œè€Œä¸æ˜¯ concatenateï¼Œé€™æ¨£å°±å®Œæˆäº† embed æ“ä½œã€‚ Embed å®Œæˆå¾Œï¼Œæˆ‘å€‘å°‡ Token é€å…¥åˆ° ATTENTION ä¸­ï¼ŒåŒæ™‚ç‚ºäº†ä¸¦è¡Œè¨ˆç®—ï¼Œæ­¤å·¥ä½œæœƒç›´æ¥ä½¿ç”¨ nn.Linear, å°‡è¼¸å‡ºå¾ [b, 197, 768] è®Šæˆ [b, 197, 3*768]ï¼Œç„¶å¾Œ reshape æˆ [3,b, 197, 768]ï¼Œä¾æ¬¡å¾—åˆ° qkvï¼Œç„¶å¾ŒæŒ‰ç…§ VIT è«–æ–‡ä¸­çš„è¨ˆç®—å…¬å¼ï¼Œå¾—åˆ°è¼¸å‡ºçµæœã€‚æ­¤æ™‚è¦æ³¨æ„çš„åœ°æ–¹åœ¨æ–¼ï¼Œå¦‚æœæ˜¯æœ‰å¤šå€‹é ­éƒ¨ï¼Œé‚£éº¼å¾—åˆ°çš„ qkv æ‡‰è©²æ˜¯ [3,b, head_num, 197, 768/head_num]ï¼Œæ‰€æœ‰çš„æ“ä½œåˆ†æˆå¤šå€‹é ­éƒ¨é€²è¡Œï¼Œå¾—åˆ°çš„è¼¸å‡ºå†æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä½†ç”±æ–¼æ­¤å·¥ä½œæ˜¯ä¸¦è¡Œè¨ˆç®—ï¼Œæ‰€ä»¥æœ€å¾Œå¾—åˆ°çš„è¼¸å‡ºæ‡‰è©²æ˜¯ [b, head_num, 197, 768/head_num]ï¼Œç„¶å¾Œç›´æ¥ reshape è®Šæˆ [b, 197, 768]å°±å®Œæˆäº†å…¨éƒ¨æ“ä½œã€‚åœ¨è©²å•é¡Œä¸­ï¼Œç”±æ–¼æ˜¯ä½¿ç”¨ Transformer ä¾†æå–ç‰¹å¾µåœ–ï¼Œè€Œéç›´æ¥åˆ†é¡ï¼Œæ‰€ä»¥ä¸¦ä¸éœ€è¦ä½¿ç”¨ cls tokenï¼Œä¸¦ä¸”é€™æ¨£ä¹Ÿæ–¹ä¾¿å¾ŒçºŒè™•ç†å·¥ä½œã€‚


2. Cross Modality Fusion

CMF æ¨¡å¡Šæ˜¯ä¸€å€‹ç‰¹å¾µèåˆæ¨¡å¡Šï¼Œå°‡ä¸Šè¿°ä¸‰å€‹æ¨¡å¡Šçš„è¼¸å‡ºèåˆå†ä¸€èµ·ã€‚å…·é«”çš„æ“ä½œèˆ‡ä¸€å€‹å¤šé ­è‡ªè¨»æ„åŠ›æ©Ÿåˆ¶é¡ä¼¼ï¼Œè€Œèˆ‡ MSA ç›¸ä¼¼åœ°æ–¹åœ¨æ–¼ï¼ŒCMF æ¨¡å¡Šä¸­ä¹Ÿå­˜åœ¨ qkv ï¼Œå…¶ q æ˜¯é€šéç©ºåŸŸç‰¹å¾µåœ–å·ç©å¾—åˆ°ï¼Œk å’Œ v æ˜¯é€šéé »åŸŸç‰¹å¾µåœ–å·ç©å¾—åˆ°ã€‚é¦–å…ˆæŒ‰ç…§ MSA ç®—æ³•å°‡ç©ºåŸŸç‰¹å¾µåœ–å’Œé »åŸŸç‰¹å¾µåœ–èåˆï¼Œå…·é«”çš„è¨ˆç®—å…¬å¼å¦‚ä¸‹æ‰€ç¤ºã€‚

$$
\begin{aligned}
&Q=\operatorname{Conv}_{q}\left(f_{s}\right), K=\operatorname{Conv}_{k}\left(f_{f q}\right), V=\operatorname{Conv}_{v}\left(f_{f q}\right) \\
&f_{\text {fuse }}=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{H / 4 \times W / 4 \times C}}\right) V
\end{aligned}
$$

å¾—åˆ°çš„èåˆç‰¹å¾µ $f_{fuse}$ åœ¨å’Œç©ºåŸŸç‰¹å¾µå’Œ Transformer è¼¸å‡ºçš„ç‰¹å¾µåœ–ç›¸åŠ ï¼Œä½œç‚ºç‰¹å¾µåœ–é€å…¥å·ç©ç¶²çµ¡ä¸­é€²ä¸€æ­¥æå–ç‰¹å¾µã€‚å…¶å…·é«”çš„è¨ˆç®—å…¬å¼å¦‚ä¸‹ã€‚

$$
f_{c m f}=C o n v_{3 \times 3}\left(f_{s}+f_{m t}+f_{f u s e}\right)
$$


## 1. INTRODUCTION

Figure 1: Visual artifacts of forged images in existing datasets, including color mismatch (row 1 col 1, row 2 col 3,
row 3 col 1, row 3 col 2, row 3 col 3) , shape distortion (row 1 col 3, row 2 col 1), visible boundaries (row 2 col 2), and facial blurring (row 1 col 2, row 4 col 1, row 4 col 2, row 4 col3).

åœ– 1ï¼šç¾æœ‰æ•¸æ“šé›†ä¸­å½é€ åœ–åƒçš„è¦–è¦ºå½å½±ï¼ŒåŒ…æ‹¬é¡è‰²ä¸åŒ¹é…ï¼ˆç¬¬ 1 è¡Œç¬¬ 1 è¡Œï¼Œç¬¬ 2 è¡Œç¬¬ 3 è¡Œï¼Œç¬¬ 3 è¡Œç¬¬ 1 è¡Œã€ç¬¬ 3 è¡Œç¬¬ 2 è¡Œã€ç¬¬ 3 è¡Œç¬¬ 3 è¡Œï¼‰ã€å½¢ç‹€å¤±çœŸï¼ˆç¬¬ 1 è¡Œç¬¬ 3 è¡Œã€ç¬¬ 2 è¡Œç¬¬ 1 è¡Œï¼‰ã€å¯è¦‹é‚Šç•Œï¼ˆç¬¬ 2 è¡Œç¬¬ 2 è¡Œï¼‰å’Œè‡‰éƒ¨æ¨¡ç³Šï¼ˆç¬¬ 1 è¡Œç¬¬ 2 è¡Œã€ç¬¬ 4 è¡Œï¼‰ç¬¬ 1 åˆ—ï¼Œç¬¬ 4 è¡Œ ç¬¬ 2 åˆ—ï¼Œç¬¬ 4 è¡Œ col3)ã€‚

Recent years have witnessed the rapid development of Deepfake techniques, which enable attackers to manipulate the facial area of an image and generate a forged image. 

As synthesized images are becoming more photo-realistic, it is extremely difficult to distinguish whether an image/video has been manipulated even for human eyes. 

At the same time, these forged images might be distributed on the Internet for malicious purposes, which could bring societal implications. 

The above challenges have driven the development of Deepfake forensics using deep neural networks.

Most existing approaches take as inputs a face region cropped out of an entire image and produce a binary real/fake prediction with deep CNN models. 

These methods capture artifacts from the face regions in a single scale with stacked convolutional operations. 

While decent detection results are achieved by stacked convolutions, they excel at modeling local information but fails
to consider the relationships of pixels globally due to constrained receptive field.

We posit that relationships among pixels are particularly useful for Deepfake detection, since pixels in certain artifacts are clearly different from the remaining pixels in the image. 

On the other hand, we observe that forgery patterns vary in sizes. 




è¿‘å¹´ä¾† Deepfake æŠ€è¡“è¿…é€Ÿç™¼å±•ï¼Œä½¿æ”»æ“Šè€…èƒ½å¤ æ“ç¸±åœ–åƒçš„é¢éƒ¨å€åŸŸä¸¦ç”Ÿæˆå½é€ åœ–åƒï¼Œä¸”éš¨è‘—åˆæˆåœ–åƒè®Šå¾—æ›´åŠ é€¼çœŸï¼Œå³ä½¿æ˜¯äººçœ¼ä¹Ÿå¾ˆé›£å€åˆ†åœ–åƒ/è¦–é »æ˜¯å¦å·²è¢«æ“ç¸±ã€‚åŒæ™‚é€™äº›å½é€ çš„åœ–åƒå¯èƒ½æœƒå‡ºæ–¼æƒ¡æ„ç›®çš„åœ¨äº’è¯ç¶²ä¸Šå‚³æ’­ï¼Œé€™å¯èƒ½æœƒå¸¶ä¾†ç¤¾æœƒå½±éŸ¿ã€‚ä¸Šè¿°æŒ‘æˆ°æ¨å‹•äº†ä½¿ç”¨æ·±åº¦ç¥ç¶“ç¶²çµ¡çš„ Deepfake å–è­‰æŠ€è¡“çš„ç™¼å±•ã€‚

å¤§å¤šæ•¸ç¾æœ‰æ–¹æ³•å°‡å¾æ•´å€‹åœ–åƒä¸­è£å‰ªå‡ºçš„äººè‡‰å€åŸŸä½œç‚ºè¼¸å…¥ï¼Œä¸¦ä½¿ç”¨æ·±åº¦ CNN æ¨¡å‹ç”ŸæˆäºŒé€²åˆ¶çœŸå¯¦/è™›å‡é æ¸¬ã€‚é€™äº›æ–¹æ³•é€šéå †ç–Šå·ç©æ“ä½œä»¥å–®ä¸€å°ºåº¦å¾é¢éƒ¨å€åŸŸæ•ç²å½å½±ã€‚é›–ç„¶é€šéå †ç–Šå·ç©å¯¦ç¾äº†ä¸éŒ¯çš„æª¢æ¸¬çµæœï¼Œä½†å®ƒå€‘æ“…é•·å°å±€éƒ¨ä¿¡æ¯é€²è¡Œå»ºæ¨¡ä½†å¤±æ•—äº†
ç”±æ–¼å—ç´„æŸçš„æ„Ÿå—é‡ï¼Œå…¨å±€è€ƒæ…®åƒç´ çš„é—œä¿‚ã€‚å‡è¨­åƒç´ ä¹‹é–“çš„é—œä¿‚å°æ–¼ Deepfake æª¢æ¸¬ç‰¹åˆ¥æœ‰ç”¨ï¼Œå› ç‚ºæŸäº›å½åƒä¸­çš„åƒç´ æ˜é¡¯ä¸åŒæ–¼åœ–åƒä¸­çš„å…¶é¤˜åƒç´ ã€‚å¦ä¸€æ–¹é¢ï¼Œç ”ç©¶è€…ä¹Ÿè§€å¯Ÿåˆ°å½é€ åœ–æ¡ˆçš„å¤§å°å„ä¸ç›¸åŒã€‚

For instance, Figure 1 gives examples from popular Deepfake datasets. 

We can see that some forgery traces such as color mismatch occur in small regions (like the mouth corners), while other forgery signals such as visible boundaries that almost span the entire image (see row 3 col 2 in Figure 1). 

Therefore, how to effectively explore regions of different scales in images is extremely critical for Deepfake detection.

To address the above limitations, we explore transformers to model the relationships of pixels due to their strong capability of long-term dependency modeling for both natural language processing tasks and computer vision tasks.

Unlike traditional transformers operating on a single-scale, we propose a multi-scale architecture to capture forged regions that potentially have different sizes. 

Furthermore, suggest that the artifacts of forged images will be destroyed by perturbations such as JPEG compression, making them imperceptible in the RGB domain but can still be detected in the frequency domain. 

This motivates us to use frequency information as a complementary modality in order to reveal artifacts that are no longer perceptible in the RGB domain.

Figure 2: Overview of the proposed M2TR.

The input is a suspicious face image (H x W x C), and the output includes both a forgery detection result and a predicted mask (H x W x 1), which locates the forgery regions.

To this end, we introduce M2TR, a Multi-modal Multi-scale Transformer, for Deepfake detection. M2TR is a multimodal framework, consisting of a Multi-scale Transformer (MT) module and a Cross Modality Fusion (CMF) module. 

In particular, M2TR first extracts features of an input image with a few convolutional layers.

We then generate patches of different sizes from the feature map, which are used as inputs to different heads of the transformer. 

Similarities of spatial patches across different scales are calculated to capture the inconsistency among different regions at multiple scales.

This benefits the discovery of forgery artifacts, since certain subtle forgery clues, e.g., blurring and color inconsistency, are often times hidden in small local patches. 

The outputs from the multi-scale transformer are further augmented with frequency information to derive fused feature representations using a cross modality fusion module. 

Finally, the integrated features are used as inputs to several convolutional layers to generate prediction results. 

In addition to binary classification, we also predict the manipulated regions of the face image in a multi-task manner. 

The rationale behind is that binary classification tends to result in easily overfitted models.

Therefore, we use face masks as additional supervisory signals to mitigate overfitting.

The availability of large-scale training data is an essential factor in the development of Deepfake detection methods.

Existing Deepfake datasets include the UADFV dataset, the DeepFake-TIMIT dataset (DF-TIMIT), the FaceForensics++ dataset (FF++), the Google DeepFake detection dataset (DFD), the FaceBook DeepFake detection challenge (DFDC) dataset, the WildDeepfake dataset, and the Celeb-DF dataset. 

However, the quality of visual samples in current Deepfake datasets is limited, containing clear artifacts (see Figure 1) like color mismatch, shape distortion, visible boundaries, and facial blurring.

Therefore, there is still a huge gap between the images in existing datasets and forged images in the wild which are circulated on the Internet.

Although the visual quality of Celeb-DF is relatively high compared to others, they use only one face swapping method to generate forged images, lacking sample diversity. 

In addition, there are no unbiased and comprehensive evaluation metrics to measure the quality of Deepfake datasets, which is not conducive to the development of subsequent Deepfake research.

In this paper, we present a large-scale and high-quality Deepfake dataset, Swapping and Reenactment DeepFake (SR-DF) dataset, which is generated using the state-of-the-art face swapping and facial reenactment methods for the development and evaluation of Deepfake detection methods. 

We visualize in Figure 4 the sampled forged faces in the proposed SR-DF dataset.

Besides, we propose a set of evaluation criteria to measure the quality of Deepfake
datasets from different perspectives.

We hope the release of SR-DF dataset and the evaluation systems will benefit the future research of Deepfake detection.

Our work makes the following key contributions:

- We propose a Multi-modal Multi-scale Transformer (M2TR) for Deepfake forensics, which uses a multi-scale transformer to detect local inconsistency at different scales and leverages frequency features to improve the robustness of detection. Extensive experiments demonstrate that our method achieves state-of-the-art detection performance on different datasets.

- We introduce a large-scale and challenging Deepfake dataset SR-DF, which is generated with state-of-the-art face swapping and facial reenactment methods.

- We construct the most comprehensive evaluation system and demonstrate that SR-DF dataset is well-suited for the
training Deepfake detection methods due to its visual quality and diversity.

## 2. RELATED WORK

- Deepfake Generation

- Deepfake Detection 

- Visual Transformers

## 3. APPROACH

Figure 3: Illustration of the Multi-scale Transformer.

3.1 Multi-scale Transformer

3.2 Cross Modality Fusion

3.3 Loss functions

- Cross-entropy loss.

- Segmentation loss

- Contrastive loss

## 4. SR-DF DATASET

4.1 Dataset Construction

- Synthesis Approaches

- Post-processing

Figure 4: Example frames from the SR-DF dataset. The first two rows are generated by manipulating facial expressions: (a) First-order-motion and (b) IcFace, while the last two rows are generated by manipulating facial identity: (c) FaceShifter and (d) FSGAN.


Figure 5: Synthesized images of blending the altered face into the background image.We compare three blending methods: naive stitching (left), stitching with color transfer (middle), and stitching with DoveNet (right).

Table 1: A comparison of SR-DF dataset with existing datasets for Deepfake detection. LQ: low-quality, HQ: highquality.

- Existing Deepfake Datasets

4.2 Visual Quality Assessment

- Mask-SSIM

- Perceptual Loss

Table 2: Average Mask-SSIM scores and perceptual loss of different Deepfake datasets.

The value of Mask-SSIM is in the range of [0,1], with the higher value corresponding to better image quality.

We follow to calculate MaskSSIM on videos that we have exact corresponding correspondences for DFD and DFDC dataset. 

For perceptual loss, lower value indicates the better image quality.

Figure 6: A feature perspective comparison of Celeb-DF, FF++ dataset (RAW) and SR-DF dataset. 

We use an ImageNetpretrained ResNet-18 network to extract features and t-SNE for dimension reduction.

Note that we only select one frame in each video for visualization.

Table 3: Average ğ¸ğ‘¤ğ‘ğ‘Ÿğ‘ values of different datasets, with lower value corresponding to smoother temporal results.

We also calculate the ğ¸ğ‘¤ğ‘ğ‘Ÿğ‘ of pristine videos in our dataset.

- Ewarp

- Feature Space Distribution

## 5. EXPERIMENTS

5.1 Experimental Settings

- Datasets

- Evaluation

- Implementation Details

5.2 Evaluation on FaceForensics++

Table 4: Quantitative frame-level detection results on FaceForensics++ dataset under all quality settings. 

The best results are marked as bold.

5.3 Evaluation on Celeb-DF and SR-DF

5.4 Generalization Ability

Table 5: Frame-level AUC scores (%) of various Deepfake detection methods on Celeb-DF and SR-DF dataset.

5.5 From Frames to Videos

5.6 Ablation study

Table 6: AUC scores (%) for cross-dataset evaluation on FF++, Celeb-DF, and SR-DF datasets. 

Note that some methods have not made their code public, so we directly use the data reported in their paper.

â€œâˆ’â€ denotes the results are unavailable.

- Effectiveness of Different Components

Table 7: Quantitative video-level detection results on different versions of FF++ dataset and SR-DF dataset.

M2TR mean denotes averaging the extracted features obtained by M2TR for all frames as the video-level representation, while M2TR vt f denotes using VTF Block for temporal fusion.

The best results are marked as bold.

Table 8: Ablation results on FF++ (HQ) and FF++ (LQ) with and without Multi-scale Transformer and CMF.

- Effectiveness of the Multi-scale Design

- Effectiveness of the Contrastive Loss

Table 9: Ablation results on FF++ (HQ) using multi-scale Transformer (MT) or single-scale transformer.

Table 10: AUC (%) for cross-dataset evaluation on FF++ (HQ), Celeb-DF, and SR-DF with (denoted as M2TR) and without (denoted as M2TR ncl) the supervision of constrative loss.

## 6. CONCLUSION
